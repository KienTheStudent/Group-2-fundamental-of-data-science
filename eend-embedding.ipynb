{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10531093,"sourceType":"datasetVersion","datasetId":6517104},{"sourceId":601180,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":450500,"modelId":466854}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:27.923249Z","iopub.execute_input":"2025-10-24T03:09:27.923533Z","iopub.status.idle":"2025-10-24T03:09:27.930227Z","shell.execute_reply.started":"2025-10-24T03:09:27.923510Z","shell.execute_reply":"2025-10-24T03:09:27.929660Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 0.Full Block","metadata":{}},{"cell_type":"code","source":"# \"\"\"\n# ContraEEND - Phase 1: Contrastive Pretraining (FIXED VERSION)\n# Pretrain encoder on callhome for speaker discrimination\n# \"\"\"\n# import os\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torchaudio\n# import numpy as np\n# from torch.utils.data import Dataset, DataLoader\n# from pathlib import Path\n# from typing import Tuple, Dict, List\n# import random\n# from tqdm import tqdm\n# import math\n\n# def set_seed(seed=42):\n#     \"\"\"Set random seed for reproducibility\"\"\"\n#     random.seed(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed_all(seed)\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = False\n\n# def setup_device():\n#     \"\"\"Setup device with comprehensive CUDA checking for Kaggle\"\"\"\n#     if torch.cuda.is_available():\n#         device = torch.device(\"cuda:0\")\n#         print(f\"✅ CUDA is available!\")\n#         print(f\"🚀 Using GPU: {torch.cuda.get_device_name(0)}\")\n#         print(\n#             f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n#         )\n#         print(f\"🔥 CUDA Version: {torch.version.cuda}\")\n\n#         # Set memory allocation strategy for Kaggle\n#         torch.cuda.empty_cache()\n#         if hasattr(torch.cuda, \"set_memory_fraction\"):\n#             torch.cuda.set_memory_fraction(0.8)  # Use 80% of GPU memory\n\n#     else:\n#         device = torch.device(\"cpu\")\n#         print(\"⚠️  CUDA not available, using CPU\")\n#         print(\"💡 Consider enabling GPU in Kaggle: Settings -> Accelerator -> GPU\")\n\n#     return device\n\n# setup_device()\n\n# class AudioProcessor:\n#     \"\"\"Unified audio processing pipeline\"\"\"\n#     def __init__(self, \n#                  sample_rate: int = 16000,\n#                  n_fft: int = 400,  # 25ms at 16kHz\n#                  hop_length: int = 160,  # 10ms at 16kHz\n#                  n_mels: int = 83,\n#                  win_length: int = 400):\n#         self.sample_rate = sample_rate\n#         self.mel_transform = torchaudio.transforms.MelSpectrogram(\n#             sample_rate=sample_rate,\n#             n_fft=n_fft,\n#             hop_length=hop_length,\n#             win_length=win_length,\n#             n_mels=n_mels,\n#             f_min=20,\n#             f_max=sample_rate // 2\n#         )\n    \n#     def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n#         \"\"\"\n#         Args:\n#             waveform: (channels, time) or (time,)\n#         Returns:\n#             log_mel: (n_mels, frames)\n#         \"\"\"\n#         if waveform.dim() == 1:\n#             waveform = waveform.unsqueeze(0)\n        \n#         # Ensure mono\n#         if waveform.shape[0] > 1:\n#             waveform = waveform.mean(dim=0, keepdim=True)\n        \n#         # Compute mel spectrogram\n#         mel = self.mel_transform(waveform)\n        \n#         # Log scaling with small epsilon for stability\n#         log_mel = torch.log(mel + 1e-6)\n        \n#         return log_mel.squeeze(0)  # (n_mels, frames)\n\n\n# class SpecAugment(nn.Module):\n#     \"\"\"SpecAugment for contrastive learning\"\"\"\n#     def __init__(self, freq_mask_param=27, time_mask_param=100, n_freq_masks=2, n_time_masks=2):\n#         super().__init__()\n#         self.freq_mask_param = freq_mask_param\n#         self.time_mask_param = time_mask_param\n#         self.n_freq_masks = n_freq_masks\n#         self.n_time_masks = n_time_masks\n    \n#     def forward(self, mel: torch.Tensor) -> torch.Tensor:\n#         \"\"\"\n#         Args:\n#             mel: (n_mels, time)\n#         Returns:\n#             augmented: (n_mels, time)\n#         \"\"\"\n#         mel = mel.clone()\n#         n_mels, n_frames = mel.shape\n        \n#         # Frequency masking\n#         for _ in range(self.n_freq_masks):\n#             f = random.randint(0, self.freq_mask_param)\n#             f0 = random.randint(0, max(0, n_mels - f))\n#             mel[f0:f0+f, :] = 0\n        \n#         # Time masking\n#         for _ in range(self.n_time_masks):\n#             t = random.randint(0, min(self.time_mask_param, max(1, n_frames - 1)))\n#             t0 = random.randint(0, max(0, n_frames - t))\n#             mel[:, t0:t0+t] = 0\n        \n#         return mel\n\n\n# class callhomeContrastiveDataset(Dataset):\n#     \"\"\"\n#     callhome dataset for contrastive learning\n#     Each sample returns: (anchor, positive, speaker_id)\n#     \"\"\"\n#     def __init__(self, \n#                  root_dir: str,\n#                  audio_processor: AudioProcessor,\n#                  segment_length: float = 3.0,  # seconds\n#                  sample_rate: int = 16000,\n#                  min_segment_length: float = 2.0,\n#                  apply_augment: bool = True):\n#         \"\"\"\n#         Args:\n#             root_dir: path to callhome (should contain speaker folders)\n#             audio_processor: AudioProcessor instance\n#             segment_length: length of audio segments in seconds\n#             sample_rate: audio sample rate\n#             min_segment_length: minimum audio length to consider\n#             apply_augment: whether to apply SpecAugment\n#         \"\"\"\n#         self.root_dir = Path(root_dir)\n#         self.audio_processor = audio_processor\n#         self.segment_samples = int(segment_length * sample_rate)\n#         self.min_samples = int(min_segment_length * sample_rate)\n#         self.apply_augment = apply_augment\n#         self.spec_augment = SpecAugment() if apply_augment else None\n        \n#         # Build speaker to utterances mapping\n#         self.speaker_to_utts = self._build_speaker_map()\n#         self.speakers = list(self.speaker_to_utts.keys())\n        \n#         # Create flat list for indexing\n#         self.samples = []\n#         for spk_id, utts in self.speaker_to_utts.items():\n#             for utt in utts:\n#                 self.samples.append((spk_id, utt))\n        \n#         print(f\"Loaded {len(self.speakers)} speakers, {len(self.samples)} utterances\")\n#         print(f\"SpecAugment: {'enabled' if apply_augment else 'disabled'}\")\n    \n#     def _build_speaker_map(self) -> Dict[str, List[Path]]:\n#         \"\"\"Build mapping from speaker_id to list of audio files\"\"\"\n#         speaker_map = {}\n        \n#         # Assuming structure: root_dir/speaker_id/*.wav or nested\n#         audio_extensions = {'.wav', '.flac', '.m4a', '.mp3'}\n        \n#         for audio_file in self.root_dir.rglob('*'):\n#             if audio_file.suffix.lower() in audio_extensions:\n#                 # Extract speaker ID (assuming it's a parent directory)\n#                 # Adjust this logic based on your callhome structure\n#                 speaker_id = audio_file.parent.name\n                \n#                 if speaker_id not in speaker_map:\n#                     speaker_map[speaker_id] = []\n#                 speaker_map[speaker_id].append(audio_file)\n        \n#         # Filter speakers with at least 2 utterances\n#         speaker_map = {k: v for k, v in speaker_map.items() if len(v) >= 2}\n        \n#         return speaker_map\n    \n#     def _load_audio_segment(self, audio_path: Path) -> torch.Tensor:\n#         \"\"\"Load random segment from audio file\"\"\"\n#         # Load audio\n#         waveform, sr = torchaudio.load(audio_path)\n        \n#         # Resample if needed\n#         if sr != self.audio_processor.sample_rate:\n#             resampler = torchaudio.transforms.Resample(sr, self.audio_processor.sample_rate)\n#             waveform = resampler(waveform)\n        \n#         # Take mono\n#         if waveform.shape[0] > 1:\n#             waveform = waveform.mean(dim=0, keepdim=True)\n        \n#         total_samples = waveform.shape[1]\n        \n#         # If audio is too short, pad it\n#         if total_samples < self.segment_samples:\n#             padding = self.segment_samples - total_samples\n#             waveform = F.pad(waveform, (0, padding))\n#         # Otherwise, random crop\n#         else:\n#             start_idx = random.randint(0, total_samples - self.segment_samples)\n#             waveform = waveform[:, start_idx:start_idx + self.segment_samples]\n        \n#         return waveform.squeeze(0)\n    \n#     def __len__(self) -> int:\n#         return len(self.samples)\n    \n#     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n#         \"\"\"\n#         Returns:\n#             anchor: (n_mels, frames)\n#             positive: (n_mels, frames) - different utterance from same speaker\n#             speaker_id: int\n#         \"\"\"\n#         speaker_id, anchor_path = self.samples[idx]\n        \n#         # Load anchor\n#         anchor_wav = self._load_audio_segment(anchor_path)\n#         anchor_mel = self.audio_processor(anchor_wav)\n        \n#         # Apply augmentation\n#         if self.apply_augment and self.spec_augment is not None:\n#             anchor_mel = self.spec_augment(anchor_mel)\n        \n#         # Load positive (different utterance from same speaker)\n#         positive_path = random.choice(self.speaker_to_utts[speaker_id])\n#         # Ensure it's different from anchor if possible\n#         if len(self.speaker_to_utts[speaker_id]) > 1:\n#             while positive_path == anchor_path:\n#                 positive_path = random.choice(self.speaker_to_utts[speaker_id])\n        \n#         positive_wav = self._load_audio_segment(positive_path)\n#         positive_mel = self.audio_processor(positive_wav)\n        \n#         # Apply different augmentation to positive\n#         if self.apply_augment and self.spec_augment is not None:\n#             positive_mel = self.spec_augment(positive_mel)\n        \n#         # Convert speaker_id to numeric\n#         speaker_idx = self.speakers.index(speaker_id)\n        \n#         return anchor_mel, positive_mel, speaker_idx\n\n\n# class ConformerBlock(nn.Module):\n#     \"\"\"Single Conformer block with multi-head attention and convolution\"\"\"\n#     def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n#         super().__init__()\n        \n#         # Feed-forward module 1\n#         self.ff1 = nn.Sequential(\n#             nn.LayerNorm(d_model),\n#             nn.Linear(d_model, d_model * 4),\n#             nn.SiLU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(d_model * 4, d_model),\n#             nn.Dropout(dropout)\n#         )\n        \n#         # Multi-head self-attention\n#         self.norm_attn = nn.LayerNorm(d_model)\n#         self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n#         self.dropout_attn = nn.Dropout(dropout)\n        \n#         # Convolution module\n#         self.norm_conv = nn.LayerNorm(d_model)\n#         self.conv = nn.Sequential(\n#             nn.Conv1d(d_model, d_model * 2, 1),\n#             nn.GLU(dim=1),\n#             nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n#             nn.BatchNorm1d(d_model),\n#             nn.SiLU(),\n#             nn.Conv1d(d_model, d_model, 1),\n#             nn.Dropout(dropout)\n#         )\n        \n#         # Feed-forward module 2\n#         self.ff2 = nn.Sequential(\n#             nn.LayerNorm(d_model),\n#             nn.Linear(d_model, d_model * 4),\n#             nn.SiLU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(d_model * 4, d_model),\n#             nn.Dropout(dropout)\n#         )\n        \n#         self.norm_out = nn.LayerNorm(d_model)\n    \n#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n#         \"\"\"\n#         Args:\n#             x: (batch, time, d_model)\n#         Returns:\n#             output: (batch, time, d_model)\n#         \"\"\"\n#         # FF1\n#         x = x + 0.5 * self.ff1(x)\n        \n#         # Attention\n#         x_norm = self.norm_attn(x)\n#         attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n#         x = x + self.dropout_attn(attn_out)\n        \n#         # Convolution\n#         x_norm = self.norm_conv(x)\n#         x_conv = x_norm.transpose(1, 2)  # (batch, d_model, time)\n#         x_conv = self.conv(x_conv)\n#         x = x + x_conv.transpose(1, 2)\n        \n#         # FF2\n#         x = x + 0.5 * self.ff2(x)\n        \n#         return self.norm_out(x)\n\n\n# class ConformerEncoder(nn.Module):\n#     \"\"\"Conformer encoder for audio processing\"\"\"\n#     def __init__(self, \n#                  input_dim: int = 83,\n#                  d_model: int = 256,\n#                  n_layers: int = 8,\n#                  n_heads: int = 4,\n#                  conv_kernel: int = 31,\n#                  dropout: float = 0.1,\n#                  subsampling_factor: int = 4):\n#         super().__init__()\n        \n#         # Subsampling layer (reduce frame rate by 4x)\n#         self.subsampling = nn.Sequential(\n#             nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1),\n#             nn.ReLU(),\n#             nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1),\n#             nn.ReLU()\n#         )\n        \n#         # Positional encoding\n#         self.pos_encoding = PositionalEncoding(d_model, dropout)\n        \n#         # Conformer blocks\n#         self.blocks = nn.ModuleList([\n#             ConformerBlock(d_model, n_heads, conv_kernel, dropout)\n#             for _ in range(n_layers)\n#         ])\n        \n#         self.d_model = d_model\n    \n#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n#         \"\"\"\n#         Args:\n#             x: (batch, n_mels, time)\n#         Returns:\n#             output: (batch, time//4, d_model)\n#         \"\"\"\n#         # Subsampling\n#         x = self.subsampling(x)  # (batch, d_model, time//4)\n#         x = x.transpose(1, 2)  # (batch, time//4, d_model)\n        \n#         # Positional encoding\n#         x = self.pos_encoding(x)\n        \n#         # Conformer blocks\n#         for block in self.blocks:\n#             x = block(x)\n        \n#         return x\n\n\n# class PositionalEncoding(nn.Module):\n#     \"\"\"Sinusoidal positional encoding\"\"\"\n#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n#         super().__init__()\n#         self.dropout = nn.Dropout(p=dropout)\n        \n#         position = torch.arange(max_len).unsqueeze(1)\n#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n#         pe = torch.zeros(1, max_len, d_model)\n#         pe[0, :, 0::2] = torch.sin(position * div_term)\n#         pe[0, :, 1::2] = torch.cos(position * div_term)\n#         self.register_buffer('pe', pe)\n    \n#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n#         x = x + self.pe[:, :x.size(1)]\n#         return self.dropout(x)\n\n\n# class ProjectionHead(nn.Module):\n#     \"\"\"Projection head for contrastive learning\"\"\"\n#     def __init__(self, input_dim: int, hidden_dim: int = 256, output_dim: int = 128):\n#         super().__init__()\n#         self.net = nn.Sequential(\n#             nn.Linear(input_dim, hidden_dim),\n#             nn.ReLU(),\n#             nn.Linear(hidden_dim, output_dim)\n#         )\n    \n#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n#         return self.net(x)\n\n\n# class ContraEENDEncoder(nn.Module):\n#     \"\"\"Full encoder with projection head\"\"\"\n#     def __init__(self, \n#                  input_dim: int = 83,\n#                  d_model: int = 256,\n#                  n_layers: int = 8,\n#                  n_heads: int = 4,\n#                  projection_dim: int = 128):\n#         super().__init__()\n        \n#         self.encoder = ConformerEncoder(\n#             input_dim=input_dim,\n#             d_model=d_model,\n#             n_layers=n_layers,\n#             n_heads=n_heads\n#         )\n        \n#         # Temporal pooling for utterance-level representation\n#         self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n        \n#         # Projection head\n#         self.projection = ProjectionHead(d_model, d_model, projection_dim)\n    \n#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n#         \"\"\"\n#         Args:\n#             x: (batch, n_mels, time)\n#         Returns:\n#             embeddings: (batch, d_model) - frame-level pooled\n#             projections: (batch, projection_dim) - for contrastive loss\n#         \"\"\"\n#         # Encode\n#         encoded = self.encoder(x)  # (batch, time//4, d_model)\n        \n#         # Temporal pooling\n#         pooled = self.temporal_pool(encoded.transpose(1, 2)).squeeze(-1)  # (batch, d_model)\n        \n#         # Project\n#         projected = self.projection(pooled)  # (batch, projection_dim)\n        \n#         # L2 normalize projections\n#         projected = F.normalize(projected, p=2, dim=1)\n        \n#         return pooled, projected\n\n\n# class SupConLoss(nn.Module):\n#     \"\"\"\n#     Supervised Contrastive Loss with hard negative mining\n#     Based on: https://arxiv.org/abs/2004.11362\n#     \"\"\"\n#     def __init__(self, temperature: float = 0.07, base_temperature: float = 0.07):\n#         super().__init__()\n#         self.temperature = temperature\n#         self.base_temperature = base_temperature\n    \n#     def forward(self, features: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n#         \"\"\"\n#         Args:\n#             features: (batch_size, projection_dim) - L2 normalized\n#             labels: (batch_size,) - speaker labels\n#         Returns:\n#             loss: scalar\n#         \"\"\"\n#         device = features.device\n#         batch_size = features.shape[0]\n        \n#         # Create mask for positives (same speaker)\n#         labels = labels.contiguous().view(-1, 1)\n#         mask = torch.eq(labels, labels.T).float().to(device)  # (batch, batch)\n        \n#         # Compute similarity matrix\n#         anchor_dot_contrast = torch.div(\n#             torch.matmul(features, features.T),\n#             self.temperature\n#         )  # (batch, batch)\n        \n#         # For numerical stability\n#         logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n#         logits = anchor_dot_contrast - logits_max.detach()\n        \n#         # Remove self-contrast\n#         logits_mask = torch.scatter(\n#             torch.ones_like(mask),\n#             1,\n#             torch.arange(batch_size).view(-1, 1).to(device),\n#             0\n#         )\n#         mask = mask * logits_mask\n        \n#         # Compute log_prob\n#         exp_logits = torch.exp(logits) * logits_mask\n#         log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n        \n#         # Compute mean of log-likelihood over positive\n#         mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n        \n#         # Loss\n#         loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n#         loss = loss.mean()\n        \n#         return loss\n\n\n# class ContrastiveTrainer:\n#     \"\"\"Trainer for Phase 1: Contrastive Pretraining\"\"\"\n#     def __init__(self,\n#                  model: nn.Module,\n#                  train_loader: DataLoader,\n#                  val_loader: DataLoader,\n#                  num_epochs: int,\n#                  device: str = 'cuda',\n#                  learning_rate: float = 1e-3,\n#                  weight_decay: float = 1e-4,\n#                  temperature: float = 0.07):\n        \n#         self.model = model.to(device)\n#         self.train_loader = train_loader\n#         self.val_loader = val_loader\n#         self.device = device\n        \n#         # Loss\n#         self.criterion = SupConLoss(temperature=temperature)\n        \n#         # Optimizer\n#         self.optimizer = torch.optim.AdamW(\n#             model.parameters(),\n#             lr=learning_rate,\n#             weight_decay=weight_decay,\n#             betas=(0.9, 0.98)\n#         )\n        \n#         # Warmup + Cosine scheduler\n#         self.warmup_epochs = 5\n#         self.total_steps = len(train_loader) * num_epochs\n#         self.warmup_steps = len(train_loader) * self.warmup_epochs\n#         self.current_step = 0\n        \n#         def lr_lambda(current_step):\n#             if current_step < self.warmup_steps:\n#                 # Linear warmup\n#                 return float(current_step) / float(max(1, self.warmup_steps))\n#             # Cosine decay after warmup\n#             progress = float(current_step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n#             return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n        \n#         from torch.optim.lr_scheduler import LambdaLR\n#         self.scheduler = LambdaLR(self.optimizer, lr_lambda)\n        \n#         # Mixed precision\n#         self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n#         self.use_amp = device == 'cuda'\n        \n#         # Early stopping\n#         self.best_val_loss = float('inf')\n#         self.patience = 15\n#         self.patience_counter = 0\n#         self.min_delta = 1e-4\n    \n#     def train_epoch(self, epoch: int) -> float:\n#         \"\"\"Train one epoch with AMP\"\"\"\n#         self.model.train()\n#         total_loss = 0\n        \n#         pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n#         for batch_idx, (anchor, positive, labels) in enumerate(pbar):\n#             anchor = anchor.to(self.device)\n#             positive = positive.to(self.device)\n#             labels = labels.to(self.device)\n            \n#             # Combine anchor and positive in same batch for contrastive learning\n#             combined = torch.cat([anchor, positive], dim=0)  # (2*batch, n_mels, time)\n#             combined_labels = torch.cat([labels, labels], dim=0)  # (2*batch,)\n            \n#             # Mixed precision forward pass\n#             with torch.cuda.amp.autocast(enabled=self.use_amp):\n#                 _, projections = self.model(combined)\n#                 loss = self.criterion(projections, combined_labels)\n            \n#             # Backward with scaling\n#             self.optimizer.zero_grad()\n#             if self.use_amp:\n#                 self.scaler.scale(loss).backward()\n#                 self.scaler.unscale_(self.optimizer)\n#                 torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n#                 self.scaler.step(self.optimizer)\n#                 self.scaler.update()\n#             else:\n#                 loss.backward()\n#                 torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n#                 self.optimizer.step()\n            \n#             # Step scheduler per batch\n#             self.current_step += 1\n#             self.scheduler.step()\n            \n#             total_loss += loss.item()\n#             current_lr = self.optimizer.param_groups[0]['lr']\n#             pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{current_lr:.6f}'})\n        \n#         avg_loss = total_loss / len(self.train_loader)\n#         return avg_loss\n    \n#     def validate(self, epoch: int) -> float:\n#         \"\"\"Validate\"\"\"\n#         self.model.eval()\n#         total_loss = 0\n        \n#         with torch.no_grad():\n#             pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n#             for anchor, positive, labels in pbar:\n#                 anchor = anchor.to(self.device)\n#                 positive = positive.to(self.device)\n#                 labels = labels.to(self.device)\n                \n#                 combined = torch.cat([anchor, positive], dim=0)\n#                 combined_labels = torch.cat([labels, labels], dim=0)\n                \n#                 with torch.cuda.amp.autocast(enabled=self.use_amp):\n#                     _, projections = self.model(combined)\n#                     loss = self.criterion(projections, combined_labels)\n                \n#                 total_loss += loss.item()\n#                 pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n#         avg_loss = total_loss / len(self.val_loader)\n#         return avg_loss\n    \n#     def compute_embedding_metrics(self, loader: DataLoader) -> Dict[str, float]:\n#         \"\"\"Compute cosine similarity metrics for validation\"\"\"\n#         self.model.eval()\n#         embeddings_list = []\n#         labels_list = []\n        \n#         with torch.no_grad():\n#             for batch_idx, (anchor, positive, labels) in enumerate(loader):\n#                 anchor = anchor.to(self.device)\n#                 with torch.cuda.amp.autocast(enabled=self.use_amp):\n#                     _, anchor_proj = self.model(anchor)\n#                 embeddings_list.append(anchor_proj.cpu())\n#                 labels_list.append(labels)\n                \n#                 if batch_idx >= 20:  # Limit samples for speed\n#                     break\n        \n#         if len(embeddings_list) == 0:\n#             return {'pos_sim_mean': 0.0, 'neg_sim_mean': 0.0, 'separation': 0.0}\n        \n#         embeddings = torch.cat(embeddings_list, dim=0)  # (N, projection_dim)\n#         labels = torch.cat(labels_list, dim=0)  # (N,)\n        \n#         # Compute cosine similarity matrix\n#         sim_matrix = torch.matmul(embeddings, embeddings.T)  # Already normalized\n        \n#         # Same speaker pairs (positive)\n#         same_speaker_mask = (labels.unsqueeze(0) == labels.unsqueeze(1))\n#         same_speaker_mask.fill_diagonal_(False)  # Exclude self\n#         same_speaker_sims = sim_matrix[same_speaker_mask]\n        \n#         # Different speaker pairs (negative)\n#         diff_speaker_mask = ~same_speaker_mask\n#         diff_speaker_mask.fill_diagonal_(False)\n#         diff_speaker_sims = sim_matrix[diff_speaker_mask]\n        \n#         metrics = {\n#             'pos_sim_mean': same_speaker_sims.mean().item() if len(same_speaker_sims) > 0 else 0.0,\n#             'neg_sim_mean': diff_speaker_sims.mean().item() if len(diff_speaker_sims) > 0 else 0.0,\n#             'separation': (same_speaker_sims.mean() - diff_speaker_sims.mean()).item() if len(same_speaker_sims) > 0 else 0.0\n#         }\n        \n#         return metrics\n    \n#     def save_checkpoint(self, epoch: int, loss: float, filepath: str):\n#         \"\"\"Save training checkpoint\"\"\"\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': self.model.state_dict(),\n#             'optimizer_state_dict': self.optimizer.state_dict(),\n#             'scheduler_state_dict': self.scheduler.state_dict(),\n#             'loss': loss,\n#             'best_val_loss': self.best_val_loss,\n#             'current_step': self.current_step,\n#             'patience_counter': self.patience_counter\n#         }, filepath)\n#         print(f\"Checkpoint saved: {filepath}\")\n    \n#     def load_checkpoint(self, filepath: str) -> int:\n#         \"\"\"Resume from checkpoint\"\"\"\n#         if not os.path.exists(filepath):\n#             print(f\"No checkpoint found at {filepath}\")\n#             return 0\n        \n#         checkpoint = torch.load(filepath, map_location=self.device)\n#         self.model.load_state_dict(checkpoint['model_state_dict'])\n#         self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n#         self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n#         self.current_step = checkpoint.get('current_step', 0)\n#         self.patience_counter = checkpoint.get('patience_counter', 0)\n        \n#         epoch = checkpoint['epoch']\n#         print(f\"✓ Resumed from epoch {epoch}, best val loss: {self.best_val_loss:.4f}\")\n#         return epoch\n    \n#     def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints', start_epoch: int = 0):\n#         \"\"\"Full training loop with early stopping\"\"\"\n#         Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        \n#         for epoch in range(start_epoch + 1, num_epochs + 1):\n#             print(f\"\\n{'='*60}\")\n#             print(f\"Epoch {epoch}/{num_epochs}\")\n#             print(f\"{'='*60}\")\n            \n#             # Train\n#             train_loss = self.train_epoch(epoch)\n#             print(f\"Train Loss: {train_loss:.4f}\")\n            \n#             # Validate\n#             val_loss = self.validate(epoch)\n#             print(f\"Val Loss: {val_loss:.4f}\")\n            \n#             # Compute embedding metrics\n#             metrics = self.compute_embedding_metrics(self.val_loader)\n#             print(f\"Pos Sim: {metrics['pos_sim_mean']:.4f} | Neg Sim: {metrics['neg_sim_mean']:.4f} | Separation: {metrics['separation']:.4f}\")\n            \n#             current_lr = self.optimizer.param_groups[0]['lr']\n#             print(f\"Learning Rate: {current_lr:.6f}\")\n            \n#             # Save checkpoint every epoch\n#             checkpoint_path = Path(checkpoint_dir) / f'contraeend_epoch_{epoch}.pth'\n#             self.save_checkpoint(epoch, val_loss, str(checkpoint_path))\n            \n#             # Early stopping check\n#             if val_loss < self.best_val_loss - self.min_delta:\n#                 self.best_val_loss = val_loss\n#                 self.patience_counter = 0\n#                 best_path = Path(checkpoint_dir) / 'contraeend_best.pth'\n#                 self.save_checkpoint(epoch, val_loss, str(best_path))\n#                 print(f\"✓ New best model saved! Val Loss: {val_loss:.4f}\")\n#             else:\n#                 self.patience_counter += 1\n#                 print(f\"Patience: {self.patience_counter}/{self.patience}\")\n                \n#                 if self.patience_counter >= self.patience:\n#                     print(f\"\\n⚠️ Early stopping triggered after {epoch} epochs\")\n#                     break\n        \n#         print(f\"\\nTraining completed! Best validation loss: {self.best_val_loss:.4f}\")\n\n\n# def main():\n#     \"\"\"Main training function for Phase 1\"\"\"\n    \n#     # Set random seed for reproducibility\n#     set_seed(42)\n    \n#     # Configuration\n#     config = {\n#         'callhome_path': '/kaggle/input/callhome',\n#         'batch_size': 64,  # 32 if no GPU\n#         'num_epochs': 100,\n#         'learning_rate': 1e-3,\n#         'weight_decay': 1e-4,\n#         'temperature': 0.07,\n#         'segment_length': 3.0,  # seconds\n#         'd_model': 256,\n#         'n_layers': 8,\n#         'n_heads': 4,\n#         'projection_dim': 128,\n#         'num_workers': 4,\n#         'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n#         'resume_from': None,  # Set to checkpoint path to resume\n#     }\n    \n#     print(\"=\"*60)\n#     print(\"ContraEEND - Phase 1: Contrastive Pretraining (FIXED)\")\n#     print(\"=\"*60)\n#     print(f\"Device: {config['device']}\")\n#     print(f\"Batch Size: {config['batch_size']}\")\n#     print(f\"Learning Rate: {config['learning_rate']}\")\n#     print(f\"Temperature: {config['temperature']}\")\n#     print(f\"Model: Conformer ({config['n_layers']} layers, {config['d_model']} dim)\")\n#     print(f\"Random Seed: 42\")\n#     print(\"=\"*60)\n    \n#     # Audio processor\n#     audio_processor = AudioProcessor()\n    \n#     # Datasets\n#     print(\"\\nLoading datasets...\")\n#     full_dataset = callhomeContrastiveDataset(\n#         root_dir=config['callhome_path'],\n#         audio_processor=audio_processor,\n#         segment_length=config['segment_length'],\n#         apply_augment=True  # Enable SpecAugment\n#     )\n    \n#     # Split for validation (90/10)\n#     train_size = int(0.9 * len(full_dataset))\n#     val_size = len(full_dataset) - train_size\n#     train_dataset, val_dataset = torch.utils.data.random_split(\n#         full_dataset, [train_size, val_size],\n#         generator=torch.Generator().manual_seed(42)\n#     )\n    \n#     print(f\"Train samples: {len(train_dataset)}\")\n#     print(f\"Val samples: {len(val_dataset)}\")\n    \n#     # Data loaders\n#     train_loader = DataLoader(\n#         train_dataset,\n#         batch_size=config['batch_size'],\n#         shuffle=True,\n#         num_workers=config['num_workers'],\n#         pin_memory=True if config['device'] == 'cuda' else False,\n#         drop_last=True\n#     )\n    \n#     val_loader = DataLoader(\n#         val_dataset,\n#         batch_size=config['batch_size'],\n#         shuffle=False,\n#         num_workers=config['num_workers'],\n#         pin_memory=True if config['device'] == 'cuda' else False\n#     )\n    \n#     # Model\n#     print(\"\\nInitializing model...\")\n#     model = ContraEENDEncoder(\n#         input_dim=83,\n#         d_model=config['d_model'],\n#         n_layers=config['n_layers'],\n#         n_heads=config['n_heads'],\n#         projection_dim=config['projection_dim']\n#     )\n    \n#     total_params = sum(p.numel() for p in model.parameters())\n#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     print(f\"Total parameters: {total_params:,}\")\n#     print(f\"Trainable parameters: {trainable_params:,}\")\n    \n#     # Trainer\n#     print(\"\\nInitializing trainer...\")\n#     trainer = ContrastiveTrainer(\n#         model=model,\n#         train_loader=train_loader,\n#         val_loader=val_loader,\n#         num_epochs=config['num_epochs'],\n#         device=config['device'],\n#         learning_rate=config['learning_rate'],\n#         weight_decay=config['weight_decay'],\n#         temperature=config['temperature']\n#     )\n    \n#     # Resume from checkpoint if specified\n#     start_epoch = 0\n#     if config['resume_from'] is not None and os.path.exists(config['resume_from']):\n#         print(f\"\\nResuming from checkpoint: {config['resume_from']}\")\n#         start_epoch = trainer.load_checkpoint(config['resume_from'])\n    \n#     # Train\n#     print(\"\\nStarting training...\")\n#     print(\"Features enabled:\")\n#     print(\"  ✓ Warmup (5 epochs) + Cosine LR schedule\")\n#     print(\"  ✓ Mixed Precision Training (AMP)\")\n#     print(\"  ✓ SpecAugment (freq + time masking)\")\n#     print(\"  ✓ Early Stopping (patience=15)\")\n#     print(\"  ✓ Embedding similarity monitoring\")\n#     print(\"  ✓ Random seed for reproducibility\")\n#     print(\"=\"*60)\n    \n#     trainer.train(\n#         num_epochs=config['num_epochs'],\n#         start_epoch=start_epoch\n#     )\n    \n#     print(\"\\n\" + \"=\"*60)\n#     print(\"Phase 1 Training Complete!\")\n#     print(f\"Best Validation Loss: {trainer.best_val_loss:.4f}\")\n#     print(\"=\"*60)\n\n\n# if __name__ == '__main__':\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:28.194002Z","iopub.execute_input":"2025-10-24T03:09:28.194453Z","iopub.status.idle":"2025-10-24T03:09:28.221296Z","shell.execute_reply.started":"2025-10-24T03:09:28.194420Z","shell.execute_reply":"2025-10-24T03:09:28.220354Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nContraEEND - Phase 1: Contrastive Pretraining (FIXED VERSION)\nPretrain encoder on callhome for speaker discrimination\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List\nimport random\nfrom tqdm import tqdm\nimport math\nimport time\ndef set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking for Kaggle\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"✅ CUDA is available!\")\n        print(f\"🚀 Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(\n            f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n        )\n        print(f\"🔥 CUDA Version: {torch.version.cuda}\")\n\n        # Set memory allocation strategy for Kaggle\n        torch.cuda.empty_cache()\n        if hasattr(torch.cuda, \"set_memory_fraction\"):\n            torch.cuda.set_memory_fraction(0.8)  # Use 80% of GPU memory\n\n    else:\n        device = torch.device(\"cpu\")\n        print(\"⚠️  CUDA not available, using CPU\")\n        print(\"💡 Consider enabling GPU in Kaggle: Settings -> Accelerator -> GPU\")\n\n    return device\n\nsetup_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:28.222909Z","iopub.execute_input":"2025-10-24T03:09:28.223399Z","iopub.status.idle":"2025-10-24T03:09:30.150445Z","shell.execute_reply.started":"2025-10-24T03:09:28.223378Z","shell.execute_reply":"2025-10-24T03:09:30.149828Z"}},"outputs":[{"name":"stdout","text":"✅ CUDA is available!\n🚀 Using GPU: Tesla P100-PCIE-16GB\n💾 GPU Memory: 17.1 GB\n🔥 CUDA Version: 12.4\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# 1. AUDIO PROCESSING","metadata":{}},{"cell_type":"code","source":"class AudioProcessor:\n    \"\"\"Unified audio processing pipeline\"\"\"\n    def __init__(self, \n                 sample_rate: int = 16000,\n                 n_fft: int = 400,  # 25ms at 16kHz\n                 hop_length: int = 160,  # 10ms at 16kHz\n                 n_mels: int = 83,\n                 win_length: int = 400):\n        self.sample_rate = sample_rate\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            n_mels=n_mels,\n            f_min=20,\n            f_max=sample_rate // 2\n        )\n    \n    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            waveform: (channels, time) or (time,)\n        Returns:\n            log_mel: (n_mels, frames)\n        \"\"\"\n        if waveform.dim() == 1:\n            waveform = waveform.unsqueeze(0)\n        \n        # Ensure mono\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n        \n        # Compute mel spectrogram\n        mel = self.mel_transform(waveform)\n        \n        # Log scaling with small epsilon for stability\n        log_mel = torch.log(mel + 1e-6)\n        \n        return log_mel.squeeze(0)  # (n_mels, frames)\n\n\nclass SpecAugment(nn.Module):\n    \"\"\"SpecAugment for contrastive learning\"\"\"\n    def __init__(self, freq_mask_param=27, time_mask_param=100, n_freq_masks=2, n_time_masks=2):\n        super().__init__()\n        self.freq_mask_param = freq_mask_param\n        self.time_mask_param = time_mask_param\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n    \n    def forward(self, mel: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            mel: (n_mels, time)\n        Returns:\n            augmented: (n_mels, time)\n        \"\"\"\n        mel = mel.clone()\n        n_mels, n_frames = mel.shape\n        \n        # Frequency masking\n        for _ in range(self.n_freq_masks):\n            f = random.randint(0, self.freq_mask_param)\n            f0 = random.randint(0, max(0, n_mels - f))\n            mel[f0:f0+f, :] = 0\n        \n        # Time masking\n        for _ in range(self.n_time_masks):\n            t = random.randint(0, min(self.time_mask_param, max(1, n_frames - 1)))\n            t0 = random.randint(0, max(0, n_frames - t))\n            mel[:, t0:t0+t] = 0\n        \n        return mel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.151802Z","iopub.execute_input":"2025-10-24T03:09:30.152121Z","iopub.status.idle":"2025-10-24T03:09:30.162145Z","shell.execute_reply.started":"2025-10-24T03:09:30.152102Z","shell.execute_reply":"2025-10-24T03:09:30.161366Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 2. DATASET","metadata":{}},{"cell_type":"code","source":"def build_audio_rttm_mapping(audio_dir: str, rttm_dir: str) -> List[Tuple[Path, Path]]:\n    \"\"\"\n    Build mapping between audio files and RTTM files by index\n    audio_0.wav <-> labels_0.rttm\n    \"\"\"\n    audio_dir = Path(audio_dir)\n    rttm_dir = Path(rttm_dir)\n    \n    # Get all audio and rttm files\n    audio_files = sorted(audio_dir.glob('audio_*.wav'))\n    rttm_files = sorted(rttm_dir.glob('labels_*.rttm'))\n    \n    # Also try other extensions\n    if len(audio_files) == 0:\n        for ext in ['.flac', '.sph', '.mp3']:\n            audio_files = sorted(audio_dir.glob(f'audio_*{ext}'))\n            if len(audio_files) > 0:\n                break\n    \n    print(f\"\\nFound {len(audio_files)} audio files\")\n    print(f\"Found {len(rttm_files)} RTTM files\")\n    \n    if len(audio_files) == 0:\n        raise ValueError(f\"No audio files found in {audio_dir}\")\n    if len(rttm_files) == 0:\n        raise ValueError(f\"No RTTM files found in {rttm_dir}\")\n    \n    # Extract indices and match\n    audio_map = {}\n    for audio_file in audio_files:\n        # Extract index from \"audio_123.wav\"\n        try:\n            idx = int(audio_file.stem.split('_')[1])\n            audio_map[idx] = audio_file\n        except (IndexError, ValueError):\n            print(f\"Warning: Cannot parse index from {audio_file.name}\")\n    \n    rttm_map = {}\n    for rttm_file in rttm_files:\n        # Extract index from \"labels_123.rttm\"\n        try:\n            idx = int(rttm_file.stem.split('_')[1])\n            rttm_map[idx] = rttm_file\n        except (IndexError, ValueError):\n            print(f\"Warning: Cannot parse index from {rttm_file.name}\")\n    \n    # Match by index\n    pairs = []\n    for idx in sorted(audio_map.keys()):\n        if idx in rttm_map:\n            pairs.append((audio_map[idx], rttm_map[idx]))\n        else:\n            print(f\"Warning: No RTTM file for audio index {idx}\")\n    \n    print(f\"Matched {len(pairs)} audio-RTTM pairs\")\n    \n    if len(pairs) == 0:\n        raise ValueError(\"No matching audio-RTTM pairs found\")\n    \n    # Show examples\n    print(f\"\\nExample pairs:\")\n    for audio_file, rttm_file in pairs[:3]:\n        print(f\"  {audio_file.name} <-> {rttm_file.name}\")\n    \n    return pairs\n\n\nclass callhomeContrastiveDataset(Dataset):\n    \"\"\"\n    Callhome dataset for contrastive learning\n    Parses RTTM files to extract speaker segments\n    Handles indexed naming: audio_0.wav <-> labels_0.rttm\n    \n    OPTIMIZATION: Caches audio files in memory for faster loading\n    \"\"\"\n    def __init__(self, \n                 audio_dir: str,\n                 rttm_dir: str,\n                 audio_processor: AudioProcessor,\n                 segment_length: float = 3.0,\n                 sample_rate: int = 16000,\n                 min_segment_length: float = 2.0,\n                 apply_augment: bool = True,\n                 cache_audio: bool = True):\n        \n        self.audio_dir = Path(audio_dir)\n        self.rttm_dir = Path(rttm_dir)\n        self.audio_processor = audio_processor\n        self.segment_length = segment_length\n        self.segment_samples = int(segment_length * sample_rate)\n        self.min_samples = int(min_segment_length * sample_rate)\n        self.sample_rate = sample_rate\n        self.apply_augment = apply_augment\n        self.cache_audio = cache_audio\n        self.audio_cache = {}  # Cache for loaded audio files\n        \n        self.spec_augment = SpecAugment(\n            freq_mask_param=15,\n            time_mask_param=50,\n            n_freq_masks=1,\n            n_time_masks=1\n        ) if apply_augment else None\n        \n        # Build audio-RTTM pairs by index\n        print(\"\\n\" + \"=\"*60)\n        print(\"Building audio-RTTM mapping...\")\n        print(\"=\"*60)\n        self.audio_rttm_pairs = build_audio_rttm_mapping(audio_dir, rttm_dir)\n        \n        # Parse RTTM files to build speaker segments\n        self.speaker_segments = self._parse_rttm_files()\n        self.speakers = list(self.speaker_segments.keys())\n        \n        # Create flat list for indexing\n        self.samples = []\n        for spk_id, segments in self.speaker_segments.items():\n            for seg in segments:\n                # Only keep segments longer than minimum\n                if seg['duration'] >= min_segment_length:\n                    self.samples.append((spk_id, seg))\n        \n        # Preload all audio files into memory if caching enabled\n        if self.cache_audio:\n            print(\"\\n📦 Caching audio files in memory...\")\n            self._cache_all_audio()\n        \n        print(f\"\\n✓ Loaded {len(self.speakers)} speakers, {len(self.samples)} segments\")\n        print(f\"  Segment length: {segment_length}s, Min length: {min_segment_length}s\")\n        print(f\"  SpecAugment: {'enabled' if apply_augment else 'disabled'}\")\n        print(f\"  Audio caching: {'enabled' if cache_audio else 'disabled'}\")\n    \n    def _parse_rttm_files(self) -> Dict[str, List[Dict]]:\n        \"\"\"\n        Parse RTTM files to extract speaker segments\n        Each audio file can have multiple speakers\n        \"\"\"\n        speaker_segments = {}\n        \n        print(f\"\\nParsing {len(self.audio_rttm_pairs)} audio-RTTM pairs...\")\n        \n        segments_found = 0\n        \n        for audio_path, rttm_path in tqdm(self.audio_rttm_pairs, desc=\"Parsing RTTM\"):\n            # Get file identifier for this pair\n            file_idx = audio_path.stem.split('_')[1]  # e.g., \"0\" from \"audio_0.wav\"\n            \n            with open(rttm_path, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    \n                    parts = line.split()\n                    if len(parts) < 8 or parts[0] != 'SPEAKER':\n                        continue\n                    \n                    # RTTM format: SPEAKER <file_id> 1 <start> <duration> <NA> <NA> <speaker_id> <NA>\n                    file_id_in_rttm = parts[1]  # This might be different from our filename\n                    start_time = float(parts[3])\n                    duration = float(parts[4])\n                    speaker_id = parts[7]\n                    \n                    # Build unique speaker ID: use file index + speaker ID\n                    # This ensures speakers from different files are treated as different\n                    full_speaker_id = f\"file{file_idx}_spk{speaker_id}\"\n                    \n                    # Create segment info\n                    segment = {\n                        'file_idx': file_idx,\n                        'audio_path': audio_path,\n                        'start': start_time,\n                        'duration': duration,\n                        'speaker_id': speaker_id\n                    }\n                    \n                    if full_speaker_id not in speaker_segments:\n                        speaker_segments[full_speaker_id] = []\n                    \n                    speaker_segments[full_speaker_id].append(segment)\n                    segments_found += 1\n        \n        print(f\"Total segments found: {segments_found}\")\n        \n        # Filter speakers with at least 2 segments\n        original_speaker_count = len(speaker_segments)\n        speaker_segments = {\n            spk: segs for spk, segs in speaker_segments.items() \n            if len(segs) >= 2\n        }\n        \n        filtered_count = original_speaker_count - len(speaker_segments)\n        print(f\"Speakers after filtering (≥2 segments): {len(speaker_segments)}\")\n        if filtered_count > 0:\n            print(f\"  Removed {filtered_count} speakers with <2 segments\")\n        \n        # Print statistics\n        if speaker_segments:\n            seg_counts = [len(segs) for segs in speaker_segments.values()]\n            durations = [seg['duration'] for segs in speaker_segments.values() for seg in segs]\n            print(f\"\\nSegment statistics:\")\n            print(f\"  Per speaker - Min: {min(seg_counts)}, Max: {max(seg_counts)}, Avg: {np.mean(seg_counts):.1f}\")\n            print(f\"  Duration - Min: {min(durations):.1f}s, Max: {max(durations):.1f}s, Avg: {np.mean(durations):.1f}s\")\n        \n        return speaker_segments\n    \n    def _cache_all_audio(self):\n        \"\"\"Preload all audio files into memory\"\"\"\n        unique_audio_files = set()\n        for _, segment in self.samples:\n            unique_audio_files.add(segment['audio_path'])\n        \n        print(f\"Loading {len(unique_audio_files)} unique audio files...\")\n        for audio_path in tqdm(unique_audio_files, desc=\"Caching audio\"):\n            try:\n                waveform, sr = torchaudio.load(str(audio_path))\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    waveform = resampler(waveform)\n                \n                # Convert to mono\n                if waveform.shape[0] > 1:\n                    waveform = waveform.mean(dim=0, keepdim=True)\n                \n                self.audio_cache[str(audio_path)] = waveform.squeeze(0)\n            except Exception as e:\n                print(f\"\\nError caching {audio_path}: {e}\")\n        \n        print(f\"✓ Cached {len(self.audio_cache)} audio files\")\n    \n    def _load_audio_segment(self, segment_info: Dict) -> torch.Tensor:\n        \"\"\"Load audio segment from file or cache based on RTTM timing\"\"\"\n        audio_path = segment_info['audio_path']\n        start_time = segment_info['start']\n        duration = segment_info['duration']\n        \n        try:\n            # Load from cache or file\n            if self.cache_audio and str(audio_path) in self.audio_cache:\n                waveform = self.audio_cache[str(audio_path)]\n            else:\n                waveform, sr = torchaudio.load(str(audio_path))\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    waveform = resampler(waveform)\n                \n                # Convert to mono\n                if waveform.shape[0] > 1:\n                    waveform = waveform.mean(dim=0, keepdim=True)\n                \n                waveform = waveform.squeeze(0)\n            \n            # Extract segment based on RTTM timing\n            start_sample = int(start_time * self.sample_rate)\n            duration_samples = int(duration * self.sample_rate)\n            end_sample = start_sample + duration_samples\n            \n            # Clip to valid range\n            start_sample = max(0, start_sample)\n            end_sample = min(len(waveform), end_sample)\n            \n            segment = waveform[start_sample:end_sample]\n            \n            # Crop or pad to target length\n            segment_len = len(segment)\n            \n            if segment_len < self.segment_samples:\n                # Pad if too short\n                padding = self.segment_samples - segment_len\n                segment = F.pad(segment.unsqueeze(0), (0, padding)).squeeze(0)\n            elif segment_len > self.segment_samples:\n                # Random crop if too long\n                max_start = segment_len - self.segment_samples\n                crop_start = random.randint(0, max_start)\n                segment = segment[crop_start:crop_start + self.segment_samples]\n            \n            return segment\n        \n        except Exception as e:\n            print(f\"\\nError loading {audio_path}: {e}\")\n            # Return silence if loading fails\n            return torch.zeros(self.segment_samples)\n    \n    def __len__(self) -> int:\n        return len(self.samples)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \"\"\"\n        Returns:\n            anchor: (n_mels, frames)\n            positive: (n_mels, frames) - different segment from same speaker\n            speaker_id: int\n        \"\"\"\n        speaker_id, anchor_segment = self.samples[idx]\n        \n        # Load anchor\n        anchor_wav = self._load_audio_segment(anchor_segment)\n        anchor_mel = self.audio_processor(anchor_wav)\n        \n        # Apply augmentation\n        if self.apply_augment and self.spec_augment is not None:\n            anchor_mel = self.spec_augment(anchor_mel)\n        \n        # Load positive (different segment from same speaker)\n        positive_segment = random.choice(self.speaker_segments[speaker_id])\n        \n        # Ensure it's different from anchor if possible\n        if len(self.speaker_segments[speaker_id]) > 1:\n            max_attempts = 10\n            for _ in range(max_attempts):\n                positive_segment = random.choice(self.speaker_segments[speaker_id])\n                # Check if different segment (different start time)\n                if positive_segment['start'] != anchor_segment['start']:\n                    break\n        \n        positive_wav = self._load_audio_segment(positive_segment)\n        positive_mel = self.audio_processor(positive_wav)\n        \n        # Apply different augmentation to positive\n        if self.apply_augment and self.spec_augment is not None:\n            positive_mel = self.spec_augment(positive_mel)\n        \n        # Convert speaker_id to numeric\n        speaker_idx = self.speakers.index(speaker_id)\n        \n        return anchor_mel, positive_mel, speaker_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.162929Z","iopub.execute_input":"2025-10-24T03:09:30.163124Z","iopub.status.idle":"2025-10-24T03:09:30.192656Z","shell.execute_reply.started":"2025-10-24T03:09:30.163108Z","shell.execute_reply":"2025-10-24T03:09:30.191961Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 3. MODEL ARCHITECTURE","metadata":{}},{"cell_type":"code","source":"class ConformerBlock(nn.Module):\n    \"\"\"Single Conformer block with multi-head attention and convolution\"\"\"\n    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        \n        # Feed-forward module 1\n        self.ff1 = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        # Multi-head self-attention\n        self.norm_attn = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.dropout_attn = nn.Dropout(dropout)\n        \n        # Convolution module\n        self.norm_conv = nn.LayerNorm(d_model)\n        self.conv = nn.Sequential(\n            nn.Conv1d(d_model, d_model * 2, 1),\n            nn.GLU(dim=1),\n            nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n            nn.BatchNorm1d(d_model),\n            nn.SiLU(),\n            nn.Conv1d(d_model, d_model, 1),\n            nn.Dropout(dropout)\n        )\n        \n        # Feed-forward module 2\n        self.ff2 = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        self.norm_out = nn.LayerNorm(d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, time, d_model)\n        Returns:\n            output: (batch, time, d_model)\n        \"\"\"\n        # FF1\n        x = x + 0.5 * self.ff1(x)\n        \n        # Attention\n        x_norm = self.norm_attn(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + self.dropout_attn(attn_out)\n        \n        # Convolution\n        x_norm = self.norm_conv(x)\n        x_conv = x_norm.transpose(1, 2)  # (batch, d_model, time)\n        x_conv = self.conv(x_conv)\n        x = x + x_conv.transpose(1, 2)\n        \n        # FF2\n        x = x + 0.5 * self.ff2(x)\n        \n        return self.norm_out(x)\n\n\nclass ConformerEncoder(nn.Module):\n    \"\"\"Conformer encoder for audio processing\"\"\"\n    def __init__(self, \n                 input_dim: int = 83,\n                 d_model: int = 256,\n                 n_layers: int = 8,\n                 n_heads: int = 4,\n                 conv_kernel: int = 31,\n                 dropout: float = 0.1,\n                 subsampling_factor: int = 4):\n        super().__init__()\n        \n        # Subsampling layer (reduce frame rate by 4x)\n        self.subsampling = nn.Sequential(\n            nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n        \n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(d_model, dropout)\n        \n        # Conformer blocks\n        self.blocks = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.d_model = d_model\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n        Returns:\n            output: (batch, time//4, d_model)\n        \"\"\"\n        # Subsampling\n        x = self.subsampling(x)  # (batch, d_model, time//4)\n        x = x.transpose(1, 2)  # (batch, time//4, d_model)\n        \n        # Positional encoding\n        x = self.pos_encoding(x)\n        \n        # Conformer blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n\nclass ProjectionHead(nn.Module):\n    \"\"\"Projection head for contrastive learning\"\"\"\n    def __init__(self, input_dim: int, hidden_dim: int = 256, output_dim: int = 128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\n\nclass ContraEENDEncoder(nn.Module):\n    \"\"\"Full encoder with projection head\"\"\"\n    def __init__(self, \n                 input_dim: int = 83,\n                 d_model: int = 256,\n                 n_layers: int = 8,\n                 n_heads: int = 4,\n                 projection_dim: int = 128):\n        super().__init__()\n        \n        self.encoder = ConformerEncoder(\n            input_dim=input_dim,\n            d_model=d_model,\n            n_layers=n_layers,\n            n_heads=n_heads\n        )\n        \n        # Temporal pooling for utterance-level representation\n        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n        \n        # Projection head\n        self.projection = ProjectionHead(d_model, d_model, projection_dim)\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n        Returns:\n            embeddings: (batch, d_model) - frame-level pooled\n            projections: (batch, projection_dim) - for contrastive loss\n        \"\"\"\n        # Encode\n        encoded = self.encoder(x)  # (batch, time//4, d_model)\n        \n        # Temporal pooling\n        pooled = self.temporal_pool(encoded.transpose(1, 2)).squeeze(-1)  # (batch, d_model)\n        \n        # Project\n        projected = self.projection(pooled)  # (batch, projection_dim)\n        \n        # L2 normalize projections\n        projected = F.normalize(projected, p=2, dim=1)\n        \n        return pooled, projected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.194051Z","iopub.execute_input":"2025-10-24T03:09:30.194314Z","iopub.status.idle":"2025-10-24T03:09:30.219545Z","shell.execute_reply.started":"2025-10-24T03:09:30.194296Z","shell.execute_reply":"2025-10-24T03:09:30.219037Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 4. CONTRASTIVE LOSS\n","metadata":{}},{"cell_type":"code","source":"class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Loss with hard negative mining\"\"\"\n    def __init__(self, temperature: float = 0.2, base_temperature: float = 0.2):\n        super().__init__()\n        self.temperature = temperature\n        self.base_temperature = base_temperature\n    \n    def forward(self, features: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        device = features.device\n        batch_size = features.shape[0]\n        \n        labels = labels.contiguous().view(-1, 1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        \n        # Compute similarity matrix\n        anchor_dot_contrast = torch.div(\n            torch.matmul(features, features.T),\n            self.temperature\n        )\n        \n        # For numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n        \n        # Remove self-contrast\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size).view(-1, 1).to(device),\n            0\n        )\n        mask = mask * logits_mask\n        \n        # HARD NEGATIVE MINING: Weight harder negatives more\n        exp_logits = torch.exp(logits) * logits_mask\n        \n        # Hard negative mask (only keep negatives, exclude self and positives)\n        hard_negative_mask = (1 - mask) * logits_mask\n        \n        # Weighted by difficulty (higher similarity = harder negative)\n        hard_weights = torch.exp(logits) * hard_negative_mask\n        hard_weights = hard_weights / (hard_weights.sum(1, keepdim=True) + 1e-8)\n        \n        # Reweight denominator with hard negatives\n        weighted_exp_logits = exp_logits * (1 + hard_weights)\n        \n        log_prob = logits - torch.log(weighted_exp_logits.sum(1, keepdim=True))\n        \n        # Compute mean over positives\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n        \n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.mean()\n        \n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.220239Z","iopub.execute_input":"2025-10-24T03:09:30.220504Z","iopub.status.idle":"2025-10-24T03:09:30.239166Z","shell.execute_reply.started":"2025-10-24T03:09:30.220481Z","shell.execute_reply":"2025-10-24T03:09:30.238639Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 5. Training","metadata":{}},{"cell_type":"code","source":"import csv\nfrom datetime import datetime\n\nclass MetricsLogger:\n    \"\"\"CSV logger for tracking training metrics\"\"\"\n    def __init__(self, log_dir: str = './logs', experiment_name: str = 'contraeend'):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        self.csv_path = self.log_dir / f'{experiment_name}_{timestamp}.csv'\n        \n        self.columns = [\n            'timestamp', 'epoch', 'phase', 'learning_rate',\n            'train_loss', 'val_loss',\n            'pos_sim_mean', 'neg_sim_mean', 'separation',\n            'accuracy', 'adjusted_rand_score', 'der', 'transition_rate',\n            'avg_segment_duration', 'num_segments', 'temporal_stability',\n            'per_speaker_accuracy_mean', 'per_speaker_accuracy_std',\n            'num_speakers_detected', 'num_speakers_true',\n            'has_temporal_modeling', 'oracle_speakers',\n            'filename', 'duration',\n            'batch_size', 'd_model', 'n_layers', 'temperature',\n        ]\n        \n        with open(self.csv_path, 'w', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=self.columns)\n            writer.writeheader()\n        \n        print(f\"📊 Metrics logger initialized: {self.csv_path}\")\n    \n    def log_epoch(self, epoch, phase, train_loss, val_loss, learning_rate,\n                  embedding_metrics=None, config=None):\n        \"\"\"Log metrics for one epoch\"\"\"\n        row = {\n            'timestamp': datetime.now().isoformat(),\n            'epoch': epoch,\n            'phase': phase,\n            'learning_rate': learning_rate,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n        }\n        \n        if embedding_metrics:\n            row.update({\n                'pos_sim_mean': embedding_metrics.get('pos_sim_mean', ''),\n                'neg_sim_mean': embedding_metrics.get('neg_sim_mean', ''),\n                'separation': embedding_metrics.get('separation', ''),\n            })\n        \n        if config:\n            row.update({\n                'batch_size': config.get('batch_size', ''),\n                'd_model': config.get('d_model', ''),\n                'n_layers': config.get('n_layers', ''),\n                'temperature': config.get('temperature', ''),\n            })\n        \n        with open(self.csv_path, 'a', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=self.columns)\n            writer.writerow(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.239919Z","iopub.execute_input":"2025-10-24T03:09:30.240139Z","iopub.status.idle":"2025-10-24T03:09:30.259358Z","shell.execute_reply.started":"2025-10-24T03:09:30.240114Z","shell.execute_reply":"2025-10-24T03:09:30.258645Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class ContrastiveTrainer:\n    \"\"\"Trainer for Phase 1: Contrastive Pretraining\"\"\"\n    def __init__(self,\n                 model: nn.Module,\n                 train_loader: DataLoader,\n                 val_loader: DataLoader,\n                 num_epochs: int,\n                 device: str = 'cuda',\n                 learning_rate: float = 1e-3,\n                 weight_decay: float = 1e-4,\n                 temperature: float = 0.2,\n                 accumulation_steps: int = 4, \n                 log_dir: str = './logs',\n                 config: dict = None):  \n        \n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        \n        # Loss\n        self.criterion = SupConLoss(temperature=temperature)\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.98)\n        )\n        \n        # Warmup + Cosine scheduler\n        self.warmup_epochs = 5\n        self.total_steps = len(train_loader) * num_epochs\n        self.warmup_steps = len(train_loader) * self.warmup_epochs\n        self.current_step = 0\n        \n        def lr_lambda(current_step):\n            if current_step < self.warmup_steps:\n                # Linear warmup\n                return float(current_step) / float(max(1, self.warmup_steps))\n            # Cosine decay after warmup\n            progress = float(current_step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n        \n        from torch.optim.lr_scheduler import LambdaLR\n        self.scheduler = LambdaLR(self.optimizer, lr_lambda)\n        \n        # Mixed precision\n        self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n        self.use_amp = device == 'cuda'\n        \n        # Early stopping\n        self.best_val_loss = float('inf')\n        self.patience = 10\n        self.patience_counter = 0\n        self.min_delta = 1e-4\n        self.accumulation_steps = accumulation_steps\n        self.logger = MetricsLogger(log_dir=log_dir, experiment_name='contraeend')\n        self.config = config\n        print(f\"Effective batch size: {train_loader.batch_size * 2 * accumulation_steps}\")\n    \n    \n    def train_epoch(self, epoch: int) -> float:\n        \"\"\"Train one epoch with gradient accumulation\"\"\"\n        self.model.train()\n        # Check GPU usage on first batch\n        if epoch == 1:\n            print(\"\\n🔍 Checking GPU usage on first batch...\")\n        total_loss = 0\n        accumulated_loss = 0\n\n        # Timing\n        data_time = 0\n        forward_time = 0\n        backward_time = 0\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n        for batch_idx, (anchor, positive, labels) in enumerate(pbar):\n            t0 = time.time()\n            \n            anchor = anchor.to(self.device)\n            positive = positive.to(self.device)\n            labels = labels.to(self.device)\n            combined = torch.cat([anchor, positive], dim=0)\n            combined_labels = torch.cat([labels, labels], dim=0)\n            \n            data_time += time.time() - t0\n            t1 = time.time()\n            \n            # Forward\n            with torch.cuda.amp.autocast(enabled=self.use_amp):\n                _, projections = self.model(combined)\n                loss = self.criterion(projections, combined_labels)\n                loss = loss / self.accumulation_steps\n            \n            forward_time += time.time() - t1\n            t2 = time.time()\n                \n            # Backward\n            if self.use_amp:\n                self.scaler.scale(loss).backward()\n            else:\n                loss.backward()\n            \n            accumulated_loss += loss.item()\n            backward_time += time.time() - t2\n            # Show timing on first epoch\n            if epoch == 1 and batch_idx == 10:\n                print(f\"\\n⏱️ Timing breakdown (first 10 batches):\")\n                print(f\"  Data loading: {data_time/10:.2f}s per batch\")\n                print(f\"  Forward pass: {forward_time/10:.2f}s per batch\")\n                print(f\"  Backward pass: {backward_time/10:.2f}s per batch\")\n            \n            # Update weights every accumulation_steps\n            if (batch_idx + 1) % self.accumulation_steps == 0:\n                if self.use_amp:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                    self.optimizer.step()\n                \n                self.optimizer.zero_grad()\n                \n                # Step scheduler\n                self.current_step += 1\n                self.scheduler.step()\n                \n                total_loss += accumulated_loss\n                current_lr = self.optimizer.param_groups[0]['lr']\n                pbar.set_postfix({\n                    'loss': f'{accumulated_loss:.4f}',\n                    'lr': f'{current_lr:.6f}'\n                })\n                accumulated_loss = 0\n        \n        avg_loss = total_loss / (len(self.train_loader) // self.accumulation_steps)\n        return avg_loss\n    \n    def validate(self, epoch: int) -> float:\n        \"\"\"Validate\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n            for anchor, positive, labels in pbar:\n                anchor = anchor.to(self.device)\n                positive = positive.to(self.device)\n                labels = labels.to(self.device)\n                \n                combined = torch.cat([anchor, positive], dim=0)\n                combined_labels = torch.cat([labels, labels], dim=0)\n                \n                with torch.cuda.amp.autocast(enabled=self.use_amp):\n                    _, projections = self.model(combined)\n                    loss = self.criterion(projections, combined_labels)\n                \n                total_loss += loss.item()\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_loss = total_loss / len(self.val_loader)\n        return avg_loss\n    \n    def compute_embedding_metrics(self, loader: DataLoader) -> Dict[str, float]:\n        \"\"\"Compute cosine similarity metrics for validation\"\"\"\n        self.model.eval()\n        embeddings_list = []\n        labels_list = []\n        \n        with torch.no_grad():\n            for batch_idx, (anchor, positive, labels) in enumerate(loader):\n                anchor = anchor.to(self.device)\n                with torch.cuda.amp.autocast(enabled=self.use_amp):\n                    _, anchor_proj = self.model(anchor)\n                embeddings_list.append(anchor_proj.cpu())\n                labels_list.append(labels)\n                \n                if batch_idx >= 20:  # Limit samples for speed\n                    break\n        \n        if len(embeddings_list) == 0:\n            return {'pos_sim_mean': 0.0, 'neg_sim_mean': 0.0, 'separation': 0.0}\n        \n        embeddings = torch.cat(embeddings_list, dim=0)  # (N, projection_dim)\n        labels = torch.cat(labels_list, dim=0)  # (N,)\n        \n        # Compute cosine similarity matrix\n        sim_matrix = torch.matmul(embeddings, embeddings.T)  # Already normalized\n        \n        # Same speaker pairs (positive)\n        same_speaker_mask = (labels.unsqueeze(0) == labels.unsqueeze(1))\n        same_speaker_mask.fill_diagonal_(False)  # Exclude self\n        same_speaker_sims = sim_matrix[same_speaker_mask]\n        \n        # Different speaker pairs (negative)\n        diff_speaker_mask = ~same_speaker_mask\n        diff_speaker_mask.fill_diagonal_(False)\n        diff_speaker_sims = sim_matrix[diff_speaker_mask]\n        \n        metrics = {\n            'pos_sim_mean': same_speaker_sims.mean().item() if len(same_speaker_sims) > 0 else 0.0,\n            'neg_sim_mean': diff_speaker_sims.mean().item() if len(diff_speaker_sims) > 0 else 0.0,\n            'separation': (same_speaker_sims.mean() - diff_speaker_sims.mean()).item() if len(same_speaker_sims) > 0 else 0.0\n        }\n        \n        return metrics\n    \n    def save_checkpoint(self, epoch: int, loss: float, filepath: str):\n        \"\"\"Save training checkpoint\"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'loss': loss,\n            'best_val_loss': self.best_val_loss,\n            'current_step': self.current_step,\n            'patience_counter': self.patience_counter\n        }, filepath)\n        print(f\"Checkpoint saved: {filepath}\")\n    \n    def load_checkpoint(self, filepath: str) -> int:\n        \"\"\"Resume from checkpoint\"\"\"\n        if not os.path.exists(filepath):\n            print(f\"No checkpoint found at {filepath}\")\n            return 0\n        \n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n        self.current_step = checkpoint.get('current_step', 0)\n        self.patience_counter = checkpoint.get('patience_counter', 0)\n        \n        epoch = checkpoint['epoch']\n        print(f\"✓ Resumed from epoch {epoch}, best val loss: {self.best_val_loss:.4f}\")\n        return epoch\n    \n    def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints', start_epoch: int = 0):\n        \"\"\"Full training loop with early stopping\"\"\"\n        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        \n        for epoch in range(start_epoch + 1, num_epochs + 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch}/{num_epochs}\")\n            print(f\"{'='*60}\")\n            \n            # Train\n            train_loss = self.train_epoch(epoch)\n            print(f\"Train Loss: {train_loss:.4f}\")\n            \n            # Validate\n            val_loss = self.validate(epoch)\n            print(f\"Val Loss: {val_loss:.4f}\")\n            \n            # Compute embedding metrics\n            metrics = self.compute_embedding_metrics(self.val_loader)\n            print(f\"Pos Sim: {metrics['pos_sim_mean']:.4f} | Neg Sim: {metrics['neg_sim_mean']:.4f} | Separation: {metrics['separation']:.4f}\")\n            \n            current_lr = self.optimizer.param_groups[0]['lr']\n            print(f\"Learning Rate: {current_lr:.6f}\")\n            \n            self.logger.log_epoch(\n                epoch=epoch, phase='pretrain', train_loss=train_loss,\n                val_loss=val_loss, learning_rate=current_lr,\n                embedding_metrics=metrics, config=self.config\n            )\n\n            # Save checkpoint every epoch\n            checkpoint_path = Path(checkpoint_dir) / f'contraeend_epoch_{epoch}.pth'\n            self.save_checkpoint(epoch, val_loss, str(checkpoint_path))\n            \n            # Early stopping check\n            if val_loss < self.best_val_loss - self.min_delta:\n                self.best_val_loss = val_loss\n                self.patience_counter = 0\n                best_path = Path(checkpoint_dir) / 'contraeend_best.pth'\n                self.save_checkpoint(epoch, val_loss, str(best_path))\n                print(f\"✓ New best model saved! Val Loss: {val_loss:.4f}\")\n            else:\n                self.patience_counter += 1\n                print(f\"Patience: {self.patience_counter}/{self.patience}\")\n                \n                if self.patience_counter >= self.patience:\n                    print(f\"\\n⚠️ Early stopping triggered after {epoch} epochs\")\n                    break\n        \n        print(f\"\\nTraining completed! Best validation loss: {self.best_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.260246Z","iopub.execute_input":"2025-10-24T03:09:30.260735Z","iopub.status.idle":"2025-10-24T03:09:30.288273Z","shell.execute_reply.started":"2025-10-24T03:09:30.260712Z","shell.execute_reply":"2025-10-24T03:09:30.287536Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 6. Training Script","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Main training function for Phase 1\"\"\"\n    \n    # Set random seed for reproducibility\n    set_seed(42)\n\n    # Configuration\n    config = {\n        'audio_dir': '/kaggle/input/callhome/audio',\n        'rttm_dir': '/kaggle/input/callhome/labels',\n        'batch_size': 64,  # Increased from 32\n        'num_epochs': 300,\n        'learning_rate': 5e-4,\n        'weight_decay': 1e-4,\n        'temperature': 0.2,\n        'segment_length': 3.0,\n        'd_model': 128,\n        'n_layers': 6,\n        'n_heads': 4,\n        'projection_dim': 64,\n        'num_workers': 4,  # Increased from 2\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n        'resume_from': '/kaggle/input/contraeend/pytorch/default/1/contraeend_best.pth',\n        'cache_audio': True,  # NEW: Enable audio caching\n    }\n    \n    print(\"=\"*60)\n    print(\"ContraEEND - Phase 1: Contrastive Pretraining (FIXED)\")\n    print(\"=\"*60)\n    print(f\"Device: {config['device']}\")\n    print(f\"Audio dir: {config['audio_dir']}\")\n    print(f\"RTTM dir: {config['rttm_dir']}\")\n    print(f\"Batch Size: {config['batch_size']}\")\n    print(f\"Temperature: {config['temperature']}\")\n    print(f\"Model: Conformer ({config['n_layers']} layers, {config['d_model']} dim)\")\n    print(\"=\"*60)\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Dataset with RTTM parsing\n    print(\"\\nLoading datasets...\")\n    full_dataset = callhomeContrastiveDataset(\n        audio_dir=config['audio_dir'],\n        rttm_dir=config['rttm_dir'],\n        audio_processor=audio_processor,\n        segment_length=config['segment_length'],\n        apply_augment=True,\n        cache_audio=config['cache_audio']  # NEW: Pass cache parameter\n    )\n        \n    # Check if dataset is empty\n    if len(full_dataset) == 0:\n        print(\"\\n❌ ERROR: No valid samples found!\")\n        print(\"\\nPossible issues:\")\n        print(\"1. Audio files don't match RTTM file IDs\")\n        print(\"2. All segments are too short (< 2.0s)\")\n        print(\"3. No speakers have ≥2 segments\")\n        return\n    \n    # Dataset diagnostics\n    print(\"\\n\" + \"=\"*60)\n    print(\"🔍 DATASET DIAGNOSTICS\")\n    print(\"=\"*60)\n    print(f\"Total samples: {len(full_dataset)}\")\n    print(f\"Total speakers: {len(full_dataset.speakers)}\")\n    \n    if len(full_dataset.speakers) > 0:\n        speaker_counts = {spk: len(segs) for spk, segs in full_dataset.speaker_segments.items()}\n        segments_per_speaker = list(speaker_counts.values())\n        \n        print(f\"Avg segments per speaker: {np.mean(segments_per_speaker):.1f}\")\n        print(f\"Min segments per speaker: {min(segments_per_speaker)}\")\n        print(f\"Max segments per speaker: {max(segments_per_speaker)}\")\n        \n        print(f\"\\nSample speakers: {full_dataset.speakers[:5]}\")\n    \n    # Check if dataset is sufficient\n    if len(full_dataset) < 200:\n        print(\"\\n⚠️  WARNING: Dataset is small!\")\n        print(f\"   Current: {len(full_dataset)} samples\")\n        print(f\"   This may limit model performance\")\n    \n    if len(full_dataset.speakers) < 20:\n        print(\"\\n⚠️  WARNING: Few speakers!\")\n        print(f\"   Current: {len(full_dataset.speakers)} speakers\")\n        print(f\"   Contrastive learning works best with 100+ speakers\")\n    \n    print(\"=\"*60 + \"\\n\")\n    \n    # Split for validation (90/10)\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    print(f\"Val samples: {len(val_dataset)}\")\n    \n    # Adjust batch size and accumulation if needed\n    if len(train_dataset) < config['batch_size'] * 4:\n        print(\"\\n⚠️  Small dataset: adjusting batch size and disabling accumulation\")\n        config['batch_size'] = min(16, len(train_dataset) // 2)\n        accumulation_steps = 1\n    else:\n        accumulation_steps = 4\n    \n    print(f\"Batch size: {config['batch_size']}\")\n    print(f\"Accumulation steps: {accumulation_steps}\")\n    print(f\"Effective batch size: {config['batch_size'] * accumulation_steps * 2}\")\n    \n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True if config['device'] == 'cuda' else False,\n        drop_last=True if len(train_dataset) > config['batch_size'] else False,\n        persistent_workers=True if config['num_workers'] > 0 else False  # NEW\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True if config['device'] == 'cuda' else False,\n        persistent_workers=True if config['num_workers'] > 0 else False  # NEW\n    )\n        \n    print(f\"Train batches: {len(train_loader)}\")\n    print(f\"Val batches: {len(val_loader)}\")\n    \n    # Model\n    print(\"\\nInitializing model...\")\n    model = ContraEENDEncoder(\n        input_dim=83,\n        d_model=config['d_model'],\n        n_layers=config['n_layers'],\n        n_heads=config['n_heads'],\n        projection_dim=config['projection_dim']\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    # Trainer\n    print(\"\\nInitializing trainer...\")\n    trainer = ContrastiveTrainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=config['num_epochs'],\n        device=config['device'],\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        temperature=config['temperature'],\n        accumulation_steps=accumulation_steps, \n        log_dir='./logs',\n        config=config\n    )\n    \n    # Resume from checkpoint if specified\n    start_epoch = 0\n    if config['resume_from'] is not None and os.path.exists(config['resume_from']):\n        print(f\"\\nResuming from checkpoint: {config['resume_from']}\")\n        start_epoch = trainer.load_checkpoint(config['resume_from'])\n    \n    # Train\n    print(\"\\nStarting training...\")\n    \n    trainer.train(\n        num_epochs=config['num_epochs'],\n        start_epoch=start_epoch\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Phase 1 Training Complete!\")\n    print(f\"Best Validation Loss: {trainer.best_val_loss:.4f}\")\n    print(\"=\"*60)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:09:30.289136Z","iopub.execute_input":"2025-10-24T03:09:30.289354Z","execution_failed":"2025-10-24T03:18:42.932Z"}},"outputs":[{"name":"stdout","text":"============================================================\nContraEEND - Phase 1: Contrastive Pretraining (FIXED)\n============================================================\nDevice: cuda\nAudio dir: /kaggle/input/callhome/audio\nRTTM dir: /kaggle/input/callhome/labels\nBatch Size: 64\nTemperature: 0.2\nModel: Conformer (6 layers, 128 dim)\n============================================================\n\nLoading datasets...\n\n============================================================\nBuilding audio-RTTM mapping...\n============================================================\n\nFound 140 audio files\nFound 140 RTTM files\nMatched 140 audio-RTTM pairs\n\nExample pairs:\n  audio_0.wav <-> labels_0.rttm\n  audio_1.wav <-> labels_1.rttm\n  audio_2.wav <-> labels_2.rttm\n\nParsing 140 audio-RTTM pairs...\n","output_type":"stream"},{"name":"stderr","text":"Parsing RTTM: 100%|██████████| 140/140 [00:00<00:00, 513.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total segments found: 33471\nSpeakers after filtering (≥2 segments): 289\n  Removed 5 speakers with <2 segments\n\nSegment statistics:\n  Per speaker - Min: 2, Max: 226, Avg: 115.8\n  Duration - Min: 0.0s, Max: 21.1s, Avg: 2.1s\n\n📦 Caching audio files in memory...\nLoading 140 unique audio files...\n","output_type":"stream"},{"name":"stderr","text":"Caching audio: 100%|██████████| 140/140 [00:13<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Cached 140 audio files\n\n✓ Loaded 289 speakers, 12876 segments\n  Segment length: 3.0s, Min length: 2.0s\n  SpecAugment: enabled\n  Audio caching: enabled\n\n============================================================\n🔍 DATASET DIAGNOSTICS\n============================================================\nTotal samples: 12876\nTotal speakers: 289\nAvg segments per speaker: 115.8\nMin segments per speaker: 2\nMax segments per speaker: 226\n\nSample speakers: ['file0_spkA', 'file0_spkB', 'file1_spkA', 'file1_spkB', 'file2_spkB']\n============================================================\n\nTrain samples: 11588\nVal samples: 1288\nBatch size: 64\nAccumulation steps: 4\nEffective batch size: 512\nTrain batches: 181\nVal batches: 21\n\nInitializing model...\nTotal parameters: 2,413,888\nTrainable parameters: 2,413,888\n\nInitializing trainer...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_123/216765348.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n","output_type":"stream"},{"name":"stdout","text":"📊 Metrics logger initialized: logs/contraeend_20251024_030945.csv\nEffective batch size: 512\n\nResuming from checkpoint: /kaggle/input/contraeend/pytorch/default/1/contraeend_best.pth\n✓ Resumed from epoch 83, best val loss: 1.8993\n\nStarting training...\n\n============================================================\nEpoch 84/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84 [Train]:   0%|          | 0/181 [00:00<?, ?it/s]/tmp/ipykernel_123/216765348.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.use_amp):\nEpoch 84 [Train]: 100%|██████████| 181/181 [00:20<00:00,  8.83it/s, loss=2.0599, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0359\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84 [Val]:   0%|          | 0/21 [00:00<?, ?it/s]/tmp/ipykernel_123/216765348.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.use_amp):\nEpoch 84 [Val]: 100%|██████████| 21/21 [00:02<00:00,  9.87it/s, loss=0.9080]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.0515\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_123/216765348.py:177: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.use_amp):\n","output_type":"stream"},{"name":"stdout","text":"Pos Sim: 0.8270 | Neg Sim: 0.0056 | Separation: 0.8214\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_84.pth\nPatience: 1/10\n\n============================================================\nEpoch 85/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.20it/s, loss=2.0300, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0130\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.56it/s, loss=1.2420]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.0625\nPos Sim: 0.8297 | Neg Sim: 0.0037 | Separation: 0.8259\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_85.pth\nPatience: 2/10\n\n============================================================\nEpoch 86/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.12it/s, loss=2.0425, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9997\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.29it/s, loss=0.8981]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.0968\nPos Sim: 0.8300 | Neg Sim: 0.0043 | Separation: 0.8257\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_86.pth\nPatience: 3/10\n\n============================================================\nEpoch 87/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.29it/s, loss=1.8439, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9837\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.51it/s, loss=0.9112]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9935\nPos Sim: 0.8266 | Neg Sim: 0.0029 | Separation: 0.8238\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_87.pth\nPatience: 4/10\n\n============================================================\nEpoch 88/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.27it/s, loss=1.9059, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9888\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.55it/s, loss=0.8173]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.0052\nPos Sim: 0.8395 | Neg Sim: 0.0039 | Separation: 0.8356\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_88.pth\nPatience: 5/10\n\n============================================================\nEpoch 89/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.31it/s, loss=1.9704, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9678\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.98it/s, loss=0.7361]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9267\nPos Sim: 0.8547 | Neg Sim: 0.0010 | Separation: 0.8537\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_89.pth\nPatience: 6/10\n\n============================================================\nEpoch 90/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.33it/s, loss=1.8119, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.92it/s, loss=0.7868]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9283\nPos Sim: 0.8461 | Neg Sim: 0.0054 | Separation: 0.8407\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_90.pth\nPatience: 7/10\n\n============================================================\nEpoch 91/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.41it/s, loss=2.1036, lr=0.000496]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9546\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.67it/s, loss=0.4645]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8981\nPos Sim: 0.8489 | Neg Sim: 0.0008 | Separation: 0.8481\nLearning Rate: 0.000496\nCheckpoint saved: checkpoints/contraeend_epoch_91.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n✓ New best model saved! Val Loss: 1.8981\n\n============================================================\nEpoch 92/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.23it/s, loss=1.9550, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9229\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.40it/s, loss=0.4589]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8675\nPos Sim: 0.8471 | Neg Sim: 0.0039 | Separation: 0.8431\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_92.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n✓ New best model saved! Val Loss: 1.8675\n\n============================================================\nEpoch 93/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.31it/s, loss=1.9253, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9149\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.48it/s, loss=0.8442]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.0011\nPos Sim: 0.8315 | Neg Sim: 0.0033 | Separation: 0.8282\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_93.pth\nPatience: 1/10\n\n============================================================\nEpoch 94/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.25it/s, loss=1.8229, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9206\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.23it/s, loss=1.0481]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8908\nPos Sim: 0.8489 | Neg Sim: 0.0008 | Separation: 0.8481\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_94.pth\nPatience: 2/10\n\n============================================================\nEpoch 95/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.22it/s, loss=2.0438, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9284\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95 [Val]: 100%|██████████| 21/21 [00:01<00:00, 10.91it/s, loss=1.1371]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8766\nPos Sim: 0.8487 | Neg Sim: 0.0014 | Separation: 0.8473\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_95.pth\nPatience: 3/10\n\n============================================================\nEpoch 96/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.25it/s, loss=1.9561, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9182\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.73it/s, loss=0.6945]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8850\nPos Sim: 0.8485 | Neg Sim: 0.0008 | Separation: 0.8477\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_96.pth\nPatience: 4/10\n\n============================================================\nEpoch 97/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.28it/s, loss=1.8631, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9004\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.71it/s, loss=0.5186]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9263\nPos Sim: 0.8335 | Neg Sim: 0.0015 | Separation: 0.8319\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_97.pth\nPatience: 5/10\n\n============================================================\nEpoch 98/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.20it/s, loss=1.9233, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.62it/s, loss=0.8663]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8343\nPos Sim: 0.8640 | Neg Sim: -0.0006 | Separation: 0.8646\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_98.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n✓ New best model saved! Val Loss: 1.8343\n\n============================================================\nEpoch 99/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.27it/s, loss=1.8088, lr=0.000495]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8760\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.71it/s, loss=0.9213]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8977\nPos Sim: 0.8607 | Neg Sim: 0.0007 | Separation: 0.8601\nLearning Rate: 0.000495\nCheckpoint saved: checkpoints/contraeend_epoch_99.pth\nPatience: 1/10\n\n============================================================\nEpoch 100/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.15it/s, loss=1.8734, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8666\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.71it/s, loss=0.4528]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8526\nPos Sim: 0.8536 | Neg Sim: 0.0012 | Separation: 0.8524\nLearning Rate: 0.000494\nCheckpoint saved: checkpoints/contraeend_epoch_100.pth\nPatience: 2/10\n\n============================================================\nEpoch 101/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 101 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.29it/s, loss=1.8827, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8718\n","output_type":"stream"},{"name":"stderr","text":"Epoch 101 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.38it/s, loss=0.4454]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9213\nPos Sim: 0.8509 | Neg Sim: 0.0022 | Separation: 0.8487\nLearning Rate: 0.000494\nCheckpoint saved: checkpoints/contraeend_epoch_101.pth\nPatience: 3/10\n\n============================================================\nEpoch 102/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 102 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.27it/s, loss=1.8559, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8719\n","output_type":"stream"},{"name":"stderr","text":"Epoch 102 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.77it/s, loss=0.7140]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9405\nPos Sim: 0.8333 | Neg Sim: 0.0065 | Separation: 0.8268\nLearning Rate: 0.000494\nCheckpoint saved: checkpoints/contraeend_epoch_102.pth\nPatience: 4/10\n\n============================================================\nEpoch 103/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 103 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.26it/s, loss=1.8796, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8508\n","output_type":"stream"},{"name":"stderr","text":"Epoch 103 [Val]: 100%|██████████| 21/21 [00:01<00:00, 12.01it/s, loss=0.7468]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8378\nPos Sim: 0.8609 | Neg Sim: 0.0025 | Separation: 0.8584\nLearning Rate: 0.000494\nCheckpoint saved: checkpoints/contraeend_epoch_103.pth\nPatience: 5/10\n\n============================================================\nEpoch 104/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 104 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.22it/s, loss=1.7802, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8312\n","output_type":"stream"},{"name":"stderr","text":"Epoch 104 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.51it/s, loss=0.5291]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7704\nPos Sim: 0.8611 | Neg Sim: -0.0002 | Separation: 0.8614\nLearning Rate: 0.000494\nCheckpoint saved: checkpoints/contraeend_epoch_104.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n✓ New best model saved! Val Loss: 1.7704\n\n============================================================\nEpoch 105/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 105 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.18it/s, loss=1.7446, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8199\n","output_type":"stream"},{"name":"stderr","text":"Epoch 105 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.94it/s, loss=0.4508]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7747\nPos Sim: 0.8779 | Neg Sim: -0.0027 | Separation: 0.8806\nLearning Rate: 0.000494\nCheckpoint saved: checkpoints/contraeend_epoch_105.pth\nPatience: 1/10\n\n============================================================\nEpoch 106/300\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 106 [Train]: 100%|██████████| 181/181 [00:19<00:00,  9.15it/s, loss=1.7021, lr=0.000494]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8128\n","output_type":"stream"},{"name":"stderr","text":"Epoch 106 [Val]: 100%|██████████| 21/21 [00:01<00:00, 11.84it/s, loss=0.4396]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7271\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/checkpoints","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T03:18:42.932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/working/checkpoints/contraeend_best.pth /kaggle/working/","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T03:18:42.932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r checkpoint.zip /kaggle/working/checkpoints/contraeend_best.pth","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-24T03:18:42.932Z"}},"outputs":[],"execution_count":null}]}