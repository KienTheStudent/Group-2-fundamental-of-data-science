{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3085190,"sourceType":"datasetVersion","datasetId":1877225},{"sourceId":613958,"sourceType":"modelInstanceVersion","modelInstanceId":461327,"modelId":477082},{"sourceId":613959,"sourceType":"modelInstanceVersion","modelInstanceId":461328,"modelId":477083}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nContraEEND - Phase 3: Adaptive Chunk-wise EEND with Memory\nMemory-augmented processing for long-form conversations\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List, Optional\nimport random\nfrom tqdm import tqdm\nimport math\nfrom itertools import permutations\nfrom scipy.optimize import linear_sum_assignment\nimport torch.nn.functional as F\nfrom torch.utils.data import BatchSampler\nfrom datetime import datetime\nimport csv\nimport time\n# ============================================================================\n# REUSE FROM PHASE 1 & 2\n# ============================================================================\n\ndef set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"✅ CUDA is available!\")\n        print(f\"🚀 Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        torch.cuda.empty_cache()\n    else:\n        device = torch.device(\"cpu\")\n        print(\"⚠️  CUDA not available, using CPU\")\n    return device\n\nsetup_device()\n\nclass AudioProcessor:\n    \"\"\"Unified audio processing pipeline\"\"\"\n    def __init__(self, sample_rate: int = 16000, n_fft: int = 400,\n                 hop_length: int = 160, n_mels: int = 83, win_length: int = 400):\n        self.sample_rate = sample_rate\n        self.hop_length = hop_length\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length,\n            win_length=win_length, n_mels=n_mels, f_min=20, f_max=sample_rate // 2\n        )\n    \n    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n        if waveform.dim() == 1:\n            waveform = waveform.unsqueeze(0)\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n        mel = self.mel_transform(waveform)\n        log_mel = torch.log(mel + 1e-6)\n        return log_mel.squeeze(0)\n\nclass ConformerBlock(nn.Module):\n    \"\"\"Single Conformer block\"\"\"\n    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        self.ff1 = nn.Sequential(\n            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 4), nn.SiLU(),\n            nn.Dropout(dropout), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout)\n        )\n        self.norm_attn = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.dropout_attn = nn.Dropout(dropout)\n        self.norm_conv = nn.LayerNorm(d_model)\n        self.conv = nn.Sequential(\n            nn.Conv1d(d_model, d_model * 2, 1), nn.GLU(dim=1),\n            nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n            nn.BatchNorm1d(d_model), nn.SiLU(), nn.Conv1d(d_model, d_model, 1), nn.Dropout(dropout)\n        )\n        self.ff2 = nn.Sequential(\n            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 4), nn.SiLU(),\n            nn.Dropout(dropout), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout)\n        )\n        self.norm_out = nn.LayerNorm(d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + 0.5 * self.ff1(x)\n        x_norm = self.norm_attn(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + self.dropout_attn(attn_out)\n        x_norm = self.norm_conv(x)\n        x_conv = self.conv(x_norm.transpose(1, 2))\n        x = x + x_conv.transpose(1, 2)\n        x = x + 0.5 * self.ff2(x)\n        return self.norm_out(x)\n\nclass ConformerEncoder(nn.Module):\n    \"\"\"Conformer encoder with exact output size calculation\"\"\"\n    def __init__(self, input_dim: int = 83, d_model: int = 128, n_layers: int = 6,\n                 n_heads: int = 4, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        self.subsampling = nn.Sequential(\n            nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1), nn.ReLU()\n        )\n        self.pos_encoding = PositionalEncoding(d_model, dropout)\n        self.blocks = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel, dropout) for _ in range(n_layers)\n        ])\n        self.d_model = d_model\n    \n    @staticmethod\n    def compute_output_frames_static(input_frames: int) -> int:\n        \"\"\"Static method to compute exact output frames\"\"\"\n        frames = ((input_frames + 2 * 1 - 3) // 2) + 1\n        frames = ((frames + 2 * 1 - 3) // 2) + 1\n        return frames\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.subsampling(x)\n        x = x.transpose(1, 2)\n        x = self.pos_encoding(x)\n        for block in self.blocks:\n            x = block(x)\n        return x\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass EENDDecoder(nn.Module):\n    \"\"\"EEND decoder with attention pooling\"\"\"\n    def __init__(self, d_model: int = 128, num_speakers: int = 6, n_layers: int = 2,\n                 n_heads: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.num_speakers = num_speakers\n        \n        # Decoder layers\n        self.decoder_layers = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel=31, dropout=dropout)\n            for _ in range(n_layers)\n        ])\n        \n        # Add attention pooling\n        self.attention_pool = AttentionPooling(d_model)\n        \n        # Output projection\n        self.output_proj = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, num_speakers),\n        )\n    \n    def forward(self, encoded: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            encoded: (batch, frames, d_model)\n        Returns:\n            logits: (batch, frames, num_speakers)\n        \"\"\"\n        x = encoded\n        \n        # Decoder layers\n        for layer in self.decoder_layers:\n            x = layer(x)\n        \n        # Project to speaker logits\n        logits = self.output_proj(x)\n        \n        return logits\n\nclass ContrastiveHead(nn.Module):\n    \"\"\"Frame-level contrastive head\"\"\"\n    def __init__(self, d_model: int = 128, projection_dim: int = 64):\n        super().__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, projection_dim)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        proj = self.projection(x)\n        proj = F.normalize(proj, p=2, dim=-1)\n        return proj\n        \nclass AttentionPooling(nn.Module):\n    \"\"\"Self-attention pooling for better speaker embeddings\"\"\"\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.Tanh(),\n            nn.Linear(d_model // 2, 1)\n        )\n    \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, frames, d_model)\n            mask: (batch, frames) - optional, 1 for valid frames\n        Returns:\n            pooled: (batch, d_model)\n        \"\"\"\n        # Compute attention weights\n        attn_weights = self.attention(x).squeeze(-1)  # (batch, frames)\n        \n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(~mask.bool(), -1e9)\n        \n        attn_weights = F.softmax(attn_weights, dim=1)  # (batch, frames)\n        \n        # Weighted sum\n        pooled = (x * attn_weights.unsqueeze(-1)).sum(dim=1)  # (batch, d_model)\n        return pooled\n\nclass ImprovedAdaptiveThresholdPredictor:\n    \"\"\"\n    Better threshold calibration that considers:\n    - Per-speaker thresholds\n    - Silence detection\n    - F1 optimization\n    \"\"\"\n    def __init__(self, num_speakers: int = 6):\n        self.num_speakers = num_speakers\n        self.thresholds = None\n        self.silence_threshold = 0.15  # If all speakers < this, predict silence\n    \n    def calibrate(self, model, val_loader, device):\n        \"\"\"Calibrate thresholds to maximize F1\"\"\"\n        print(\"\\n🎯 Calibrating adaptive thresholds...\")\n        \n        all_probs = []\n        all_labels = []\n        \n        model.eval()\n        with torch.no_grad():\n            for mel, labels, _ in val_loader:\n                mel = mel.to(device)\n                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n                    logits = model(mel, return_embeddings=False)\n                    probs = torch.sigmoid(logits)\n                \n                all_probs.append(probs.cpu())\n                all_labels.append(labels.cpu())\n        \n        all_probs = torch.cat(all_probs, dim=0)\n        all_labels = torch.cat(all_labels, dim=0)\n        \n        # Find optimal threshold per speaker using F1\n        self.thresholds = []\n        for spk in range(self.num_speakers):\n            best_thresh = 0.5\n            best_f1 = 0\n            \n            # Test thresholds from 0.2 to 0.8\n            for thresh in np.linspace(0.2, 0.8, 31):\n                pred = (all_probs[:, :, spk] > thresh).float()\n                \n                tp = ((pred == 1) & (all_labels[:, :, spk] == 1)).sum().item()\n                fp = ((pred == 1) & (all_labels[:, :, spk] == 0)).sum().item()\n                fn = ((pred == 0) & (all_labels[:, :, spk] == 1)).sum().item()\n                \n                prec = tp / (tp + fp + 1e-8)\n                rec = tp / (tp + fn + 1e-8)\n                f1 = 2 * prec * rec / (prec + rec + 1e-8)\n                \n                if f1 > best_f1:\n                    best_f1 = f1\n                    best_thresh = thresh\n            \n            self.thresholds.append(best_thresh)\n            print(f\"  Speaker {spk}: threshold={best_thresh:.3f}, F1={best_f1:.3f}\")\n        \n        self.thresholds = torch.tensor(self.thresholds)\n        \n        # Compute optimal silence threshold\n        # Find threshold where max(all_speakers) best separates silence from speech\n        has_speech = (all_labels.sum(dim=2) > 0).float()\n        max_probs = all_probs.max(dim=2)[0]\n        \n        best_silence_thresh = 0.15\n        best_silence_f1 = 0\n        \n        for thresh in np.linspace(0.05, 0.5, 46):\n            pred_silence = (max_probs < thresh).float()\n            true_silence = 1 - has_speech\n            \n            tp = ((pred_silence == 1) & (true_silence == 1)).sum().item()\n            fp = ((pred_silence == 1) & (true_silence == 0)).sum().item()\n            fn = ((pred_silence == 0) & (true_silence == 1)).sum().item()\n            \n            prec = tp / (tp + fp + 1e-8)\n            rec = tp / (tp + fn + 1e-8)\n            f1 = 2 * prec * rec / (prec + rec + 1e-8)\n            \n            if f1 > best_silence_f1:\n                best_silence_f1 = f1\n                best_silence_thresh = thresh\n        \n        self.silence_threshold = best_silence_thresh\n        print(f\"  Silence threshold: {best_silence_thresh:.3f}, F1={best_silence_f1:.3f}\")\n    \n    def predict(self, probs):\n        \"\"\"Apply calibrated thresholds with silence detection\"\"\"\n        if self.thresholds is None:\n            return (probs > 0.5).float()\n        \n        # Apply per-speaker thresholds\n        thresholds = self.thresholds.to(probs.device).view(1, 1, -1)\n        activities = (probs > thresholds).float()\n        \n        # Silence detection: if all speakers below threshold, predict silence\n        max_probs = probs.max(dim=-1)[0]\n        silence_mask = max_probs < self.silence_threshold\n        activities[silence_mask.unsqueeze(-1).expand_as(activities)] = 0.0\n        \n        return activities\n\nclass BalancedOverlapGating:\n    \"\"\"\n    Apply overlap predictions only when confident\n    Prevents false alarm explosion\n    \"\"\"\n    @staticmethod\n    def apply(speaker_activities: torch.Tensor, \n              overlap_probs: torch.Tensor,\n              overlap_threshold: float = 0.65,\n              min_second_speaker_prob: float = 0.25) -> torch.Tensor:\n        \"\"\"\n        Args:\n            speaker_activities: (batch, frames, num_speakers) - binary\n            overlap_probs: (batch, frames) - probabilities\n            overlap_threshold: Confidence threshold for overlap\n            min_second_speaker_prob: Min probability for second speaker\n        \n        Returns:\n            gated_activities: Overlap only where confident\n        \"\"\"\n        # Don't enforce overlap where it's already detected\n        num_active = speaker_activities.sum(dim=-1)\n        \n        # Identify frames where overlap is predicted but confidence is low\n        low_confidence_overlap = (num_active >= 2) & (overlap_probs < overlap_threshold)\n        \n        # For low-confidence overlap, remove weaker speaker\n        if low_confidence_overlap.any():\n            for b in range(speaker_activities.shape[0]):\n                for f in range(speaker_activities.shape[1]):\n                    if low_confidence_overlap[b, f]:\n                        # Get active speakers\n                        active_speakers = speaker_activities[b, f].nonzero(as_tuple=True)[0]\n                        \n                        if len(active_speakers) >= 2:\n                            # Keep only the strongest speaker\n                            speaker_activities[b, f, :] = 0\n                            speaker_activities[b, f, active_speakers[0]] = 1\n        \n        return speaker_activities\n\n\nclass OverlapDetectionHead(nn.Module):\n    \"\"\"Dedicated head for detecting overlapping speech\"\"\"\n    def __init__(self, d_model: int = 128, dropout: float = 0.1):\n        super().__init__()\n        self.detector = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, 1)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.detector(x)\n\n\n# ============================================================================\n# PHASE 3: MEMORY COMPONENTS\n# ============================================================================\n\nclass SpeakerMemoryBank(nn.Module):\n    \"\"\"\n    Memory bank that stores speaker representations across chunks.\n    Key innovation: Maintains speaker identity over time!\n    \"\"\"\n    def __init__(self, d_model: int = 128, num_speakers: int = 4, \n                 memory_size: int = 100, temperature: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.num_speakers = num_speakers\n        self.memory_size = memory_size\n        self.temperature = temperature\n        \n        # Memory storage: (num_speakers, memory_size, d_model)\n        self.register_buffer('memory', torch.zeros(num_speakers, memory_size, d_model))\n        self.register_buffer('memory_ptr', torch.zeros(num_speakers, dtype=torch.long))\n        self.register_buffer('memory_filled', torch.zeros(num_speakers, dtype=torch.bool))\n        \n        # Speaker statistics\n        self.register_buffer('speaker_counts', torch.zeros(num_speakers))\n        \n    def update(self, embeddings: torch.Tensor, speaker_labels: torch.Tensor):\n        \"\"\"\n        Update memory with new speaker embeddings\n        \n        Args:\n            embeddings: (batch, frames, d_model)\n            speaker_labels: (batch, frames, num_speakers) - binary\n        \"\"\"\n        # Detach to prevent backprop through memory updates\n        embeddings = embeddings.detach()\n        speaker_labels = speaker_labels.detach()\n\n        batch_size, frames, _ = embeddings.shape\n        \n        for spk_idx in range(self.num_speakers):\n            # Get frames for this speaker\n            spk_mask = speaker_labels[:, :, spk_idx] > 0.5  # (batch, frames)\n            \n            if spk_mask.sum() == 0:\n                continue\n            \n            # Extract speaker embeddings\n            spk_embeddings = embeddings[spk_mask]  # (N, d_model)\n            \n            # Aggregate to single representation (mean pooling)\n            spk_repr = spk_embeddings.mean(dim=0)  # (d_model,)\n            \n            # Store in memory (FIFO)\n            ptr = self.memory_ptr[spk_idx].item()\n            self.memory[spk_idx, ptr] = spk_repr\n            \n            # Update pointer\n            self.memory_ptr[spk_idx] = (ptr + 1) % self.memory_size\n            \n            # Mark as filled after first full cycle\n            if ptr == self.memory_size - 1:\n                self.memory_filled[spk_idx] = True\n            \n            # Update statistics\n            self.speaker_counts[spk_idx] += spk_mask.sum().item()\n    \n    def query(self, embeddings: torch.Tensor, top_k: int = 5) -> torch.Tensor:\n        \"\"\"\n        Query memory to get speaker context\n        \n        Args:\n            embeddings: (batch, frames, d_model)\n            top_k: Number of closest memory entries to retrieve\n        \n        Returns:\n            memory_context: (batch, frames, d_model)\n        \"\"\"\n        batch_size, frames, d_model = embeddings.shape\n        \n        # Flatten embeddings for efficient computation\n        emb_flat = embeddings.reshape(-1, d_model)  # (B*F, D)\n        \n        # Compute similarity with all memory entries\n        memory_flat = self.memory.reshape(-1, d_model)  # (S*M, D)\n        \n        # Cosine similarity\n        sim = torch.mm(emb_flat, memory_flat.T)  # (B*F, S*M)\n        \n        # Get top-k most similar memories\n        top_k_sim, top_k_idx = torch.topk(sim, k=min(top_k, memory_flat.shape[0]), dim=1)\n        \n        # Retrieve corresponding memory entries\n        top_k_memories = memory_flat[top_k_idx]  # (B*F, top_k, D)\n        \n        # Weighted aggregation using similarity as attention\n        weights = F.softmax(top_k_sim / self.temperature, dim=1).unsqueeze(-1)  # (B*F, top_k, 1)\n        context = (top_k_memories * weights).sum(dim=1)  # (B*F, D)\n        \n        # Reshape back\n        context = context.reshape(batch_size, frames, d_model)\n        \n        return context\n    \n    def get_speaker_prototypes(self) -> torch.Tensor:\n        \"\"\"\n        Get current speaker prototypes (mean of memory)\n        \n        Returns:\n            prototypes: (num_speakers, d_model)\n        \"\"\"\n        prototypes = []\n        for spk_idx in range(self.num_speakers):\n            if self.memory_filled[spk_idx]:\n                # Use all memory\n                proto = self.memory[spk_idx].mean(dim=0)\n            else:\n                # Use filled portion\n                ptr = self.memory_ptr[spk_idx].item()\n                if ptr > 0:\n                    proto = self.memory[spk_idx, :ptr].mean(dim=0)\n                else:\n                    proto = torch.zeros(self.d_model, device=self.memory.device)\n            prototypes.append(proto)\n        \n        return torch.stack(prototypes)  # (num_speakers, d_model)\n    \n    def reset(self):\n        \"\"\"Reset memory bank (for new conversation)\"\"\"\n        self.memory.zero_()\n        self.memory_ptr.zero_()\n        self.memory_filled.zero_()\n        self.speaker_counts.zero_()\n\n\nclass CrossChunkAttention(nn.Module):\n    \"\"\"\n    Cross-chunk attention to maintain temporal consistency.\n    Attends to memory context from previous chunks.\n    \"\"\"\n    def __init__(self, d_model: int = 128, n_heads: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        \n        # Multi-head cross attention\n        self.cross_attn = nn.MultiheadAttention(\n            d_model, n_heads, dropout=dropout, batch_first=True\n        )\n        \n        # Layer norm\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        # Feed-forward\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, query: torch.Tensor, memory_context: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            query: Current chunk embeddings (batch, frames, d_model)\n            memory_context: Memory context (batch, frames, d_model)\n        \n        Returns:\n            attended: Attended embeddings (batch, frames, d_model)\n        \"\"\"\n        # Cross attention: query attends to memory\n        attn_out, _ = self.cross_attn(\n            self.norm1(query),\n            self.norm1(memory_context),\n            self.norm1(memory_context)\n        )\n        \n        # Residual connection\n        x = query + attn_out\n        \n        # Feed-forward with residual\n        x = x + self.ffn(self.norm2(x))\n        \n        return x\n\n\nclass SpeakerTracker(nn.Module):\n    \"\"\"\n    Speaker tracking module that resolves permutation problem.\n    Matches current chunk speakers to memory bank speakers.\n    Uses Hungarian algorithm for optimal assignment.\n    \"\"\"\n    def __init__(self, d_model: int = 128, similarity_threshold: float = 0.7):\n        super().__init__()\n        self.d_model = d_model\n        self.similarity_threshold = similarity_threshold\n        \n        # Projection for speaker embeddings\n        self.speaker_proj = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, d_model)\n        )\n    \n    def compute_speaker_embeddings(self, encoded: torch.Tensor, \n                                   activities: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Extract speaker embeddings from encoded features and activities\n        \n        Args:\n            encoded: (batch, frames, d_model)\n            activities: (batch, frames, num_speakers) - probabilities\n        \n        Returns:\n            speaker_embeddings: (batch, num_speakers, d_model)\n        \"\"\"\n        batch_size, frames, d_model = encoded.shape\n        num_speakers = activities.shape[2]\n        \n        speaker_embeddings = []\n        \n        for b in range(batch_size):\n            batch_embeddings = []\n            for spk in range(num_speakers):\n                # Weighted pooling by speaker activity\n                weights = activities[b, :, spk].unsqueeze(-1)  # (frames, 1)\n                \n                # Normalize weights\n                weight_sum = weights.sum()\n                if weight_sum > 0:\n                    weights = weights / weight_sum\n                else:\n                    weights = torch.ones_like(weights) / frames\n                \n                # Weighted sum\n                spk_emb = (encoded[b] * weights).sum(dim=0)  # (d_model,)\n                batch_embeddings.append(spk_emb)\n            \n            speaker_embeddings.append(torch.stack(batch_embeddings))\n        \n        speaker_embeddings = torch.stack(speaker_embeddings)  # (batch, num_speakers, d_model)\n        \n        # Project\n        speaker_embeddings = self.speaker_proj(speaker_embeddings)\n        \n        return speaker_embeddings\n    \n    def match_speakers(self, current_embeddings: torch.Tensor,\n                      memory_prototypes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Match current speakers to memory bank speakers using Hungarian algorithm\n        \n        Args:\n            current_embeddings: (batch, num_speakers, d_model)\n            memory_prototypes: (num_speakers, d_model)\n        \n        Returns:\n            permutation: (batch, num_speakers) - indices for reordering\n        \"\"\"\n        batch_size, num_speakers, d_model = current_embeddings.shape\n        \n        permutations = []\n        \n        for b in range(batch_size):\n            # Compute similarity matrix\n            sim_matrix = torch.mm(\n                current_embeddings[b],  # (num_speakers, d_model)\n                memory_prototypes.T     # (d_model, num_speakers)\n            )  # (num_speakers, num_speakers)\n            \n            # Convert to cost (maximize similarity = minimize negative similarity)\n            cost_matrix = -sim_matrix.detach().cpu().numpy()\n            \n            # Hungarian algorithm\n            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n            \n            # Create permutation tensor\n            perm = torch.tensor(col_ind, dtype=torch.long, device=current_embeddings.device)\n            permutations.append(perm)\n        \n        return torch.stack(permutations)  # (batch, num_speakers)\n    \n    def apply_permutation(self, activities: torch.Tensor, \n                         permutation: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply permutation to reorder speaker activities\n        \n        Args:\n            activities: (batch, frames, num_speakers)\n            permutation: (batch, num_speakers)\n        \n        Returns:\n            reordered_activities: (batch, frames, num_speakers)\n        \"\"\"\n        batch_size, frames, num_speakers = activities.shape\n        \n        # Create index tensor for gathering\n        perm_expanded = permutation.unsqueeze(1).expand(batch_size, frames, num_speakers)\n        \n        # Reorder\n        reordered = torch.gather(activities, dim=2, index=perm_expanded)\n        \n        return reordered\n\n\n# ============================================================================\n# PHASE 3: ADAPTIVE CHUNK-WISE EEND MODEL\n# ============================================================================\n\n\nclass SimplifiedPhase3EEND(nn.Module):\n    \"\"\"Enhanced Phase 3 with overlap detection\"\"\"\n    def __init__(self, input_dim: int = 83, d_model: int = 128,\n                 encoder_layers: int = 6, decoder_layers: int = 2,\n                 n_heads: int = 4, num_speakers: int = 6,\n                 projection_dim: int = 64, dropout: float = 0.1):\n        super().__init__()\n        \n        self.num_speakers = num_speakers\n        \n        self.encoder = ConformerEncoder(\n            input_dim=input_dim, d_model=d_model, n_layers=encoder_layers,\n            n_heads=n_heads, dropout=dropout\n        )\n        \n        self.decoder = EENDDecoder(\n            d_model=d_model, num_speakers=num_speakers,\n            n_layers=decoder_layers, n_heads=n_heads, dropout=dropout\n        )\n        \n        # NEW: Overlap detection head\n        self.overlap_detector = OverlapDetectionHead(d_model, dropout)\n        \n        self.contrastive_head = ContrastiveHead(d_model, projection_dim)\n    \n    def forward(self, x: torch.Tensor, return_embeddings: bool = False):\n        encoded = self.encoder(x)\n        logits = self.decoder(encoded)\n        \n        if return_embeddings:\n            overlap_logits = self.overlap_detector(encoded)\n            embeddings = self.contrastive_head(encoded)\n            return logits, overlap_logits, embeddings\n        \n        return logits\n    \n    def predict(self, x: torch.Tensor, threshold_predictor=None) -> torch.Tensor:\n        \"\"\"\n        Balanced prediction with:\n        1. Calibrated thresholds\n        2. Silence detection\n        3. Conservative overlap enforcement\n        \"\"\"\n        encoded = self.encoder(x)\n        logits = self.decoder(encoded)\n        overlap_logits = self.overlap_detector(encoded)\n        \n        speaker_probs = torch.sigmoid(logits)\n        overlap_probs = torch.sigmoid(overlap_logits).squeeze(-1)\n        \n        # Step 1: Base speaker activities with calibrated thresholds\n        if threshold_predictor is not None and threshold_predictor.thresholds is not None:\n            activities = threshold_predictor.predict(speaker_probs)\n        else:\n            # Fallback: higher default threshold to reduce FA\n            activities = (speaker_probs > 0.6).float()\n            \n            # Simple silence detection\n            max_probs = speaker_probs.max(dim=-1)[0]\n            silence_mask = max_probs < 0.2\n            activities[silence_mask.unsqueeze(-1).expand_as(activities)] = 0.0\n        \n        # Step 2: Overlap enforcement (conservative)\n        num_active = activities.sum(dim=-1)\n        \n        # Only add overlap if:\n        # 1. Overlap probability is high (>0.6)\n        # 2. Currently 0-1 speakers active\n        # 3. Second speaker has decent probability (>0.3)\n        overlap_detected = overlap_probs > 0.6\n        needs_overlap = overlap_detected & (num_active < 2)\n        \n        if needs_overlap.any():\n            for b in range(activities.shape[0]):\n                for f in range(activities.shape[1]):\n                    if needs_overlap[b, f]:\n                        probs = speaker_probs[b, f]\n                        top2_probs, top2_idx = torch.topk(probs, k=2)\n                        \n                        # Require minimum confidence for second speaker\n                        if top2_probs[1] > 0.3:\n                            activities[b, f, top2_idx] = 1.0\n        \n        # Step 3: Remove low-confidence overlaps\n        # If overlap detected but overlap_prob is low, keep only strongest speaker\n        low_confidence_overlap = (num_active >= 2) & (overlap_probs < 0.4)\n        \n        if low_confidence_overlap.any():\n            for b in range(activities.shape[0]):\n                for f in range(activities.shape[1]):\n                    if low_confidence_overlap[b, f]:\n                        # Find strongest speaker\n                        probs = speaker_probs[b, f]\n                        active_mask = activities[b, f] > 0\n                        \n                        if active_mask.sum() >= 2:\n                            # Keep only strongest\n                            strongest_idx = (probs * active_mask).argmax()\n                            activities[b, f, :] = 0\n                            activities[b, f, strongest_idx] = 1\n        \n        return activities\n    \n    def load_phase2_weights(self, checkpoint_path: str):\n        \"\"\"Load Phase 2 weights\"\"\"\n        print(f\"\\n📦 Loading Phase 2 checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        \n        phase2_state = checkpoint['model_state_dict']\n        model_state = {}\n        \n        for key, value in phase2_state.items():\n            if 'decoder.output_proj.3' in key:\n                if value.shape[0] != self.num_speakers:\n                    print(f\"⚠️  Skipping {key}\")\n                    continue\n            \n            if key.startswith(('encoder.', 'decoder.', 'contrastive_head.')):\n                model_state[key] = value\n        \n        missing, unexpected = self.load_state_dict(model_state, strict=False)\n        print(\"✓ Phase 2 weights loaded (overlap_detector randomly initialized)\")\n\n# ============================================================================\n# PHASE 3: CHUNK-WISE DATASET\n# ============================================================================\n\ndef build_audio_rttm_mapping(audio_dir: str, rttm_dir: str) -> List[Tuple[Path, Path]]:\n    \"\"\"Build mapping - works for both CallHome and VoxConverse\"\"\"\n    audio_dir = Path(audio_dir)\n    rttm_dir = Path(rttm_dir)\n    \n    # Try different extensions\n    audio_files = sorted(audio_dir.glob('*.wav'))\n    if len(audio_files) == 0:\n        audio_files = sorted(audio_dir.glob('*.flac'))\n    if len(audio_files) == 0:\n        audio_files = sorted(audio_dir.glob('*.mp3'))\n    \n    rttm_files = sorted(rttm_dir.glob('*.rttm'))\n    \n    print(f\"\\nFound {len(audio_files)} audio files\")\n    print(f\"Found {len(rttm_files)} RTTM files\")\n    \n    # Build mapping by filename (not index!)\n    audio_map = {}\n    for audio_file in audio_files:\n        # Use stem (filename without extension) as key\n        key = audio_file.stem\n        audio_map[key] = audio_file\n    \n    rttm_map = {}\n    for rttm_file in rttm_files:\n        key = rttm_file.stem\n        rttm_map[key] = rttm_file\n    \n    # Match by filename\n    pairs = []\n    for key in sorted(audio_map.keys()):\n        if key in rttm_map:\n            pairs.append((audio_map[key], rttm_map[key]))\n        else:\n            print(f\"⚠️  No RTTM for audio: {key}\")\n    \n    print(f\"Matched {len(pairs)} audio-RTTM pairs\\n\")\n    return pairs\n\n\n# --- Audio-style data augmentation for EEND ---\nclass ImprovedFeatureAugmentor:\n    \"\"\"\n    Diarization-friendly augmentation:\n    - No pitch/time changes (preserves speaker identity)\n    - Focus on noise robustness\n    - Preserves temporal alignment with labels\n    \"\"\"\n    def __init__(self, noise_std=0.002, spec_augment_prob=0.3):\n        self.noise_std = noise_std\n        self.spec_augment_prob = spec_augment_prob\n    \n    def add_gaussian_noise(self, mel_spec):\n        \"\"\"Add light Gaussian noise to mel spectrogram\"\"\"\n        noise = torch.randn_like(mel_spec) * self.noise_std\n        return mel_spec + noise\n    \n    def spec_augment(self, mel_spec):\n        \"\"\"\n        SpecAugment: mask time/frequency bands\n        Keeps temporal structure intact\n        \"\"\"\n        n_mels, time_steps = mel_spec.shape\n        \n        # Frequency masking (mask mel bins)\n        if random.random() < 0.5:\n            f_mask_width = random.randint(1, max(1, n_mels // 10))\n            f_mask_start = random.randint(0, n_mels - f_mask_width)\n            mel_spec[f_mask_start:f_mask_start + f_mask_width, :] = 0\n        \n        # Time masking (mask short segments)\n        if random.random() < 0.5:\n            t_mask_width = random.randint(1, max(1, time_steps // 20))\n            t_mask_start = random.randint(0, time_steps - t_mask_width)\n            mel_spec[:, t_mask_start:t_mask_start + t_mask_width] = 0\n        \n        return mel_spec\n    \n    def __call__(self, mel_spec, apply=True):\n        \"\"\"\n        Apply augmentation to mel spectrogram\n        \n        Args:\n            mel_spec: torch.Tensor (n_mels, time)\n            apply: bool - whether to apply augmentation\n        \"\"\"\n        if not apply:\n            return mel_spec\n        \n        # Light Gaussian noise (always apply for robustness)\n        mel_spec = self.add_gaussian_noise(mel_spec)\n        \n        # SpecAugment (probabilistic)\n        if random.random() < self.spec_augment_prob:\n            mel_spec = self.spec_augment(mel_spec)\n        \n        return mel_spec\n        \n\nclass ChunkWiseDataset(Dataset):\n    \"\"\"\n    Dataset for chunk-wise processing (Phase 3)\n    Processes long conversations in overlapping chunks\n    \"\"\"\n    def __init__(self, audio_dir: str, rttm_dir: str, audio_processor: AudioProcessor,\n                 chunk_size: float = 30.0, overlap: float = 5.0,\n                 sample_rate: int = 16000, max_speakers: int = 20, augment: bool = True):\n        \n        self.audio_dir = Path(audio_dir)\n        self.rttm_dir = Path(rttm_dir)\n        self.audio_processor = audio_processor\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.max_speakers = max_speakers\n        self.augment = augment\n\n        #Augmentation\n        if augment:\n            self.augmentor = ImprovedFeatureAugmentor(\n                noise_std=0.002,\n                spec_augment_prob=0.3\n            )\n        \n        # Chunk stride (non-overlapping portion)\n        self.stride = chunk_size - overlap\n        \n        # Build mapping\n        self.audio_rttm_pairs = build_audio_rttm_mapping(audio_dir, rttm_dir)\n        \n        # Parse into chunks\n        self.chunks = self._create_chunks()\n        \n        print(f\"✓ Created {len(self.chunks)} chunks from {len(self.audio_rttm_pairs)} conversations\")\n        print(f\"  Chunk size: {chunk_size}s, Overlap: {overlap}s, Stride: {self.stride}s\")\n    \n    def _create_chunks(self) -> List[Dict]:\n        \"\"\"Create overlapping chunks from full conversations\"\"\"\n        chunks = []\n        \n        for audio_path, rttm_path in tqdm(self.audio_rttm_pairs, desc=\"Creating chunks\"):\n            # Get audio duration\n            info = torchaudio.info(str(audio_path))\n            duration = info.num_frames / info.sample_rate\n            \n            # Parse RTTM\n            speaker_segments = {}\n            with open(rttm_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) < 8 or parts[0] != 'SPEAKER':\n                        continue\n                    \n                    start = float(parts[3])\n                    dur = float(parts[4])\n                    speaker = parts[7]\n                    \n                    if speaker not in speaker_segments:\n                        speaker_segments[speaker] = []\n                    \n                    speaker_segments[speaker].append({\n                        'start': start,\n                        'end': start + dur\n                    })\n            \n            if len(speaker_segments) == 0:\n                continue\n            \n            # Create overlapping chunks\n            chunk_start = 0.0\n            chunk_idx = 0\n            \n            while chunk_start < duration:\n                chunk_end = min(chunk_start + self.chunk_size, duration)\n                \n                # Only add chunk if it's long enough\n                if chunk_end - chunk_start >= self.chunk_size * 0.5:\n                    chunks.append({\n                        'audio_path': audio_path,\n                        'rttm_path': rttm_path,\n                        'chunk_start': chunk_start,\n                        'chunk_end': chunk_end,\n                        'speaker_segments': speaker_segments,\n                        'conversation_id': audio_path.stem,\n                        'chunk_idx': chunk_idx,\n                        'total_duration': duration\n                    })\n                    chunk_idx += 1\n                \n                # Move to next chunk\n                chunk_start += self.stride\n                \n                # Stop if we've covered the whole audio\n                if chunk_end >= duration:\n                    break\n        \n        return chunks\n    \n    def _create_label_tensor(self, speaker_segments: Dict, chunk_start: float,\n                            chunk_end: float, num_frames: int) -> torch.Tensor:\n        \"\"\"Create frame-level labels for chunk\"\"\"\n        labels = torch.zeros(num_frames, self.max_speakers)\n        \n        speaker_list = sorted(speaker_segments.keys())[:self.max_speakers]\n        frame_duration = (chunk_end - chunk_start) / num_frames\n        \n        for spk_idx, speaker in enumerate(speaker_list):\n            segments = speaker_segments[speaker]\n            \n            for segment in segments:\n                seg_start = segment['start']\n                seg_end = segment['end']\n                \n                # Check overlap with chunk\n                overlap_start = max(seg_start, chunk_start)\n                overlap_end = min(seg_end, chunk_end)\n                \n                if overlap_start < overlap_end:\n                    # Convert to frame indices\n                    frame_start = int((overlap_start - chunk_start) / frame_duration)\n                    frame_end = int((overlap_end - chunk_start) / frame_duration)\n                    \n                    frame_start = max(0, min(frame_start, num_frames - 1))\n                    frame_end = max(0, min(frame_end, num_frames))\n                    \n                    labels[frame_start:frame_end, spk_idx] = 1.0\n        \n        return labels\n    \n    def __len__(self) -> int:\n        return len(self.chunks)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n        \"\"\"\n        Returns:\n            mel: (n_mels, frames)\n            labels: (encoder_frames, max_speakers)\n            metadata: Dict with chunk info\n        \"\"\"\n        chunk = self.chunks[idx]\n        \n        # Load audio chunk\n        start_frame = int(chunk['chunk_start'] * self.sample_rate)\n        num_frames = int((chunk['chunk_end'] - chunk['chunk_start']) * self.sample_rate)\n        \n        waveform, sr = torchaudio.load(\n            str(chunk['audio_path']),\n            frame_offset=start_frame,\n            num_frames=num_frames\n        )\n        \n        # Resample if needed\n        if sr != self.sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n            waveform = resampler(waveform)\n        \n        # Ensure correct length\n        if waveform.shape[1] < num_frames:\n            waveform = F.pad(waveform, (0, num_frames - waveform.shape[1]))\n        elif waveform.shape[1] > num_frames:\n            waveform = waveform[:, :num_frames]\n        \n        # Convert to mel\n        mel = self.audio_processor(waveform.squeeze(0))\n        \n        # Apply augmentation if in training mode\n        if self.augment:\n            mel = self.augmentor(mel, apply=True)\n        \n        mel_frames = mel.shape[1]\n        encoder_frames = ConformerEncoder.compute_output_frames_static(mel_frames)\n        \n        labels = self._create_label_tensor(\n            chunk['speaker_segments'],\n            chunk['chunk_start'],\n            chunk['chunk_end'],\n            encoder_frames\n        )\n        \n        metadata = {\n            'conversation_id': chunk['conversation_id'],\n            'chunk_idx': chunk['chunk_idx'],\n            'chunk_start': chunk['chunk_start'],\n            'chunk_end': chunk['chunk_end']\n        }\n        \n        return mel, labels, metadata\n\n\n# ============================================================================\n# PHASE 3: LOSS FUNCTIONS WITH HARD NEGATIVE MINING\n# ============================================================================\n\nclass ImprovedOverlapWeightedPITLoss(nn.Module):\n    \"\"\"Enhanced PIT loss with focal loss and higher overlap weighting\"\"\"\n    def __init__(self, overlap_weight: float = 10.0, focal_gamma: float = 2.0):\n        super().__init__()\n        self.overlap_weight = overlap_weight\n        self.focal_gamma = focal_gamma\n        self._cached_perms = None\n    \n    def focal_bce_loss(self, pred_logits, target):\n        \"\"\"Focal loss to focus on hard examples\"\"\"\n        bce = F.binary_cross_entropy_with_logits(pred_logits, target, reduction='none')\n        \n        pred_probs = torch.sigmoid(pred_logits)\n        pt = target * pred_probs + (1 - target) * (1 - pred_probs)\n        focal_weight = (1 - pt) ** self.focal_gamma\n        \n        return focal_weight * bce\n    \n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        batch_size, frames, num_speakers = pred.shape\n        \n        overlap_mask = (target.sum(dim=2) > 1).float()\n        \n        weights = torch.ones(batch_size, frames, 1, device=pred.device)\n        weights[overlap_mask > 0] = self.overlap_weight\n        \n        if self._cached_perms is None or len(self._cached_perms[0]) != num_speakers:\n            self._cached_perms = list(permutations(range(num_speakers)))\n        \n        min_loss = float('inf')\n        \n        for perm in self._cached_perms:\n            pred_perm = pred[:, :, list(perm)]\n            loss_per_element = self.focal_bce_loss(pred_perm, target)\n            loss = (loss_per_element * weights).mean()\n            \n            if loss < min_loss:\n                min_loss = loss\n                if min_loss < 0.01:\n                    break\n        \n        return min_loss\n\n\n\nclass BalancedOverlapAwareLoss(nn.Module):\n    \"\"\"\n    Properly balanced loss that handles:\n    1. Base speaker detection\n    2. Overlap detection\n    3. Silence frames\n    \"\"\"\n    def __init__(self, overlap_weight: float = 6.0, \n                 overlap_detection_weight: float = 1.5,\n                 focal_gamma: float = 2.0):\n        super().__init__()\n        self.overlap_weight = overlap_weight\n        self.overlap_detection_weight = overlap_detection_weight\n        self.focal_gamma = focal_gamma\n        self._cached_perms = None\n    \n    def focal_bce_loss(self, pred_logits, target):\n        \"\"\"Focal loss - focuses on hard examples\"\"\"\n        bce = F.binary_cross_entropy_with_logits(pred_logits, target, reduction='none')\n        \n        pred_probs = torch.sigmoid(pred_logits)\n        pt = target * pred_probs + (1 - target) * (1 - pred_probs)\n        focal_weight = (1 - pt) ** self.focal_gamma\n        \n        return focal_weight * bce\n    \n    def forward(self, speaker_logits, overlap_logits, labels):\n        \"\"\"\n        Args:\n            speaker_logits: (batch, frames, num_speakers)\n            overlap_logits: (batch, frames, 1)\n            labels: (batch, frames, num_speakers)\n        \"\"\"\n        batch_size, frames, num_speakers = speaker_logits.shape\n        \n        # === 1. Speaker Diarization Loss (PIT with overlap weighting) ===\n        \n        # Detect overlap and silence frames\n        num_speakers_active = labels.sum(dim=2)\n        overlap_mask = (num_speakers_active > 1).float()  # >1 speaker\n        silence_mask = (num_speakers_active == 0).float()  # 0 speakers\n        \n        # Create frame weights\n        weights = torch.ones(batch_size, frames, 1, device=speaker_logits.device)\n        weights[overlap_mask > 0] = self.overlap_weight      # Emphasize overlap\n        weights[silence_mask > 0] = 1.5                       # Slightly emphasize silence (reduce FA)\n        \n        # PIT loss\n        if self._cached_perms is None or len(self._cached_perms[0]) != num_speakers:\n            self._cached_perms = list(permutations(range(num_speakers)))\n        \n        min_pit_loss = float('inf')\n        \n        for perm in self._cached_perms:\n            speaker_logits_perm = speaker_logits[:, :, list(perm)]\n            \n            # Focal BCE for hard examples\n            loss_per_element = self.focal_bce_loss(speaker_logits_perm, labels)\n            loss = (loss_per_element * weights).mean()\n            \n            if loss < min_pit_loss:\n                min_pit_loss = loss\n                if min_pit_loss < 0.01:\n                    break\n        \n        # === 2. Overlap Detection Loss ===\n        \n        overlap_labels = (num_speakers_active > 1).float().unsqueeze(-1)\n        \n        # Balanced pos_weight (not too high)\n        pos_weight = torch.tensor([3.0], device=overlap_logits.device)\n        \n        overlap_loss = F.binary_cross_entropy_with_logits(\n            overlap_logits, \n            overlap_labels,\n            pos_weight=pos_weight\n        )\n        \n        # === 3. Combined Loss ===\n        \n        total_loss = min_pit_loss + self.overlap_detection_weight * overlap_loss\n        \n        return total_loss, min_pit_loss, overlap_loss\n\n\nclass HardNegativeContrastiveLoss(nn.Module):\n    \"\"\"\n    Enhanced contrastive loss with hard negative mining (Phase 3)\n    Mines hard negatives from memory bank\n    \"\"\"\n    def __init__(self, temperature: float = 0.1, max_samples: int = 500,\n                 hard_negative_ratio: float = 0.3):\n        super().__init__()\n        self.temperature = temperature\n        self.max_samples = max_samples\n        self.hard_negative_ratio = hard_negative_ratio\n    \n    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor,\n            memory_prototypes: Optional[torch.Tensor] = None,\n            contrastive_head: Optional[nn.Module] = None) -> torch.Tensor:  #  Add constrastive head parameter\n        \"\"\"\n        Args:\n            embeddings: (batch, frames, projection_dim) - L2 normalized\n            labels: (batch, frames, num_speakers) - binary\n            memory_prototypes: (num_speakers, projection_dim) - speaker prototypes from memory\n        \"\"\"\n        batch_size, frames, proj_dim = embeddings.shape\n        num_speakers = labels.shape[2]\n        \n        total_loss = 0\n        valid_speakers = 0\n        \n        for spk in range(num_speakers):\n            spk_labels = labels[:, :, spk]\n            active_mask = spk_labels > 0.5\n            \n            if active_mask.sum() < 2:\n                continue\n            \n            active_embeddings = embeddings[active_mask]\n            \n            # Sample if too many\n            if len(active_embeddings) > self.max_samples:\n                indices = torch.randperm(len(active_embeddings), device=embeddings.device)[:self.max_samples]\n                active_embeddings = active_embeddings[indices]\n            \n            if len(active_embeddings) < 2:\n                continue\n            \n            # Compute similarity matrix\n            sim_matrix = torch.mm(active_embeddings, active_embeddings.T) / self.temperature\n            \n            # Add hard negatives from memory if available\n            if memory_prototypes is not None and spk < len(memory_prototypes):\n                # Get memory prototype for OTHER speakers (hard negatives)\n                other_speaker_protos = []\n                for other_spk in range(num_speakers):\n                    if other_spk != spk:\n                        other_speaker_protos.append(memory_prototypes[other_spk])\n                \n                if len(other_speaker_protos) > 0:\n                    other_protos = torch.stack(other_speaker_protos)  # (n_others, d_model=128)\n                    \n                    if contrastive_head is not None:\n                        other_protos = contrastive_head.projection(other_protos)  # (n_others, 64)\n                        other_protos = F.normalize(other_protos, p=2, dim=-1)\n                                \n                    # Similarity to hard negatives\n                    hard_neg_sim = torch.mm(active_embeddings, other_protos.T) / self.temperature\n                    \n                    # Select hardest negatives (highest similarity to wrong speakers)\n                    n_hard = int(len(active_embeddings) * self.hard_negative_ratio)\n                    if n_hard > 0:\n                        hardest_sim, _ = hard_neg_sim.max(dim=1)  # Max similarity to wrong speaker\n                        hard_indices = torch.topk(hardest_sim, k=min(n_hard, len(hardest_sim)))[1]\n                        \n                        # Weight hard negatives more\n                        weights = torch.ones(len(active_embeddings), device=embeddings.device)\n                        weights[hard_indices] *= 2.0  # Double weight for hard negatives\n                    else:\n                        weights = torch.ones(len(active_embeddings), device=embeddings.device)\n                else:\n                    weights = torch.ones(len(active_embeddings), device=embeddings.device)\n            else:\n                weights = torch.ones(len(active_embeddings), device=embeddings.device)\n            \n            # Mask diagonal\n            mask = torch.ones_like(sim_matrix)\n            mask.fill_diagonal_(0)\n            \n            # InfoNCE loss with weights\n            exp_sim = torch.exp(sim_matrix) * mask\n            log_prob = sim_matrix - torch.log(exp_sim.sum(1, keepdim=True) + 1e-8)\n            loss = -(mask * log_prob).sum(1) / mask.sum(1)\n            \n            # Apply weights\n            weighted_loss = (loss * weights).sum() / weights.sum()\n            \n            total_loss += weighted_loss\n            valid_speakers += 1\n        \n        if valid_speakers == 0:\n            return torch.tensor(0.0, device=embeddings.device)\n        \n        return total_loss / valid_speakers\n\n\nclass MemoryConsistencyLoss(nn.Module):\n    \"\"\"\n    NEW: Memory consistency loss\n    Encourages consistent speaker representations across chunks\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, speaker_embeddings: torch.Tensor,\n                memory_prototypes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            speaker_embeddings: (batch, num_speakers, d_model)\n            memory_prototypes: (num_speakers, d_model)\n        \n        Returns:\n            loss: scalar - encourages current embeddings to match memory\n        \"\"\"\n        # Normalize\n        speaker_embeddings = F.normalize(speaker_embeddings, p=2, dim=-1)\n        memory_prototypes = F.normalize(memory_prototypes, p=2, dim=-1)\n        \n        # Cosine similarity\n        similarity = torch.mm(\n            speaker_embeddings.reshape(-1, speaker_embeddings.shape[-1]),\n            memory_prototypes.T\n        )\n        \n        # We want diagonal to be high (each speaker matches its prototype)\n        batch_size = speaker_embeddings.shape[0]\n        num_speakers = speaker_embeddings.shape[1]\n        \n        # Create target: identity matrix repeated for batch\n        target = torch.eye(num_speakers, device=speaker_embeddings.device)\n        target = target.unsqueeze(0).repeat(batch_size, 1, 1).reshape(-1, num_speakers)\n        \n        # Cross-entropy loss\n        log_sim = F.log_softmax(similarity, dim=1)\n        loss = -(target * log_sim).sum(1).mean()\n        \n        return loss\n        \n# ============================================================================\n# PHASE 3: TRAINER\n# ============================================================================\n\nclass ImprovedPhase3Trainer:\n    \"\"\"Enhanced trainer with CSV logging and better monitoring\"\"\"\n    def __init__(self, model: nn.Module, train_loader: DataLoader,\n                 val_loader: DataLoader, device: str = 'cuda',\n                 learning_rate: float = 5e-5, weight_decay: float = 1e-4,\n                 contrastive_weight: float = 0.02,  # Reduced from 0.05\n                 log_dir: str = './logs'):\n        \n        # Normalize device to torch.device\n        self.device = torch.device(device) if isinstance(device, str) else device\n        self.model = model.to(self.device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.contrastive_weight = contrastive_weight\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Create CSV log file\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.csv_path = self.log_dir / f'training_log_{timestamp}.csv'\n        self._init_csv()\n        \n        self.overlap_aware_loss = BalancedOverlapAwareLoss(\n            overlap_weight=6.0,\n            overlap_detection_weight=1.5,\n            focal_gamma=2.0\n        )\n        self.contrastive_loss = HardNegativeContrastiveLoss(temperature=0.1)\n\n        self.threshold_predictor = ImprovedAdaptiveThresholdPredictor(\n            num_speakers=model.num_speakers\n        )\n        \n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.98)\n        )\n        \n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=30,\n            eta_min=1e-6\n        )\n        \n        self.use_amp = (self.device.type == 'cuda')\n        self.scaler = torch.amp.GradScaler('cuda') if self.use_amp else None\n        \n        self.best_val_loss = float('inf')\n        self.best_der = float('inf')  # Track best DER\n        self.patience = 10\n        self.patience_counter = 0\n\n    def compute_jer(self, predictions: torch.Tensor, targets: torch.Tensor, \n                   mask: Optional[torch.Tensor] = None) -> float:\n        \"\"\"\n        Compute Jaccard Error Rate (micro-averaged)\n        \n        Args:\n            predictions: (batch, frames, num_speakers) - binary or probabilities\n            targets: (batch, frames, num_speakers) - binary ground truth\n            mask: (batch, frames) - valid frame mask\n        \n        Returns:\n            jer: Jaccard Error Rate (0-1, lower is better)\n        \"\"\"\n        # Binarize predictions\n        pred_binary = (predictions > 0.5).float()\n        \n        # Apply mask if provided\n        if mask is not None:\n            mask_expanded = mask.unsqueeze(-1).expand_as(pred_binary)\n            pred_binary = pred_binary * mask_expanded.float()\n            targets = targets * mask_expanded.float()\n        \n        # Compute intersection and union per frame\n        intersection = (pred_binary * targets).sum()\n        union = ((pred_binary + targets) > 0).float().sum()\n        \n        # Jaccard index\n        jaccard = intersection / (union + 1e-8)\n        jer = 1.0 - jaccard.item()\n        \n        return jer\n    \n    def compute_overlap_metrics(self, predictions: torch.Tensor, targets: torch.Tensor,\n                                mask: Optional[torch.Tensor] = None) -> Dict[str, float]:\n        \"\"\"\n        Compute overlap detection metrics (precision, recall, F1)\n        \n        Args:\n            predictions: (batch, frames, num_speakers) - probabilities\n            targets: (batch, frames, num_speakers) - binary\n            mask: (batch, frames) - valid frames\n        \n        Returns:\n            metrics: dict with overlap_precision, overlap_recall, overlap_f1\n        \"\"\"\n        pred_binary = (predictions > 0.5).float()\n        \n        # Detect overlapping frames (>1 speaker active)\n        overlap_pred = (pred_binary.sum(dim=-1) > 1).float()  # (batch, frames)\n        overlap_true = (targets.sum(dim=-1) > 1).float()\n        \n        # Apply mask\n        if mask is not None:\n            overlap_pred = overlap_pred * mask.float()\n            overlap_true = overlap_true * mask.float()\n        \n        # Compute TP, FP, FN\n        TP = ((overlap_pred == 1) & (overlap_true == 1)).sum().item()\n        FP = ((overlap_pred == 1) & (overlap_true == 0)).sum().item()\n        FN = ((overlap_pred == 0) & (overlap_true == 1)).sum().item()\n        \n        # Precision, Recall, F1\n        precision = TP / (TP + FP + 1e-8)\n        recall = TP / (TP + FN + 1e-8)\n        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n        \n        return {\n            'overlap_precision': precision,\n            'overlap_recall': recall,\n            'overlap_f1': f1\n        }\n    \n    def compute_fa_miss_rates(self, predictions: torch.Tensor, targets: torch.Tensor,\n                              mask: Optional[torch.Tensor] = None) -> Dict[str, float]:\n        \"\"\"\n        Compute False Alarm and Miss rates\n        \n        Args:\n            predictions: (batch, frames, num_speakers)\n            targets: (batch, frames, num_speakers)\n            mask: (batch, frames)\n        \n        Returns:\n            metrics: dict with fa_rate, miss_rate\n        \"\"\"\n        pred_binary = (predictions > 0.5).float()\n        \n        # Speech activity (any speaker active)\n        speech_pred = (pred_binary.sum(dim=-1) > 0).float()  # (batch, frames)\n        speech_true = (targets.sum(dim=-1) > 0).float()\n        \n        # Apply mask\n        if mask is not None:\n            speech_pred = speech_pred * mask.float()\n            speech_true = speech_true * mask.float()\n        \n        # False Alarm: predicted speech when there's silence\n        FA = ((speech_pred == 1) & (speech_true == 0)).sum().item()\n        \n        # Miss: silence predicted when there's speech\n        Miss = ((speech_pred == 0) & (speech_true == 1)).sum().item()\n        \n        # Total frames\n        total_silence = (speech_true == 0).sum().item()\n        total_speech = (speech_true == 1).sum().item()\n        \n        fa_rate = FA / (total_silence + 1e-8)\n        miss_rate = Miss / (total_speech + 1e-8)\n        \n        return {\n            'fa_rate': fa_rate,\n            'miss_rate': miss_rate\n        }\n    \n    def compute_model_size(self) -> Dict[str, float]:\n        \"\"\"\n        Compute model size breakdown\n        \n        Returns:\n            metrics: dict with parameter counts and checkpoint size\n        \"\"\"\n        # Total parameters\n        total_params = sum(p.numel() for p in self.model.parameters())\n        \n        # Encoder parameters\n        encoder_params = sum(p.numel() for p in self.model.encoder.parameters())\n        \n        # Decoder parameters\n        decoder_params = sum(p.numel() for p in self.model.decoder.parameters())\n        \n        # Checkpoint size (if exists)\n        checkpoint_size_mb = 0.0\n        checkpoint_dir = Path('./checkpoints_phase3_simple')\n        if checkpoint_dir.exists():\n            checkpoints = list(checkpoint_dir.glob('*.pth'))\n            if checkpoints:\n                # Get size of latest checkpoint\n                latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)\n                checkpoint_size_mb = latest_checkpoint.stat().st_size / (1024 ** 2)\n        \n        return {\n            'model_params_millions': total_params / 1e6,\n            'encoder_params_millions': encoder_params / 1e6,\n            'decoder_params_millions': decoder_params / 1e6,\n            'checkpoint_size_mb': checkpoint_size_mb\n        }\n    \n    def measure_inference_time(self, num_samples: int = 5) -> Dict[str, float]:\n        \"\"\"\n        Measure inference time on random validation samples\n        \n        Args:\n            num_samples: Number of samples to average over\n        \n        Returns:\n            metrics: dict with timing info and RTF\n        \"\"\"\n        self.model.eval()\n        \n        mel_times = []\n        model_times = []\n        audio_durations = []\n        \n        # Sample random batches\n        sample_indices = random.sample(range(len(self.val_loader.dataset)), \n                                      min(num_samples, len(self.val_loader.dataset)))\n        \n        with torch.no_grad():\n            for idx in sample_indices:\n                # Get sample\n                mel, labels, metadata = self.val_loader.dataset[idx]\n                \n                # Simulate mel extraction time (already done, so estimate)\n                # Approximate: 160 hop_length, 16000 sample_rate\n                audio_duration = mel.shape[-1] * 160 / 16000  # seconds\n                audio_durations.append(audio_duration)\n                \n                # Mel extraction time (estimated, ~1% of audio duration for efficient implementation)\n                mel_time = audio_duration * 0.01\n                mel_times.append(mel_time)\n                \n                # Move to device and add batch dim\n                mel = mel.unsqueeze(0).to(self.device)\n                \n                # Measure model inference time\n                torch.cuda.synchronize() if self.device.type == 'cuda' else None\n                start = time.time()\n                \n                with torch.amp.autocast('cuda', enabled=self.use_amp):\n                    _ = self.model.predict(mel)\n                \n                torch.cuda.synchronize() if self.device.type == 'cuda' else None\n                model_time = time.time() - start\n                model_times.append(model_time)\n        \n        # Compute averages\n        avg_mel_time = np.mean(mel_times)\n        avg_model_time = np.mean(model_times)\n        avg_total_time = avg_mel_time + avg_model_time\n        avg_audio_duration = np.mean(audio_durations)\n        \n        # Real-Time Factor\n        rtf = avg_total_time / avg_audio_duration\n        \n        return {\n            'inference_time_mel_sec': avg_mel_time,\n            'inference_time_model_sec': avg_model_time,\n            'inference_time_total_sec': avg_total_time,\n            'rtf': rtf\n        }\n    \n    def _init_csv(self):\n        \"\"\"Initialize CSV file with headers\"\"\"\n        with open(self.csv_path, 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow([\n                'epoch', 'phase',\n                'pit_loss', 'overlap_loss','contrast_loss', 'total_loss',\n                'der', 'jer', 'fa_rate', 'miss_rate',\n                'overlap_precision', 'overlap_recall', 'overlap_f1',\n                'inference_time_mel_sec', 'inference_time_model_sec', \n                'inference_time_total_sec', 'rtf',\n                'model_params_millions', 'encoder_params_millions', \n                'decoder_params_millions', 'checkpoint_size_mb',\n                'learning_rate', 'timestamp'\n            ])\n        print(f\"✓ CSV log created: {self.csv_path}\")\n    \n    def _log_to_csv(self, epoch: int, phase: str, metrics: Dict):\n        \"\"\"Log metrics to CSV\"\"\"\n        with open(self.csv_path, 'a', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow([\n                epoch,\n                phase,\n                f\"{metrics.get('pit_loss', 0):.6f}\",\n                f\"{metrics.get('contrast_loss', 0):.6f}\",\n                f\"{metrics.get('total_loss', 0):.6f}\",\n                f\"{metrics.get('der', 0):.4f}\",\n                f\"{metrics.get('jer', 0):.4f}\",\n                f\"{metrics.get('fa_rate', 0):.4f}\",\n                f\"{metrics.get('miss_rate', 0):.4f}\",\n                f\"{metrics.get('overlap_precision', 0):.4f}\",\n                f\"{metrics.get('overlap_recall', 0):.4f}\",\n                f\"{metrics.get('overlap_f1', 0):.4f}\",\n                f\"{metrics.get('inference_time_mel_sec', 0):.4f}\",\n                f\"{metrics.get('inference_time_model_sec', 0):.4f}\",\n                f\"{metrics.get('inference_time_total_sec', 0):.4f}\",\n                f\"{metrics.get('rtf', 0):.4f}\",\n                f\"{metrics.get('model_params_millions', 0):.2f}\",\n                f\"{metrics.get('encoder_params_millions', 0):.2f}\",\n                f\"{metrics.get('decoder_params_millions', 0):.2f}\",\n                f\"{metrics.get('checkpoint_size_mb', 0):.2f}\",\n                f\"{metrics.get('learning_rate', 0):.8f}\",\n                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            ])\n    \n    def train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"Train one epoch\"\"\"\n        self.model.train()\n        total_pit_loss = 0\n        total_overlap_loss = 0\n        total_contrast_loss = 0\n        total_loss = 0\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n        \n        for batch_idx, (mel, labels, metadata) in enumerate(pbar):\n            mel = mel.to(self.device)\n            labels = labels.to(self.device)\n            \n            with torch.amp.autocast('cuda', enabled=self.use_amp):\n                logits, overlap_logits, embeddings = self.model(mel, return_embeddings=True)\n                \n                loss, pit_loss, overlap_loss = self.overlap_aware_loss(\n                    logits, overlap_logits, labels\n                )\n                \n                contrast_loss = self.contrastive_loss(\n                    embeddings, labels, \n                    memory_prototypes=None,\n                    contrastive_head=self.model.contrastive_head\n                )\n                \n                total = loss + self.contrastive_weight * contrast_loss\n            \n            self.optimizer.zero_grad()\n            if self.use_amp:\n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                self.optimizer.step()\n            \n            total_pit_loss += pit_loss.item()\n            total_overlap_loss += overlap_loss.item()\n            total_contrast_loss += contrast_loss.item()\n            total_loss += total.item()  # Changed from 'loss' to 'total'\n            \n            pbar.set_postfix({\n                'pit': f'{pit_loss.item():.4f}',\n                'ovlp': f'{overlap_loss.item():.4f}',\n                'cont': f'{contrast_loss.item():.4f}',\n                'total': f'{loss.item():.4f}'\n            })\n        \n        n = len(self.train_loader)\n        metrics = {\n            'pit_loss': total_pit_loss / n,\n            'overlap_loss': total_overlap_loss / n,\n            'contrast_loss': total_contrast_loss / n,\n            'total_loss': total_loss / n,\n            'learning_rate': self.optimizer.param_groups[0]['lr']\n        }\n        \n        # Log to CSV\n        self._log_to_csv(epoch, 'train', metrics)\n        \n        return metrics\n    \n    def validate(self, epoch: int) -> Dict[str, float]:\n        \"\"\"Validate with comprehensive metrics\"\"\"\n        if epoch == 1 or epoch % 3 == 0:\n            self.threshold_predictor.calibrate(\n                self.model, self.val_loader, self.device\n            )\n        \n        self.model.eval()\n        total_pit_loss = 0\n        total_overlap_loss = 0\n        total_contrast_loss = 0\n        total_loss = 0\n        \n        # Accumulators for metrics\n        all_predictions = []\n        all_targets = []\n        all_masks = []\n        \n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n            \n            for mel, labels, metadata in pbar:\n                mel = mel.to(self.device)\n                labels = labels.to(self.device)\n                \n                with torch.amp.autocast('cuda', enabled=self.use_amp):\n                    logits, overlap_logits, embeddings = self.model(mel, return_embeddings=True)\n                    \n                    loss, pit_loss, overlap_loss = self.overlap_aware_loss(\n                        logits, overlap_logits, labels\n                    )\n                    contrast_loss = self.contrastive_loss(\n                        embeddings, labels,\n                        memory_prototypes=None,\n                        contrastive_head=self.model.contrastive_head\n                    )\n                    \n                    loss = pit_loss + self.contrastive_weight * contrast_loss\n                    \n                    # Get predictions\n                    predictions = self.model.predict(mel, self.threshold_predictor)\n                \n                total_pit_loss += pit_loss.item()\n                total_contrast_loss += contrast_loss.item()\n                total_loss += loss.item()\n                \n                # Store for metric calculation\n                all_predictions.append(predictions.cpu())\n                all_targets.append(labels.cpu())\n                # Create mask (all valid for now)\n                mask = torch.ones(labels.shape[0], labels.shape[1], dtype=torch.bool)\n                all_masks.append(mask)\n                \n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        n = len(self.val_loader)\n        \n        # Concatenate all batches\n        all_predictions = torch.cat(all_predictions, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n        all_masks = torch.cat(all_masks, dim=0)\n        \n        # Compute additional metrics\n        jer = self.compute_jer(all_predictions, all_targets, all_masks)\n        overlap_metrics = self.compute_overlap_metrics(all_predictions, all_targets, all_masks)\n        fa_miss_metrics = self.compute_fa_miss_rates(all_predictions, all_targets, all_masks)\n        \n        metrics = {\n            'pit_loss': total_pit_loss / n,\n            'contrast_loss': total_contrast_loss / n,\n            'total_loss': total_loss / n,\n            'learning_rate': self.optimizer.param_groups[0]['lr'],\n            'jer': jer,\n            **overlap_metrics,\n            **fa_miss_metrics\n        }\n        \n        return metrics\n    \n    def compute_der(self, loader: DataLoader) -> float:\n        \"\"\"Compute DER\"\"\"\n        self.model.eval()\n        total_frames = 0\n        error_frames = 0\n        \n        with torch.no_grad():\n            for mel, labels, metadata in loader:\n                mel = mel.to(self.device)\n                labels = labels.to(self.device)\n                \n                with torch.amp.autocast('cuda', enabled=self.use_amp):\n                    activities = self.model.predict(mel)\n                \n                pred_binary = (activities > 0.5).float()\n                errors = torch.abs(pred_binary - labels).sum()\n                frames = labels.numel()\n                \n                error_frames += errors.item()\n                total_frames += frames\n        \n        der = (error_frames / total_frames) * 100 if total_frames > 0 else 0\n        return der\n    \n    def save_checkpoint(self, epoch: int, metrics: Dict, filepath: str):\n        \"\"\"Save checkpoint\"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'metrics': metrics,\n            'best_val_loss': self.best_val_loss,\n            'best_der': self.best_der,\n            'patience_counter': self.patience_counter\n        }, filepath)\n        print(f\"Checkpoint saved: {filepath}\")\n    \n    def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints_phase3_fixed'):\n        \"\"\"Full training loop\"\"\"\n        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Training Configuration:\")\n        print(f\"  Contrastive Weight: {self.contrastive_weight}\")\n        print(f\"  CSV Log: {self.csv_path}\")\n        print(f\"{'='*60}\\n\")\n        \n        for epoch in range(1, num_epochs + 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch}/{num_epochs}\")\n            print(f\"{'='*60}\")\n            \n            # Train\n            train_metrics = self.train_epoch(epoch)\n            print(f\"Train - PIT: {train_metrics['pit_loss']:.4f}, \"\n                  f\"Contrast: {train_metrics['contrast_loss']:.4f}, \"\n                  f\"Total: {train_metrics['total_loss']:.4f}\")\n\n            # Validate\n            val_metrics = self.validate(epoch)\n            print(f\"Val - PIT: {val_metrics['pit_loss']:.4f}, \"\n                  f\"Contrast: {val_metrics['contrast_loss']:.4f}, \"\n                  f\"Total: {val_metrics['total_loss']:.4f}\")\n            \n            # Compute DER\n            der = self.compute_der(self.val_loader)\n            val_metrics['der'] = der\n            print(f\"DER: {der:.2f}%\")\n            \n            # Compute JER and other metrics (already in val_metrics from validate())\n            print(f\"JER: {val_metrics['jer']:.4f}\")\n            print(f\"Overlap - P: {val_metrics['overlap_precision']:.3f}, \"\n                  f\"R: {val_metrics['overlap_recall']:.3f}, \"\n                  f\"F1: {val_metrics['overlap_f1']:.3f}\")\n            print(f\"FA Rate: {val_metrics['fa_rate']:.3f}, Miss Rate: {val_metrics['miss_rate']:.3f}\")\n            \n            # Measure inference time (sample 5 files)\n            print(\"Measuring inference time...\")\n            timing_metrics = self.measure_inference_time(num_samples=5)\n            val_metrics.update(timing_metrics)\n            print(f\"Inference - Mel: {timing_metrics['inference_time_mel_sec']:.3f}s, \"\n                  f\"Model: {timing_metrics['inference_time_model_sec']:.3f}s, \"\n                  f\"RTF: {timing_metrics['rtf']:.3f}\")\n            \n            # Compute model size (once per epoch for consistency)\n            if epoch == 1:\n                size_metrics = self.compute_model_size()\n                self.model_size_cache = size_metrics  # Cache for reuse\n                print(f\"Model Size - Total: {size_metrics['model_params_millions']:.2f}M params, \"\n                      f\"Checkpoint: {size_metrics['checkpoint_size_mb']:.1f} MB\")\n            \n            # Add cached model size to metrics\n            val_metrics.update(self.model_size_cache)\n\n\n            \n            # Log validation to CSV\n            self._log_to_csv(epoch, 'val', val_metrics)\n            \n            # Update scheduler\n            self.scheduler.step()\n            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n            \n            # Save checkpoint\n            checkpoint_path = Path(checkpoint_dir) / f'phase3_epoch_{epoch}.pth'\n            self.save_checkpoint(epoch, val_metrics, str(checkpoint_path))\n            \n            # Track best model by DER (more important than loss!)\n            if der < self.best_der:\n                self.best_der = der\n                self.patience_counter = 0\n                best_path = Path(checkpoint_dir) / 'phase3_best_der.pth'\n                self.save_checkpoint(epoch, val_metrics, str(best_path))\n                print(f\"✓ New best DER! {der:.2f}%\")\n            \n            # Early stopping based on val loss\n            if val_metrics['total_loss'] < self.best_val_loss - 1e-4:\n                self.best_val_loss = val_metrics['total_loss']\n                self.patience_counter = 0\n                best_path = Path(checkpoint_dir) / 'phase3_best_loss.pth'\n                self.save_checkpoint(epoch, val_metrics, str(best_path))\n                print(f\"✓ New best loss! {val_metrics['total_loss']:.4f}\")\n            else:\n                self.patience_counter += 1\n                print(f\"Patience: {self.patience_counter}/{self.patience}\")\n                \n                if self.patience_counter >= self.patience:\n                    print(f\"\\n⚠️ Early stopping triggered after {epoch} epochs\")\n                    break\n        \n        print(f\"\\n{'='*60}\")\n        print(\"Training Complete!\")\n        print(f\"Best Val Loss: {self.best_val_loss:.4f}\")\n        print(f\"Best DER: {self.best_der:.2f}%\")\n        print(f\"CSV Log: {self.csv_path}\")\n        print(f\"{'='*60}\")\n\n\nclass ConversationBatchSampler:\n    \"\"\"Sample batches from same conversation\"\"\"\n    def __init__(self, dataset, batch_size=4):\n        self.batch_size = batch_size\n        \n        # Handle if dataset is wrapped in Subset (from train_test_split)\n        if hasattr(dataset, 'dataset'):\n            # It's a Subset\n            base_dataset = dataset.dataset\n            subset_indices = dataset.indices\n            chunks = [base_dataset.chunks[i] for i in subset_indices]\n        else:\n            # It's the raw dataset\n            chunks = dataset.chunks\n            subset_indices = list(range(len(chunks)))\n        \n        # Group indices by conversation\n        self.conv_to_indices = {}\n        for enum_idx, chunk_idx in enumerate(subset_indices):\n            if hasattr(dataset, 'dataset'):\n                chunk = base_dataset.chunks[chunk_idx]\n            else:\n                chunk = chunks[enum_idx]\n            \n            conv_id = chunk['conversation_id']\n            if conv_id not in self.conv_to_indices:\n                self.conv_to_indices[conv_id] = []\n            self.conv_to_indices[conv_id].append(enum_idx)\n        \n        # Sort by chunk index within conversation\n        for conv_id in self.conv_to_indices:\n            indices = self.conv_to_indices[conv_id]\n            if hasattr(dataset, 'dataset'):\n                self.conv_to_indices[conv_id] = sorted(\n                    indices,\n                    key=lambda i: base_dataset.chunks[subset_indices[i]]['chunk_idx']\n                )\n            else:\n                self.conv_to_indices[conv_id] = sorted(\n                    indices,\n                    key=lambda i: chunks[i]['chunk_idx']\n                )\n    \n    def __iter__(self):\n        conv_ids = list(self.conv_to_indices.keys())\n        random.shuffle(conv_ids)\n        \n        for conv_id in conv_ids:\n            indices = self.conv_to_indices[conv_id]\n            \n            for i in range(0, len(indices), self.batch_size):\n                batch = indices[i:i + self.batch_size]\n                if len(batch) == self.batch_size:\n                    yield batch\n    \n    def __len__(self):\n        total = 0\n        for indices in self.conv_to_indices.values():\n            total += len(indices) // self.batch_size\n        return total\n\ndef smooth_speaker_boundaries(activities: np.ndarray, kernel_size: int = 11) -> np.ndarray:\n    \"\"\"\n    Apply median filter to smooth speaker activity boundaries\n    \n    Args:\n        activities: (frames, num_speakers) - numpy array\n        kernel_size: Size of median filter (use odd number, default 11 = 0.11s)\n    \n    Returns:\n        smoothed: (frames, num_speakers)\n    \"\"\"\n    from scipy.ndimage import median_filter\n    \n    smoothed = np.zeros_like(activities)\n    for spk in range(activities.shape[1]):\n        smoothed[:, spk] = median_filter(\n            activities[:, spk], \n            size=kernel_size, \n            mode='nearest'\n        )\n    return smoothed\n\ndef collate_fn_pad_chunks(batch):\n    \"\"\"\n    Custom collate function that pads variable-length chunks to same size\n    \n    Args:\n        batch: List of (mel, labels, metadata) tuples\n    \n    Returns:\n        Batched tensors with padding\n    \"\"\"\n    mels = []\n    labels = []\n    metadata = {\n        'conversation_id': [],\n        'chunk_idx': [],\n        'chunk_start': [],\n        'chunk_end': []\n    }\n    \n    # Find max sizes in this batch\n    max_mel_frames = max(mel.shape[1] for mel, _, _ in batch)\n    max_label_frames = max(label.shape[0] for _, label, _ in batch)\n    \n    for mel, label, meta in batch:\n        # Pad mel spectrogram to max length\n        if mel.shape[1] < max_mel_frames:\n            pad_size = max_mel_frames - mel.shape[1]\n            mel = F.pad(mel, (0, pad_size))  # Pad time dimension\n        mels.append(mel)\n        \n        # Pad labels to max length\n        if label.shape[0] < max_label_frames:\n            pad_size = max_label_frames - label.shape[0]\n            label = F.pad(label, (0, 0, 0, pad_size))  # Pad frame dimension\n        labels.append(label)\n        \n        # Collect metadata\n        for key in metadata:\n            metadata[key].append(meta[key])\n    \n    # Stack into batches\n    mels = torch.stack(mels)\n    labels = torch.stack(labels)\n    \n    return mels, labels, metadata\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    \"\"\"Main training function for Simplified Phase 3\"\"\"\n    \n    set_seed(42)\n    \n    # Simplified configuration\n    config = {\n        'audio_dir': '/kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio',\n        'rttm_dir': '/kaggle/input/voxconverse-dataset/labels/dev',\n        'phase2_checkpoint': '/kaggle/input/3/pytorch/default/1/simple_phase3_best.pth',\n        \n        # Optimized hyperparameters\n        'batch_size': 8,\n        'num_epochs': 30,\n        'learning_rate': 2e-5,\n        'weight_decay': 1e-4,\n        'contrastive_weight': 0.05,\n        'overlap_weight': 5.0,              # Moderate (was 10.0)\n        'overlap_detection_weight': 1.0,\n        \n        # Architecture (simple, proven)\n        'chunk_size': 20.0,\n        'overlap': 3.0,\n        'd_model': 128,\n        'encoder_layers': 6,\n        'decoder_layers': 2,\n        'n_heads': 4,\n        'num_speakers': 6,\n        'projection_dim': 64,\n        \n        # System\n        'num_workers': 4,\n        'persistent_workers': True,\n        'prefetch_factor': 2,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    }\n    \n    print(\"=\"*60)\n    print(\"Simplified Phase 3: Proven Techniques Only\")\n    print(\"=\"*60)\n    print(\"Improvements over Phase 2:\")\n    print(\"  ✓ Attention pooling for speaker embeddings\")\n    print(\"  ✓ Overlap-weighted PIT loss\")\n    print(\"  ✓ Boundary smoothing (inference)\")\n    print(\"=\"*60)\n    print(f\"Device: {config['device']}\")\n    print(f\"Batch Size: {config['batch_size']}\")\n    print(f\"Chunk Size: {config['chunk_size']}s\")\n    print(\"=\"*60)\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Dataset (reuse from Phase 3)\n    print(\"\\nLoading dataset...\")\n    full_dataset = ChunkWiseDataset(\n        audio_dir=config['audio_dir'],\n        rttm_dir=config['rttm_dir'],\n        audio_processor=audio_processor,\n        chunk_size=config['chunk_size'],\n        overlap=config['overlap'],\n        max_speakers=config['num_speakers'],\n        augment=True,\n    )\n    \n    # Split\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n\n    # Disable augmentation for validation\n    val_dataset.dataset.augment = False\n    \n    print(f\"Train chunks: {len(train_dataset)}\")\n    print(f\"Val chunks: {len(val_dataset)}\")\n    \n    # DataLoaders (reuse collate function)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        persistent_workers=config['persistent_workers'],\n        prefetch_factor=config['prefetch_factor'],\n        collate_fn=collate_fn_pad_chunks,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        persistent_workers=config['persistent_workers'],\n        prefetch_factor=config['prefetch_factor'],\n        collate_fn=collate_fn_pad_chunks,\n    )\n    \n    # Simplified model\n    print(\"\\nInitializing Simplified Phase 3 model...\")\n    model = SimplifiedPhase3EEND(\n        input_dim=83,\n        d_model=config['d_model'],\n        encoder_layers=config['encoder_layers'],\n        decoder_layers=config['decoder_layers'],\n        n_heads=config['n_heads'],\n        num_speakers=config['num_speakers'],\n        projection_dim=config['projection_dim']\n    )\n    \n    # Load Phase 2 weights\n    if os.path.exists(config['phase2_checkpoint']):\n        model.load_phase2_weights(config['phase2_checkpoint'])\n    else:\n        print(f\"⚠️ Warning: Phase 2 checkpoint not found\")\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_params:,}\")\n    \n    # Simplified trainer\n    print(\"\\nInitializing trainer...\")\n    trainer = ImprovedPhase3Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=config['device'],\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        contrastive_weight=config['contrastive_weight'],\n        log_dir='./logs',\n    )\n        \n    # Train\n    print(\"\\nStarting training...\")\n    print(\"=\"*60)\n    \n    trainer.train(\n        num_epochs=config['num_epochs'],\n        checkpoint_dir='./checkpoints_phase3_simple'\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Simplified Phase 3 Complete!\")\n    print(\"=\"*60)\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T08:23:53.847108Z","iopub.execute_input":"2025-10-28T08:23:53.847659Z","iopub.status.idle":"2025-10-28T08:26:55.868277Z","shell.execute_reply.started":"2025-10-28T08:23:53.847629Z","shell.execute_reply":"2025-10-28T08:26:55.866863Z"}},"outputs":[{"name":"stdout","text":"✅ CUDA is available!\n🚀 Using GPU: Tesla P100-PCIE-16GB\n💾 GPU Memory: 17.1 GB\n============================================================\nSimplified Phase 3: Proven Techniques Only\n============================================================\nImprovements over Phase 2:\n  ✓ Attention pooling for speaker embeddings\n  ✓ Overlap-weighted PIT loss\n  ✓ Boundary smoothing (inference)\n============================================================\nDevice: cuda\nBatch Size: 8\nChunk Size: 20.0s\n============================================================\n\nLoading dataset...\n\nFound 216 audio files\nFound 216 RTTM files\nMatched 216 audio-RTTM pairs\n\n","output_type":"stream"},{"name":"stderr","text":"Creating chunks: 100%|██████████| 216/216 [00:50<00:00,  4.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Created 4277 chunks from 216 conversations\n  Chunk size: 20.0s, Overlap: 3.0s, Stride: 17.0s\nTrain chunks: 3849\nVal chunks: 428\n\nInitializing Simplified Phase 3 model...\n\n📦 Loading Phase 2 checkpoint from /kaggle/input/3/pytorch/default/1/simple_phase3_best.pth\n✓ Phase 2 weights loaded (overlap_detector randomly initialized)\nTotal parameters: 3,233,608\n\nInitializing trainer...\n✓ CSV log created: logs/training_log_20251028_082451.csv\n\nStarting training...\n============================================================\n\n============================================================\nTraining Configuration:\n  Contrastive Weight: 0.05\n  CSV Log: logs/training_log_20251028_082451.csv\n============================================================\n\n\n============================================================\nEpoch 1/30\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 [Train]:  69%|██████▊   | 330/481 [02:02<00:55,  2.70it/s, pit=0.1073, ovlp=0.2966, cont=5.7435, total=0.5522]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4288415059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2291\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/4288415059.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2279\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2281\u001b[0;31m     trainer.train(\n\u001b[0m\u001b[1;32m   2282\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m         \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./checkpoints_phase3_simple'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4288415059.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, checkpoint_dir)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m             print(f\"Train - PIT: {train_metrics['pit_loss']:.4f}, \"\n\u001b[1;32m   1925\u001b[0m                   \u001b[0;34mf\"Contrast: {train_metrics['contrast_loss']:.4f}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4288415059.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m   1738\u001b[0m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1740\u001b[0;31m                 loss, pit_loss, overlap_loss = self.overlap_aware_loss(\n\u001b[0m\u001b[1;32m   1741\u001b[0m                     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4288415059.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, speaker_logits, overlap_logits, labels)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;31m# Focal BCE for hard examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m             \u001b[0mloss_per_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfocal_bce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeaker_logits_perm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_per_element\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/4288415059.py\u001b[0m in \u001b[0;36mfocal_bce_loss\u001b[0;34m(self, pred_logits, target)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred_probs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m         \u001b[0mfocal_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfocal_gamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfocal_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nAnalyze training logs from CSV\nGenerates plots and summary statistics\n\"\"\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\ndef analyze_training_log(csv_path: str, save_dir: str = './analysis'):\n    \"\"\"\n    Analyze training log and generate visualizations\n    \n    Args:\n        csv_path: Path to training CSV log\n        save_dir: Directory to save plots\n    \"\"\"\n    # Load data\n    df = pd.read_csv(csv_path)\n    \n    # Create output directory\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Split train/val\n    train_df = df[df['phase'] == 'train'].copy()\n    val_df = df[df['phase'] == 'val'].copy()\n    \n    # ============================================================================\n    # 1. SUMMARY STATISTICS\n    # ============================================================================\n    print(\"=\"*60)\n    print(\"TRAINING SUMMARY\")\n    print(\"=\"*60)\n    \n    if len(val_df) > 0:\n        best_epoch = val_df.loc[val_df['total_loss'].idxmin(), 'epoch']\n        best_loss = val_df['total_loss'].min()\n        best_der_epoch = val_df.loc[val_df['der'].idxmin(), 'epoch']\n        best_der = val_df['der'].min()\n        \n        print(f\"\\n📊 Best Performance:\")\n        print(f\"  Best Loss: {best_loss:.4f} at epoch {int(best_epoch)}\")\n        print(f\"  Best DER:  {best_der:.2f}% at epoch {int(best_der_epoch)}\")\n        \n        print(f\"\\n📈 Final Performance (Epoch {int(val_df['epoch'].max())}):\")\n        final_loss = val_df['total_loss'].iloc[-1]\n        final_der = val_df['der'].iloc[-1]\n        print(f\"  Final Loss: {final_loss:.4f}\")\n        print(f\"  Final DER:  {final_der:.2f}%\")\n        \n        print(f\"\\n📉 Improvement:\")\n        initial_loss = val_df['total_loss'].iloc[0]\n        initial_der = val_df['der'].iloc[0]\n        loss_improvement = ((initial_loss - best_loss) / initial_loss) * 100\n        der_improvement = ((initial_der - best_der) / initial_der) * 100\n        print(f\"  Loss: {loss_improvement:.1f}% improvement\")\n        print(f\"  DER:  {der_improvement:.1f}% improvement\")\n    \n    # ============================================================================\n    # 2. LOSS CURVES\n    # ============================================================================\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # Total Loss\n    ax = axes[0, 0]\n    ax.plot(train_df['epoch'], train_df['total_loss'], 'b-', label='Train', linewidth=2)\n    ax.plot(val_df['epoch'], val_df['total_loss'], 'r-', label='Val', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Total Loss', fontsize=12)\n    ax.set_title('Total Loss over Time', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    # PIT Loss\n    ax = axes[0, 1]\n    ax.plot(train_df['epoch'], train_df['pit_loss'], 'b-', label='Train', linewidth=2)\n    ax.plot(val_df['epoch'], val_df['pit_loss'], 'r-', label='Val', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('PIT Loss', fontsize=12)\n    ax.set_title('PIT Loss over Time', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    # Contrastive Loss\n    ax = axes[1, 0]\n    ax.plot(train_df['epoch'], train_df['contrast_loss'], 'b-', label='Train', linewidth=2)\n    ax.plot(val_df['epoch'], val_df['contrast_loss'], 'r-', label='Val', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Contrastive Loss', fontsize=12)\n    ax.set_title('Contrastive Loss over Time', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    # DER\n    ax = axes[1, 1]\n    ax.plot(val_df['epoch'], val_df['der'], 'g-', linewidth=2, marker='o', markersize=4)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('DER (%)', fontsize=12)\n    ax.set_title('Diarization Error Rate (DER)', fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plot_path = save_dir / 'training_curves.png'\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"\\n✓ Training curves saved: {plot_path}\")\n    plt.show()\n    plt.close()\n    \n    # ============================================================================\n    # 3. LOSS COMPONENTS COMPARISON\n    # ============================================================================\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Train loss components\n    ax = axes[0]\n    ax.plot(train_df['epoch'], train_df['pit_loss'], 'b-', label='PIT Loss', linewidth=2)\n    ax.plot(train_df['epoch'], train_df['contrast_loss'], 'r-', label='Contrastive Loss', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Loss Value', fontsize=12)\n    ax.set_title('Training Loss Components', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    # Val loss components\n    ax = axes[1]\n    ax.plot(val_df['epoch'], val_df['pit_loss'], 'b-', label='PIT Loss', linewidth=2)\n    ax.plot(val_df['epoch'], val_df['contrast_loss'], 'r-', label='Contrastive Loss', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Loss Value', fontsize=12)\n    ax.set_title('Validation Loss Components', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plot_path = save_dir / 'loss_components.png'\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"✓ Loss components saved: {plot_path}\")\n    plt.show()\n    plt.close()\n    \n    # ============================================================================\n    # 4. LEARNING RATE SCHEDULE\n    # ============================================================================\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.plot(train_df['epoch'], train_df['learning_rate'], 'b-', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Learning Rate', fontsize=12)\n    ax.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    ax.set_yscale('log')\n    \n    plt.tight_layout()\n    plot_path = save_dir / 'learning_rate.png'\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"✓ Learning rate plot saved: {plot_path}\")\n    plt.show()\n    plt.close()\n    \n    # ============================================================================\n    # 5. CONVERGENCE ANALYSIS\n    # ============================================================================\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Train-Val Gap\n    ax = axes[0]\n    gap = train_df['total_loss'].values - val_df['total_loss'].values\n    ax.plot(train_df['epoch'], gap, 'purple', linewidth=2)\n    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Train Loss - Val Loss', fontsize=12)\n    ax.set_title('Train-Val Gap (Overfitting Check)', fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    \n    # Loss ratio\n    ax = axes[1]\n    ratio = val_df['contrast_loss'] / val_df['pit_loss']\n    ax.plot(val_df['epoch'], ratio, 'orange', linewidth=2)\n    ax.axhline(y=1, color='k', linestyle='--', alpha=0.3, label='Equal contribution')\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Contrastive / PIT Loss Ratio', fontsize=12)\n    ax.set_title('Loss Component Balance', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plot_path = save_dir / 'convergence_analysis.png'\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"✓ Convergence analysis saved: {plot_path}\")\n    plt.show()\n    plt.close()\n    \n    # ============================================================================\n    # 6. SUMMARY TABLE\n    # ============================================================================\n    summary_path = save_dir / 'training_summary.csv'\n    \n    summary_data = []\n    for epoch in val_df['epoch'].unique():\n        epoch_val = val_df[val_df['epoch'] == epoch].iloc[0]\n        epoch_train = train_df[train_df['epoch'] == epoch].iloc[0]\n        \n        summary_data.append({\n            'epoch': int(epoch),\n            'train_loss': epoch_train['total_loss'],\n            'val_loss': epoch_val['total_loss'],\n            'val_der': epoch_val['der'],\n            'train_pit': epoch_train['pit_loss'],\n            'val_pit': epoch_val['pit_loss'],\n            'train_contrast': epoch_train['contrast_loss'],\n            'val_contrast': epoch_val['contrast_loss'],\n            'learning_rate': epoch_train['learning_rate']\n        })\n    \n    summary_df = pd.DataFrame(summary_data)\n    summary_df.to_csv(summary_path, index=False)\n    print(f\"✓ Summary table saved: {summary_path}\")\n    \n    # ============================================================================\n    # 7. RECOMMENDATIONS\n    # ============================================================================\n    print(\"\\n\" + \"=\"*60)\n    print(\"RECOMMENDATIONS\")\n    print(\"=\"*60)\n    \n    if len(val_df) > 5:\n        # Check overfitting\n        recent_gap = gap[-5:].mean()\n        if recent_gap > 0.1:\n            print(\"\\n⚠️  High train-val gap detected:\")\n            print(f\"   Average gap (last 5 epochs): {recent_gap:.4f}\")\n            print(\"   → Consider: stronger regularization, more dropout, or data augmentation\")\n        \n        # Check loss balance\n        final_ratio = ratio.iloc[-1]\n        if final_ratio > 3:\n            print(f\"\\n⚠️  Contrastive loss dominates (ratio: {final_ratio:.2f}):\")\n            print(\"   → Consider: reducing contrastive_weight further\")\n        elif final_ratio < 0.5:\n            print(f\"\\n⚠️  PIT loss dominates (ratio: {final_ratio:.2f}):\")\n            print(\"   → Consider: increasing contrastive_weight\")\n        else:\n            print(f\"\\n✓ Good loss balance (ratio: {final_ratio:.2f})\")\n        \n        # Check convergence\n        recent_improvement = val_df['total_loss'].iloc[-5:].std()\n        if recent_improvement < 0.01:\n            print(\"\\n⚠️  Training has plateaued:\")\n            print(f\"   Recent std dev: {recent_improvement:.6f}\")\n            print(\"   → Consider: lowering learning rate or early stopping\")\n    \n    print(\"\\n\" + \"=\"*60)\n\n\ndef compare_multiple_runs(csv_paths: list, labels: list, save_dir: str = './comparison'):\n    \"\"\"\n    Compare multiple training runs\n    \n    Args:\n        csv_paths: List of CSV file paths\n        labels: List of labels for each run\n        save_dir: Directory to save comparison plots\n    \"\"\"\n    save_dir = Path(save_dir)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    colors = ['b', 'r', 'g', 'orange', 'purple', 'brown']\n    \n    for i, (csv_path, label) in enumerate(zip(csv_paths, labels)):\n        df = pd.read_csv(csv_path)\n        val_df = df[df['phase'] == 'val']\n        color = colors[i % len(colors)]\n        \n        # Total Loss\n        axes[0, 0].plot(val_df['epoch'], val_df['total_loss'], \n                       color=color, label=label, linewidth=2, marker='o', markersize=3)\n        \n        # PIT Loss\n        axes[0, 1].plot(val_df['epoch'], val_df['pit_loss'], \n                       color=color, label=label, linewidth=2, marker='o', markersize=3)\n        \n        # Contrastive Loss\n        axes[1, 0].plot(val_df['epoch'], val_df['contrast_loss'], \n                       color=color, label=label, linewidth=2, marker='o', markersize=3)\n        \n        # DER\n        axes[1, 1].plot(val_df['epoch'], val_df['der'], \n                       color=color, label=label, linewidth=2, marker='o', markersize=3)\n    \n    axes[0, 0].set_title('Total Loss', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    axes[0, 1].set_title('PIT Loss', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Loss')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    axes[1, 0].set_title('Contrastive Loss', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Loss')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    axes[1, 1].set_title('DER', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('DER (%)')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plot_path = save_dir / 'run_comparison.png'\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"✓ Comparison plot saved: {plot_path}\")\n    plt.show()\n    plt.close()\n\n\n# ============================================================================\n# USAGE\n# ============================================================================\n\nif __name__ == '__main__':\n    # Single run analysis\n    print(\"Analyzing training log...\")\n    analyze_training_log(\n        csv_path='/kaggle/working/logs/training_log_20251020_133914.csv',  # Update with your own file\n        save_dir='./analysis'\n    )\n    \n    # Multiple runs comparison (optional)\n    # compare_multiple_runs(\n    #     csv_paths=[\n    #         './logs/run1.csv',\n    #         './logs/run2.csv',\n    #         './logs/run3.csv'\n    #     ],\n    #     labels=[\n    #         'Baseline (weight=0.05)',\n    #         'Reduced weight (0.02)',\n    #         'With augmentation'\n    #     ],\n    #     save_dir='./comparison'\n    # )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:09:43.314388Z","iopub.execute_input":"2025-10-20T15:09:43.314724Z","iopub.status.idle":"2025-10-20T15:09:48.060232Z","shell.execute_reply.started":"2025-10-20T15:09:43.314702Z","shell.execute_reply":"2025-10-20T15:09:48.059563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List, Optional\nimport random\nfrom tqdm import tqdm\nimport math\nfrom itertools import permutations\nfrom scipy.optimize import linear_sum_assignment\nimport torch.nn.functional as F\nfrom torch.utils.data import BatchSampler\ndef set_seed(seed=36):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"✅ CUDA is available!\")\n        print(f\"🚀 Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        torch.cuda.empty_cache()\n    else:\n        device = torch.device(\"cpu\")\n        print(\"⚠️  CUDA not available, using CPU\")\n    return device\nset_seed()\nsetup_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:41:38.272876Z","iopub.execute_input":"2025-10-20T08:41:38.273167Z","iopub.status.idle":"2025-10-20T08:41:38.286586Z","shell.execute_reply.started":"2025-10-20T08:41:38.273151Z","shell.execute_reply":"2025-10-20T08:41:38.285919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# For test set but doesnt exist","metadata":{}},{"cell_type":"code","source":"def inference_simplified_phase3(model_path: str, audio_dir: str, output_dir: str,\n                                chunk_size: float = 20.0, overlap: float = 3.0):\n    \"\"\"\n    Run inference on test set with Simplified Phase 3 model\n    \n    Args:\n        model_path: Path to trained model checkpoint\n        audio_dir: Directory with test audio files\n        output_dir: Where to save RTTM predictions\n        chunk_size: Chunk size in seconds (match training: 20.0)\n        overlap: Overlap in seconds (match training: 3.0)\n    \"\"\"\n    from pathlib import Path\n    import torch\n    import torchaudio\n    import numpy as np\n    from tqdm import tqdm\n    \n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    print(\"=\"*60)\n    print(\"Simplified Phase 3 Inference\")\n    print(\"=\"*60)\n    print(f\"Model: {model_path}\")\n    print(f\"Audio dir: {audio_dir}\")\n    print(f\"Output dir: {output_dir}\")\n    print(f\"Chunk size: {chunk_size}s, Overlap: {overlap}s\")\n    print(f\"Device: {device}\")\n    print(\"=\"*60)\n    \n    # Load model\n    print(\"\\n📦 Loading model...\")\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    model = SimplifiedPhase3EEND(\n        input_dim=83,\n        d_model=128,\n        encoder_layers=6,\n        decoder_layers=2,\n        n_heads=4,\n        num_speakers=6,\n        projection_dim=64\n    )\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    print(\"✓ Model loaded successfully\")\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Get test files\n    audio_dir = Path(audio_dir)\n    audio_files = sorted(audio_dir.glob('*.wav'))\n    \n    if len(audio_files) == 0:\n        print(f\"❌ No audio files found in {audio_dir}\")\n        return\n    \n    print(f\"\\n🎤 Found {len(audio_files)} test files\")\n    print(f\"Processing with chunk size {chunk_size}s, stride {chunk_size - overlap}s\\n\")\n    \n    # Process each file\n    for audio_file in tqdm(audio_files, desc=\"Processing\"):\n        try:\n            # Get audio info\n            info = torchaudio.info(str(audio_file))\n            duration = info.num_frames / info.sample_rate\n            sample_rate = info.sample_rate\n            \n            # Process in chunks\n            stride = chunk_size - overlap\n            all_predictions = []\n            \n            chunk_start = 0.0\n            while chunk_start < duration:\n                chunk_end = min(chunk_start + chunk_size, duration)\n                actual_chunk_duration = chunk_end - chunk_start\n                \n                # Skip very short chunks at the end\n                if actual_chunk_duration < chunk_size * 0.3:\n                    break\n                \n                # Load chunk\n                start_frame = int(chunk_start * sample_rate)\n                num_frames = int(actual_chunk_duration * sample_rate)\n                \n                waveform, sr = torchaudio.load(\n                    str(audio_file),\n                    frame_offset=start_frame,\n                    num_frames=num_frames\n                )\n                \n                # Resample if needed\n                if sr != 16000:\n                    resampler = torchaudio.transforms.Resample(sr, 16000)\n                    waveform = resampler(waveform)\n                \n                # Pad to expected length if needed\n                expected_samples = int(chunk_size * 16000)\n                if waveform.shape[1] < expected_samples:\n                    waveform = F.pad(waveform, (0, expected_samples - waveform.shape[1]))\n                elif waveform.shape[1] > expected_samples:\n                    waveform = waveform[:, :expected_samples]\n                \n                # Convert to mel spectrogram\n                mel = audio_processor(waveform.squeeze(0))\n                mel = mel.unsqueeze(0).to(device)\n                \n                # Inference\n                with torch.no_grad():\n                    activities = model.predict(mel)  # (1, frames, num_speakers)\n                    activities = activities.cpu().squeeze(0).numpy()  # (frames, num_speakers)\n                \n                # Apply boundary smoothing (one of improvements!)\n                activities = smooth_speaker_boundaries(activities, kernel_size=11)\n                \n                # Store predictions\n                all_predictions.append({\n                    'chunk_start': chunk_start,\n                    'chunk_end': chunk_end,\n                    'activities': activities,\n                })\n                \n                # Move to next chunk\n                chunk_start += stride\n                if chunk_end >= duration:\n                    break\n            \n            # Merge overlapping predictions\n            merged_predictions = merge_overlapping_chunks(\n                all_predictions, \n                overlap=overlap,\n                chunk_size=chunk_size\n            )\n            \n            # Create RTTM file\n            rttm_path = output_dir / f\"{audio_file.stem}.rttm\"\n            create_rttm_from_predictions(\n                merged_predictions, \n                rttm_path, \n                file_id=audio_file.stem,\n                frame_shift=0.01,\n                threshold=0.5\n            )\n            \n        except Exception as e:\n            print(f\"\\n❌ Error processing {audio_file.name}: {e}\")\n            continue\n    \n    print(f\"\\n✓ Inference complete!\")\n    print(f\"📁 Predictions saved to: {output_dir}\")\n    print(\"=\"*60)\n\n\ndef merge_overlapping_chunks(predictions: List[Dict], overlap: float, \n                            chunk_size: float) -> np.ndarray:\n    \"\"\"\n    Merge predictions from overlapping chunks with weighted averaging\n    \n    Args:\n        predictions: List of dicts with 'chunk_start', 'chunk_end', 'activities'\n        overlap: Overlap duration in seconds\n        chunk_size: Chunk duration in seconds\n    \n    Returns:\n        merged: (total_frames, num_speakers) merged predictions\n    \"\"\"\n    if len(predictions) == 0:\n        return np.array([])\n    \n    if len(predictions) == 1:\n        return predictions[0]['activities']\n    \n    # Calculate total duration\n    last_chunk = predictions[-1]\n    total_duration = last_chunk['chunk_end']\n    frame_shift = 0.01  # 10ms frames\n    total_frames = int(total_duration / frame_shift)\n    num_speakers = predictions[0]['activities'].shape[1]\n    \n    # Initialize accumulator\n    merged = np.zeros((total_frames, num_speakers))\n    weights = np.zeros((total_frames, num_speakers))\n    \n    overlap_frames = int(overlap / frame_shift)\n    \n    for pred in predictions:\n        chunk_start = pred['chunk_start']\n        activities = pred['activities']\n        chunk_frames = activities.shape[0]\n        \n        # Calculate frame indices in global timeline\n        start_frame = int(chunk_start / frame_shift)\n        end_frame = start_frame + chunk_frames\n        end_frame = min(end_frame, total_frames)\n        actual_frames = end_frame - start_frame\n        \n        # Create weights: higher in middle, lower at boundaries\n        chunk_weights = np.ones((actual_frames, num_speakers))\n        \n        # Fade in at start (overlap region)\n        if start_frame > 0:  # Not the first chunk\n            fade_in_frames = min(overlap_frames, actual_frames)\n            fade_in = np.linspace(0, 1, fade_in_frames).reshape(-1, 1)\n            chunk_weights[:fade_in_frames] *= fade_in\n        \n        # Fade out at end (overlap region)\n        if end_frame < total_frames:  # Not the last chunk\n            fade_out_frames = min(overlap_frames, actual_frames)\n            fade_out = np.linspace(1, 0, fade_out_frames).reshape(-1, 1)\n            chunk_weights[-fade_out_frames:] *= fade_out\n        \n        # Accumulate\n        merged[start_frame:end_frame] += activities[:actual_frames] * chunk_weights\n        weights[start_frame:end_frame] += chunk_weights\n    \n    # Normalize by weights\n    weights = np.maximum(weights, 1e-8)  # Avoid division by zero\n    merged = merged / weights\n    \n    return merged\n\n\ndef create_rttm_from_predictions(activities: np.ndarray, output_path: str,\n                                 file_id: str, frame_shift: float = 0.01, \n                                 threshold: float = 0.5, \n                                 min_duration: float = 0.3):\n    \"\"\"\n    Convert frame-level predictions to RTTM format with post-processing\n    \n    Args:\n        activities: (frames, num_speakers) - speaker activity probabilities\n        output_path: Path to save RTTM file\n        file_id: File identifier for RTTM\n        frame_shift: Time per frame in seconds\n        threshold: Activity threshold\n        min_duration: Minimum segment duration in seconds\n    \"\"\"\n    num_speakers = activities.shape[1]\n    min_frames = int(min_duration / frame_shift)\n    \n    with open(output_path, 'w') as f:\n        for spk_idx in range(num_speakers):\n            spk_activity = activities[:, spk_idx] > threshold\n            \n            # Find continuous segments\n            segments = []\n            in_segment = False\n            segment_start = 0\n            \n            for frame_idx, active in enumerate(spk_activity):\n                if active and not in_segment:\n                    # Start new segment\n                    in_segment = True\n                    segment_start = frame_idx\n                elif not active and in_segment:\n                    # End segment\n                    in_segment = False\n                    segment_length = frame_idx - segment_start\n                    \n                    # Only keep segments longer than minimum\n                    if segment_length >= min_frames:\n                        segments.append((segment_start, frame_idx))\n                \n            # Handle segment extending to end\n            if in_segment:\n                segment_length = len(spk_activity) - segment_start\n                if segment_length >= min_frames:\n                    segments.append((segment_start, len(spk_activity)))\n            \n            # Write segments to RTTM\n            for start_frame, end_frame in segments:\n                start_time = start_frame * frame_shift\n                duration = (end_frame - start_frame) * frame_shift\n                \n                # RTTM format: SPEAKER <file> 1 <start> <duration> <NA> <NA> <speaker> <NA> <NA>\n                f.write(f\"SPEAKER {file_id} 1 {start_time:.3f} {duration:.3f} <NA> <NA> speaker_{spk_idx} <NA> <NA>\\n\")\n\n# Run inference on VoxConverse test set\ninference_simplified_phase3(\n    model_path='/kaggle/input/3/pytorch/default/1/simple_phase3_best.pth',\n    audio_dir='/kaggle/input/voxconverse-dataset/voxconverse_test_wav/voxconverse_test_wav',\n    output_dir='./voxconverse_test_predictions',\n    chunk_size=20.0,  # Match training\n    overlap=3.0       # Match training\n)\n\nprint(\"\\n🎉 Inference complete!\")\nprint(\"📁 Predictions saved to: ./voxconverse_test_predictions\")\nprint(\"\\nNext steps:\")\nprint(\"1. If we find test labels files later, evaluate with pyannote.metrics right below\")\nprint(\"2. Compare predictions visually\")\nprint(\"3. Include in paper results section (I'll do this lol)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:41:38.287904Z","iopub.execute_input":"2025-10-20T08:41:38.288162Z","iopub.status.idle":"2025-10-20T08:53:51.440267Z","shell.execute_reply.started":"2025-10-20T08:41:38.288138Z","shell.execute_reply":"2025-10-20T08:53:51.439561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q pyannote.metrics pyannote.core\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:53:51.440978Z","iopub.execute_input":"2025-10-20T08:53:51.441371Z","iopub.status.idle":"2025-10-20T08:54:02.778837Z","shell.execute_reply.started":"2025-10-20T08:53:51.441353Z","shell.execute_reply":"2025-10-20T08:54:02.778104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom pyannote.core import Annotation, Segment\nfrom pyannote.metrics.diarization import DiarizationErrorRate\n\n\ndef evaluate_with_pyannote(pred_dir: str, ref_dir: str):\n    \"\"\"\n    Evaluate predictions with pyannote.metrics\n    \n    Args:\n        pred_dir: Directory with predicted RTTM files\n        ref_dir: Directory with reference RTTM files\n    \"\"\"\n    from pyannote.core import Annotation, Segment\n    from pyannote.metrics.diarization import DiarizationErrorRate\n    from pathlib import Path\n    \n    pred_dir = Path(pred_dir)\n    ref_dir = Path(ref_dir)\n    \n    # Initialize metric with collar tolerance\n    metric = DiarizationErrorRate(collar=0.25, skip_overlap=False)\n    \n    # Get all prediction files\n    pred_files = sorted(pred_dir.glob('*.rttm'))\n    \n    print(f\"\\n📊 Evaluating {len(pred_files)} files...\")\n    \n    for pred_file in tqdm(pred_files):\n        ref_file = ref_dir / pred_file.name\n        \n        if not ref_file.exists():\n            print(f\"⚠️  No reference for {pred_file.name}\")\n            continue\n        \n        # Load predictions\n        pred_ann = load_rttm(pred_file)\n        ref_ann = load_rttm(ref_file)\n        \n        # Compute metric\n        metric(ref_ann, pred_ann)\n    \n    # Print results\n    print(\"\\n\" + \"=\"*60)\n    print(\"Diarization Error Rate (DER)\")\n    print(\"=\"*60)\n    print(f\"DER: {abs(metric):.2f}%\")\n    print(f\"  False Alarm: {metric['false alarm']:.2f}%\")\n    print(f\"  Missed Detection: {metric['missed detection']:.2f}%\")\n    print(f\"  Confusion: {metric['confusion']:.2f}%\")\n    print(\"=\"*60)\n\ndef load_rttm(rttm_path: str) -> Annotation:\n    \"\"\"Load RTTM file into pyannote Annotation\"\"\"\n    from pyannote.core import Annotation, Segment\n    \n    annotation = Annotation()\n    \n    with open(rttm_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) < 8 or parts[0] != 'SPEAKER':\n                continue\n            \n            start = float(parts[3])\n            duration = float(parts[4])\n            speaker = parts[7]\n            \n            annotation[Segment(start, start + duration)] = speaker\n    \n    return annotation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:54:02.780926Z","iopub.execute_input":"2025-10-20T08:54:02.781223Z","iopub.status.idle":"2025-10-20T08:54:03.408638Z","shell.execute_reply.started":"2025-10-20T08:54:02.781199Z","shell.execute_reply":"2025-10-20T08:54:03.408055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:54:03.409384Z","iopub.execute_input":"2025-10-20T08:54:03.409806Z","iopub.status.idle":"2025-10-20T08:54:06.531646Z","shell.execute_reply.started":"2025-10-20T08:54:03.409765Z","shell.execute_reply":"2025-10-20T08:54:06.530669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: Analyze VoxConverse Speaker Distribution\nfrom tqdm import tqdm\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef analyze_voxconverse_speakers(rttm_dir: str):\n    \"\"\"Analyze speaker distribution in VoxConverse\"\"\"\n    rttm_dir = Path(rttm_dir)\n    rttm_files = sorted(rttm_dir.glob('*.rttm'))\n    \n    total_speakers = []\n    max_simultaneous = []\n    \n    for rttm_file in tqdm(rttm_files, desc=\"Analyzing RTTM files\"):\n        # Parse RTTM\n        speaker_segments = {}\n        with open(rttm_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 8 or parts[0] != 'SPEAKER':\n                    continue\n                \n                start = float(parts[3])\n                dur = float(parts[4])\n                speaker = parts[7]\n                \n                if speaker not in speaker_segments:\n                    speaker_segments[speaker] = []\n                \n                speaker_segments[speaker].append({\n                    'start': start,\n                    'end': start + dur\n                })\n        \n        # Total unique speakers\n        total_speakers.append(len(speaker_segments))\n        \n        # Calculate max simultaneous speakers\n        events = []\n        for spk, segments in speaker_segments.items():\n            for seg in segments:\n                events.append((seg['start'], 1))\n                events.append((seg['end'], -1))\n        \n        events.sort()\n        \n        current = 0\n        max_sim = 0\n        for time, delta in events:\n            current += delta\n            max_sim = max(max_sim, current)\n        \n        max_simultaneous.append(max_sim)\n    \n    # Print statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"VoxConverse Speaker Statistics\")\n    print(\"=\"*60)\n    print(f\"Total conversations: {len(rttm_files)}\")\n    print(\"\\nTotal unique speakers per conversation:\")\n    print(f\"  Mean: {np.mean(total_speakers):.1f}\")\n    print(f\"  Median: {np.median(total_speakers):.0f}\")\n    print(f\"  Max: {np.max(total_speakers)}\")\n    print(f\"  95th percentile: {np.percentile(total_speakers, 95):.0f}\")\n    print(\"\\nMax simultaneous speakers:\")\n    print(f\"  Mean: {np.mean(max_simultaneous):.1f}\")\n    print(f\"  Median: {np.median(max_simultaneous):.0f}\")\n    print(f\"  Max: {np.max(max_simultaneous)}\")\n    print(f\"  95th percentile: {np.percentile(max_simultaneous, 95):.0f}\")\n    print(\"\\n✅ Recommendation:\")\n    percentile_95 = np.percentile(max_simultaneous, 95)\n    if percentile_95 <= 6:\n        print(f\"   Use num_speakers=6 (covers 95% of cases)\")\n    elif percentile_95 <= 8:\n        print(f\"   Use num_speakers=8 (covers 95% of cases)\")\n    else:\n        print(f\"   Use num_speakers=10 (for safety)\")\n    print(\"=\"*60)\n    \n    # Plot\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].hist(total_speakers, bins=range(1, max(total_speakers)+2), edgecolor='black', alpha=0.7)\n    axes[0].set_xlabel('Total Unique Speakers in Conversation', fontsize=12)\n    axes[0].set_ylabel('Number of Conversations', fontsize=12)\n    axes[0].set_title('Distribution: Total Speakers', fontsize=14, fontweight='bold')\n    axes[0].axvline(6, color='red', linestyle='--', linewidth=2, label='Typical model capacity: 6')\n    axes[0].grid(alpha=0.3)\n    axes[0].legend()\n    \n    axes[1].hist(max_simultaneous, bins=range(1, max(max_simultaneous)+2), edgecolor='black', alpha=0.7, color='orange')\n    axes[1].set_xlabel('Max Simultaneous Speakers', fontsize=12)\n    axes[1].set_ylabel('Number of Conversations', fontsize=12)\n    axes[1].set_title('Distribution: Max Simultaneous Speakers', fontsize=14, fontweight='bold')\n    axes[1].axvline(6, color='red', linestyle='--', linewidth=2, label='Typical model capacity: 6')\n    axes[1].grid(alpha=0.3)\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.savefig('voxconverse_speaker_stats.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"\\n✓ Visualization saved to voxconverse_speaker_stats.png\\n\")\n    \n    return {\n        'total_speakers': total_speakers,\n        'max_simultaneous': max_simultaneous\n    }\n\n# Run analysis\nstats = analyze_voxconverse_speakers('/kaggle/input/voxconverse-dataset/labels/dev')\n\n# Print some examples\nprint(\"\\n📊 Sample conversations:\")\nprint(f\"Conversations with >6 total speakers: {sum(1 for x in stats['total_speakers'] if x > 6)}\")\nprint(f\"Conversations with >6 simultaneous speakers: {sum(1 for x in stats['max_simultaneous'] if x > 6)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T08:54:06.53289Z","iopub.execute_input":"2025-10-20T08:54:06.533145Z","iopub.status.idle":"2025-10-20T08:54:08.242328Z","shell.execute_reply.started":"2025-10-20T08:54:06.533121Z","shell.execute_reply":"2025-10-20T08:54:08.241561Z"}},"outputs":[],"execution_count":null}]}