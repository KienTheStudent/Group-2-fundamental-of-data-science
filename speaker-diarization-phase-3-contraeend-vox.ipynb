{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3085190,"sourceType":"datasetVersion","datasetId":1877225},{"sourceId":604367,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":453195,"modelId":469493}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nContraEEND - Phase 3: Adaptive Chunk-wise EEND with Memory\nMemory-augmented processing for long-form conversations\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List, Optional\nimport random\nfrom tqdm import tqdm\nimport math\nfrom itertools import permutations\nfrom scipy.optimize import linear_sum_assignment\nimport torch.nn.functional as F\nfrom torch.utils.data import BatchSampler\n\n# =============================================================================\n# REUSE FROM PHASE 1 & 2\n# ============================================================================\n\ndef set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"âœ… CUDA is available!\")\n        print(f\"ðŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        torch.cuda.empty_cache()\n    else:\n        device = torch.device(\"cpu\")\n        print(\"âš ï¸  CUDA not available, using CPU\")\n    return device\n\nsetup_device()\n\nclass AudioProcessor:\n    \"\"\"Unified audio processing pipeline\"\"\"\n    def __init__(self, sample_rate: int = 16000, n_fft: int = 400,\n                 hop_length: int = 160, n_mels: int = 83, win_length: int = 400):\n        self.sample_rate = sample_rate\n        self.hop_length = hop_length\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length,\n            win_length=win_length, n_mels=n_mels, f_min=20, f_max=sample_rate // 2\n        )\n    \n    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n        if waveform.dim() == 1:\n            waveform = waveform.unsqueeze(0)\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n        mel = self.mel_transform(waveform)\n        log_mel = torch.log(mel + 1e-6)\n        return log_mel.squeeze(0)\n\nclass ConformerBlock(nn.Module):\n    \"\"\"Single Conformer block\"\"\"\n    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        self.ff1 = nn.Sequential(\n            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 4), nn.SiLU(),\n            nn.Dropout(dropout), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout)\n        )\n        self.norm_attn = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.dropout_attn = nn.Dropout(dropout)\n        self.norm_conv = nn.LayerNorm(d_model)\n        self.conv = nn.Sequential(\n            nn.Conv1d(d_model, d_model * 2, 1), nn.GLU(dim=1),\n            nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n            nn.BatchNorm1d(d_model), nn.SiLU(), nn.Conv1d(d_model, d_model, 1), nn.Dropout(dropout)\n        )\n        self.ff2 = nn.Sequential(\n            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 4), nn.SiLU(),\n            nn.Dropout(dropout), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout)\n        )\n        self.norm_out = nn.LayerNorm(d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + 0.5 * self.ff1(x)\n        x_norm = self.norm_attn(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + self.dropout_attn(attn_out)\n        x_norm = self.norm_conv(x)\n        x_conv = self.conv(x_norm.transpose(1, 2))\n        x = x + x_conv.transpose(1, 2)\n        x = x + 0.5 * self.ff2(x)\n        return self.norm_out(x)\n\nclass ConformerEncoder(nn.Module):\n    \"\"\"Conformer encoder with exact output size calculation\"\"\"\n    def __init__(self, input_dim: int = 83, d_model: int = 128, n_layers: int = 6,\n                 n_heads: int = 4, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        self.subsampling = nn.Sequential(\n            nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1), nn.ReLU()\n        )\n        self.pos_encoding = PositionalEncoding(d_model, dropout)\n        self.blocks = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel, dropout) for _ in range(n_layers)\n        ])\n        self.d_model = d_model\n    \n    @staticmethod\n    def compute_output_frames_static(input_frames: int) -> int:\n        \"\"\"Static method to compute exact output frames\"\"\"\n        frames = ((input_frames + 2 * 1 - 3) // 2) + 1\n        frames = ((frames + 2 * 1 - 3) // 2) + 1\n        return frames\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.subsampling(x)\n        x = x.transpose(1, 2)\n        x = self.pos_encoding(x)\n        for block in self.blocks:\n            x = block(x)\n        return x\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass EENDDecoder(nn.Module):\n    \"\"\"EEND decoder with attention pooling\"\"\"\n    def __init__(self, d_model: int = 128, num_speakers: int = 6, n_layers: int = 2,\n                 n_heads: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.num_speakers = num_speakers\n        \n        # Decoder layers\n        self.decoder_layers = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel=31, dropout=dropout)\n            for _ in range(n_layers)\n        ])\n        \n        # Add attention pooling\n        self.attention_pool = AttentionPooling(d_model)\n        \n        # Output projection\n        self.output_proj = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, num_speakers),\n        )\n    \n    def forward(self, encoded: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            encoded: (batch, frames, d_model)\n        Returns:\n            logits: (batch, frames, num_speakers)\n        \"\"\"\n        x = encoded\n        \n        # Decoder layers\n        for layer in self.decoder_layers:\n            x = layer(x)\n        \n        # Project to speaker logits\n        logits = self.output_proj(x)\n        \n        return logits\n\nclass ContrastiveHead(nn.Module):\n    \"\"\"Frame-level contrastive head\"\"\"\n    def __init__(self, d_model: int = 128, projection_dim: int = 64):\n        super().__init__()\n        self.projection = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, projection_dim)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        proj = self.projection(x)\n        proj = F.normalize(proj, p=2, dim=-1)\n        return proj\n        \nclass AttentionPooling(nn.Module):\n    \"\"\"Self-attention pooling for better speaker embeddings\"\"\"\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.Tanh(),\n            nn.Linear(d_model // 2, 1)\n        )\n    \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, frames, d_model)\n            mask: (batch, frames) - optional, 1 for valid frames\n        Returns:\n            pooled: (batch, d_model)\n        \"\"\"\n        # Compute attention weights\n        attn_weights = self.attention(x).squeeze(-1)  # (batch, frames)\n        \n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(~mask.bool(), -1e9)\n        \n        attn_weights = F.softmax(attn_weights, dim=1)  # (batch, frames)\n        \n        # Weighted sum\n        pooled = (x * attn_weights.unsqueeze(-1)).sum(dim=1)  # (batch, d_model)\n        return pooled\n# ============================================================================\n# PHASE 3: MEMORY COMPONENTS\n# ============================================================================\n\nclass SpeakerMemoryBank(nn.Module):\n    \"\"\"\n    Memory bank that stores speaker representations across chunks.\n    Key innovation: Maintains speaker identity over time!\n    \"\"\"\n    def __init__(self, d_model: int = 128, num_speakers: int = 4, \n                 memory_size: int = 100, temperature: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.num_speakers = num_speakers\n        self.memory_size = memory_size\n        self.temperature = temperature\n        \n        # Memory storage: (num_speakers, memory_size, d_model)\n        self.register_buffer('memory', torch.zeros(num_speakers, memory_size, d_model))\n        self.register_buffer('memory_ptr', torch.zeros(num_speakers, dtype=torch.long))\n        self.register_buffer('memory_filled', torch.zeros(num_speakers, dtype=torch.bool))\n        \n        # Speaker statistics\n        self.register_buffer('speaker_counts', torch.zeros(num_speakers))\n        \n    def update(self, embeddings: torch.Tensor, speaker_labels: torch.Tensor):\n        \"\"\"\n        Update memory with new speaker embeddings\n        \n        Args:\n            embeddings: (batch, frames, d_model)\n            speaker_labels: (batch, frames, num_speakers) - binary\n        \"\"\"\n        # Detach to prevent backprop through memory updates\n        embeddings = embeddings.detach()\n        speaker_labels = speaker_labels.detach()\n\n        batch_size, frames, _ = embeddings.shape\n        \n        for spk_idx in range(self.num_speakers):\n            # Get frames for this speaker\n            spk_mask = speaker_labels[:, :, spk_idx] > 0.5  # (batch, frames)\n            \n            if spk_mask.sum() == 0:\n                continue\n            \n            # Extract speaker embeddings\n            spk_embeddings = embeddings[spk_mask]  # (N, d_model)\n            \n            # Aggregate to single representation (mean pooling)\n            spk_repr = spk_embeddings.mean(dim=0)  # (d_model,)\n            \n            # Store in memory (FIFO)\n            ptr = self.memory_ptr[spk_idx].item()\n            self.memory[spk_idx, ptr] = spk_repr\n            \n            # Update pointer\n            self.memory_ptr[spk_idx] = (ptr + 1) % self.memory_size\n            \n            # Mark as filled after first full cycle\n            if ptr == self.memory_size - 1:\n                self.memory_filled[spk_idx] = True\n            \n            # Update statistics\n            self.speaker_counts[spk_idx] += spk_mask.sum().item()\n    \n    def query(self, embeddings: torch.Tensor, top_k: int = 5) -> torch.Tensor:\n        \"\"\"\n        Query memory to get speaker context\n        \n        Args:\n            embeddings: (batch, frames, d_model)\n            top_k: Number of closest memory entries to retrieve\n        \n        Returns:\n            memory_context: (batch, frames, d_model)\n        \"\"\"\n        batch_size, frames, d_model = embeddings.shape\n        \n        # Flatten embeddings for efficient computation\n        emb_flat = embeddings.reshape(-1, d_model)  # (B*F, D)\n        \n        # Compute similarity with all memory entries\n        memory_flat = self.memory.reshape(-1, d_model)  # (S*M, D)\n        \n        # Cosine similarity\n        sim = torch.mm(emb_flat, memory_flat.T)  # (B*F, S*M)\n        \n        # Get top-k most similar memories\n        top_k_sim, top_k_idx = torch.topk(sim, k=min(top_k, memory_flat.shape[0]), dim=1)\n        \n        # Retrieve corresponding memory entries\n        top_k_memories = memory_flat[top_k_idx]  # (B*F, top_k, D)\n        \n        # Weighted aggregation using similarity as attention\n        weights = F.softmax(top_k_sim / self.temperature, dim=1).unsqueeze(-1)  # (B*F, top_k, 1)\n        context = (top_k_memories * weights).sum(dim=1)  # (B*F, D)\n        \n        # Reshape back\n        context = context.reshape(batch_size, frames, d_model)\n        \n        return context\n    \n    def get_speaker_prototypes(self) -> torch.Tensor:\n        \"\"\"\n        Get current speaker prototypes (mean of memory)\n        \n        Returns:\n            prototypes: (num_speakers, d_model)\n        \"\"\"\n        prototypes = []\n        for spk_idx in range(self.num_speakers):\n            if self.memory_filled[spk_idx]:\n                # Use all memory\n                proto = self.memory[spk_idx].mean(dim=0)\n            else:\n                # Use filled portion\n                ptr = self.memory_ptr[spk_idx].item()\n                if ptr > 0:\n                    proto = self.memory[spk_idx, :ptr].mean(dim=0)\n                else:\n                    proto = torch.zeros(self.d_model, device=self.memory.device)\n            prototypes.append(proto)\n        \n        return torch.stack(prototypes)  # (num_speakers, d_model)\n    \n    def reset(self):\n        \"\"\"Reset memory bank (for new conversation)\"\"\"\n        self.memory.zero_()\n        self.memory_ptr.zero_()\n        self.memory_filled.zero_()\n        self.speaker_counts.zero_()\n\n\nclass CrossChunkAttention(nn.Module):\n    \"\"\"\n    Cross-chunk attention to maintain temporal consistency.\n    Attends to memory context from previous chunks.\n    \"\"\"\n    def __init__(self, d_model: int = 128, n_heads: int = 4, dropout: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        \n        # Multi-head cross attention\n        self.cross_attn = nn.MultiheadAttention(\n            d_model, n_heads, dropout=dropout, batch_first=True\n        )\n        \n        # Layer norm\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        # Feed-forward\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_model * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, query: torch.Tensor, memory_context: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            query: Current chunk embeddings (batch, frames, d_model)\n            memory_context: Memory context (batch, frames, d_model)\n        \n        Returns:\n            attended: Attended embeddings (batch, frames, d_model)\n        \"\"\"\n        # Cross attention: query attends to memory\n        attn_out, _ = self.cross_attn(\n            self.norm1(query),\n            self.norm1(memory_context),\n            self.norm1(memory_context)\n        )\n        \n        # Residual connection\n        x = query + attn_out\n        \n        # Feed-forward with residual\n        x = x + self.ffn(self.norm2(x))\n        \n        return x\n\n\nclass SpeakerTracker(nn.Module):\n    \"\"\"\n    Speaker tracking module that resolves permutation problem.\n    Matches current chunk speakers to memory bank speakers.\n    Uses Hungarian algorithm for optimal assignment.\n    \"\"\"\n    def __init__(self, d_model: int = 128, similarity_threshold: float = 0.7):\n        super().__init__()\n        self.d_model = d_model\n        self.similarity_threshold = similarity_threshold\n        \n        # Projection for speaker embeddings\n        self.speaker_proj = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.ReLU(),\n            nn.Linear(d_model, d_model)\n        )\n    \n    def compute_speaker_embeddings(self, encoded: torch.Tensor, \n                                   activities: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Extract speaker embeddings from encoded features and activities\n        \n        Args:\n            encoded: (batch, frames, d_model)\n            activities: (batch, frames, num_speakers) - probabilities\n        \n        Returns:\n            speaker_embeddings: (batch, num_speakers, d_model)\n        \"\"\"\n        batch_size, frames, d_model = encoded.shape\n        num_speakers = activities.shape[2]\n        \n        speaker_embeddings = []\n        \n        for b in range(batch_size):\n            batch_embeddings = []\n            for spk in range(num_speakers):\n                # Weighted pooling by speaker activity\n                weights = activities[b, :, spk].unsqueeze(-1)  # (frames, 1)\n                \n                # Normalize weights\n                weight_sum = weights.sum()\n                if weight_sum > 0:\n                    weights = weights / weight_sum\n                else:\n                    weights = torch.ones_like(weights) / frames\n                \n                # Weighted sum\n                spk_emb = (encoded[b] * weights).sum(dim=0)  # (d_model,)\n                batch_embeddings.append(spk_emb)\n            \n            speaker_embeddings.append(torch.stack(batch_embeddings))\n        \n        speaker_embeddings = torch.stack(speaker_embeddings)  # (batch, num_speakers, d_model)\n        \n        # Project\n        speaker_embeddings = self.speaker_proj(speaker_embeddings)\n        \n        return speaker_embeddings\n    \n    def match_speakers(self, current_embeddings: torch.Tensor,\n                      memory_prototypes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Match current speakers to memory bank speakers using Hungarian algorithm\n        \n        Args:\n            current_embeddings: (batch, num_speakers, d_model)\n            memory_prototypes: (num_speakers, d_model)\n        \n        Returns:\n            permutation: (batch, num_speakers) - indices for reordering\n        \"\"\"\n        batch_size, num_speakers, d_model = current_embeddings.shape\n        \n        permutations = []\n        \n        for b in range(batch_size):\n            # Compute similarity matrix\n            sim_matrix = torch.mm(\n                current_embeddings[b],  # (num_speakers, d_model)\n                memory_prototypes.T     # (d_model, num_speakers)\n            )  # (num_speakers, num_speakers)\n            \n            # Convert to cost (maximize similarity = minimize negative similarity)\n            cost_matrix = -sim_matrix.detach().cpu().numpy()\n            \n            # Hungarian algorithm\n            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n            \n            # Create permutation tensor\n            perm = torch.tensor(col_ind, dtype=torch.long, device=current_embeddings.device)\n            permutations.append(perm)\n        \n        return torch.stack(permutations)  # (batch, num_speakers)\n    \n    def apply_permutation(self, activities: torch.Tensor, \n                         permutation: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Apply permutation to reorder speaker activities\n        \n        Args:\n            activities: (batch, frames, num_speakers)\n            permutation: (batch, num_speakers)\n        \n        Returns:\n            reordered_activities: (batch, frames, num_speakers)\n        \"\"\"\n        batch_size, frames, num_speakers = activities.shape\n        \n        # Create index tensor for gathering\n        perm_expanded = permutation.unsqueeze(1).expand(batch_size, frames, num_speakers)\n        \n        # Reorder\n        reordered = torch.gather(activities, dim=2, index=perm_expanded)\n        \n        return reordered\n\n\n# ============================================================================\n# PHASE 3: ADAPTIVE CHUNK-WISE EEND MODEL\n# ============================================================================\n\nclass SimplifiedPhase3EEND(nn.Module):\n    \"\"\"\n    Simplified Phase 3: Phase 2 + Attention Pooling\n    No memory bank, no speaker tracking - just proven techniques\n    \"\"\"\n    def __init__(self, input_dim: int = 83, d_model: int = 128,\n                 encoder_layers: int = 6, decoder_layers: int = 2,\n                 n_heads: int = 4, num_speakers: int = 6,\n                 projection_dim: int = 64, dropout: float = 0.1):\n        super().__init__()\n        \n        self.num_speakers = num_speakers\n        \n        # Phase 2 components (proven to work)\n        self.encoder = ConformerEncoder(\n            input_dim=input_dim, d_model=d_model, n_layers=encoder_layers,\n            n_heads=n_heads, dropout=dropout\n        )\n        \n        # Enhanced decoder with attention pooling\n        self.decoder = EENDDecoder(\n            d_model=d_model, num_speakers=num_speakers,\n            n_layers=decoder_layers, n_heads=n_heads, dropout=dropout\n        )\n        \n        # Keep contrastive head (already working in Phase 2)\n        self.contrastive_head = ContrastiveHead(d_model, projection_dim)\n    \n    def forward(self, x: torch.Tensor, return_embeddings: bool = False):\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n            return_embeddings: Whether to return contrastive embeddings\n        Returns:\n            logits: (batch, frames, num_speakers)\n            embeddings: (batch, frames, projection_dim) if return_embeddings\n        \"\"\"\n        # Encode\n        encoded = self.encoder(x)  # (batch, frames, d_model)\n        \n        # Decode to speaker logits\n        logits = self.decoder(encoded)  # (batch, frames, num_speakers)\n        \n        if return_embeddings:\n            # Contrastive embeddings\n            embeddings = self.contrastive_head(encoded)\n            return logits, embeddings\n        \n        return logits\n    \n    def predict(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Inference mode with sigmoid\"\"\"\n        logits = self.forward(x, return_embeddings=False)\n        activities = torch.sigmoid(logits)\n        return activities\n    \n    def load_phase2_weights(self, checkpoint_path: str):\n        \"\"\"Load Phase 2 weights\"\"\"\n        print(f\"\\nðŸ“¦ Loading Phase 2 checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        \n        phase2_state = checkpoint['model_state_dict']\n        model_state = {}\n        \n        for key, value in phase2_state.items():\n            # Skip decoder output layer if num_speakers different\n            if 'decoder.output_proj.3' in key:\n                if value.shape[0] != self.num_speakers:\n                    print(f\"âš ï¸  Skipping {key} (different num_speakers: {value.shape[0]} vs {self.num_speakers})\")\n                    continue\n            \n            if key.startswith(('encoder.', 'decoder.', 'contrastive_head.')):\n                model_state[key] = value\n        \n        # Load weights\n        missing, unexpected = self.load_state_dict(model_state, strict=False)\n        \n        print(\"âœ“ Phase 2 weights loaded successfully\")\n        print(f\"  - Loaded: encoder, decoder (except final layer), contrastive_head\")\n        print(f\"  - Randomly initialized: decoder.output_proj.3 (different num_speakers)\")\n\n\n# ============================================================================\n# PHASE 3: CHUNK-WISE DATASET\n# ============================================================================\n\ndef build_audio_rttm_mapping(audio_dir: str, rttm_dir: str) -> List[Tuple[Path, Path]]:\n    \"\"\"Build mapping - works for both CallHome and VoxConverse\"\"\"\n    audio_dir = Path(audio_dir)\n    rttm_dir = Path(rttm_dir)\n    \n    # Try different extensions\n    audio_files = sorted(audio_dir.glob('*.wav'))\n    if len(audio_files) == 0:\n        audio_files = sorted(audio_dir.glob('*.flac'))\n    if len(audio_files) == 0:\n        audio_files = sorted(audio_dir.glob('*.mp3'))\n    \n    rttm_files = sorted(rttm_dir.glob('*.rttm'))\n    \n    print(f\"\\nFound {len(audio_files)} audio files\")\n    print(f\"Found {len(rttm_files)} RTTM files\")\n    \n    # Build mapping by filename (not index!)\n    audio_map = {}\n    for audio_file in audio_files:\n        # Use stem (filename without extension) as key\n        key = audio_file.stem\n        audio_map[key] = audio_file\n    \n    rttm_map = {}\n    for rttm_file in rttm_files:\n        key = rttm_file.stem\n        rttm_map[key] = rttm_file\n    \n    # Match by filename\n    pairs = []\n    for key in sorted(audio_map.keys()):\n        if key in rttm_map:\n            pairs.append((audio_map[key], rttm_map[key]))\n        else:\n            print(f\"âš ï¸  No RTTM for audio: {key}\")\n    \n    print(f\"Matched {len(pairs)} audio-RTTM pairs\\n\")\n    return pairs\n\n\nclass ChunkWiseDataset(Dataset):\n    \"\"\"\n    Dataset for chunk-wise processing (Phase 3)\n    Processes long conversations in overlapping chunks\n    \"\"\"\n    def __init__(self, audio_dir: str, rttm_dir: str, audio_processor: AudioProcessor,\n                 chunk_size: float = 30.0, overlap: float = 5.0,\n                 sample_rate: int = 16000, max_speakers: int = 20):\n        \n        self.audio_dir = Path(audio_dir)\n        self.rttm_dir = Path(rttm_dir)\n        self.audio_processor = audio_processor\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.sample_rate = sample_rate\n        self.max_speakers = max_speakers\n        \n        # Chunk stride (non-overlapping portion)\n        self.stride = chunk_size - overlap\n        \n        # Build mapping\n        self.audio_rttm_pairs = build_audio_rttm_mapping(audio_dir, rttm_dir)\n        \n        # Parse into chunks\n        self.chunks = self._create_chunks()\n        \n        print(f\"âœ“ Created {len(self.chunks)} chunks from {len(self.audio_rttm_pairs)} conversations\")\n        print(f\"  Chunk size: {chunk_size}s, Overlap: {overlap}s, Stride: {self.stride}s\")\n    \n    def _create_chunks(self) -> List[Dict]:\n        \"\"\"Create overlapping chunks from full conversations\"\"\"\n        chunks = []\n        \n        for audio_path, rttm_path in tqdm(self.audio_rttm_pairs, desc=\"Creating chunks\"):\n            # Get audio duration\n            info = torchaudio.info(str(audio_path))\n            duration = info.num_frames / info.sample_rate\n            \n            # Parse RTTM\n            speaker_segments = {}\n            with open(rttm_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) < 8 or parts[0] != 'SPEAKER':\n                        continue\n                    \n                    start = float(parts[3])\n                    dur = float(parts[4])\n                    speaker = parts[7]\n                    \n                    if speaker not in speaker_segments:\n                        speaker_segments[speaker] = []\n                    \n                    speaker_segments[speaker].append({\n                        'start': start,\n                        'end': start + dur\n                    })\n            \n            if len(speaker_segments) == 0:\n                continue\n            \n            # Create overlapping chunks\n            chunk_start = 0.0\n            chunk_idx = 0\n            \n            while chunk_start < duration:\n                chunk_end = min(chunk_start + self.chunk_size, duration)\n                \n                # Only add chunk if it's long enough\n                if chunk_end - chunk_start >= self.chunk_size * 0.5:\n                    chunks.append({\n                        'audio_path': audio_path,\n                        'rttm_path': rttm_path,\n                        'chunk_start': chunk_start,\n                        'chunk_end': chunk_end,\n                        'speaker_segments': speaker_segments,\n                        'conversation_id': audio_path.stem,\n                        'chunk_idx': chunk_idx,\n                        'total_duration': duration\n                    })\n                    chunk_idx += 1\n                \n                # Move to next chunk\n                chunk_start += self.stride\n                \n                # Stop if we've covered the whole audio\n                if chunk_end >= duration:\n                    break\n        \n        return chunks\n    \n    def _create_label_tensor(self, speaker_segments: Dict, chunk_start: float,\n                            chunk_end: float, num_frames: int) -> torch.Tensor:\n        \"\"\"Create frame-level labels for chunk\"\"\"\n        labels = torch.zeros(num_frames, self.max_speakers)\n        \n        speaker_list = sorted(speaker_segments.keys())[:self.max_speakers]\n        frame_duration = (chunk_end - chunk_start) / num_frames\n        \n        for spk_idx, speaker in enumerate(speaker_list):\n            segments = speaker_segments[speaker]\n            \n            for segment in segments:\n                seg_start = segment['start']\n                seg_end = segment['end']\n                \n                # Check overlap with chunk\n                overlap_start = max(seg_start, chunk_start)\n                overlap_end = min(seg_end, chunk_end)\n                \n                if overlap_start < overlap_end:\n                    # Convert to frame indices\n                    frame_start = int((overlap_start - chunk_start) / frame_duration)\n                    frame_end = int((overlap_end - chunk_start) / frame_duration)\n                    \n                    frame_start = max(0, min(frame_start, num_frames - 1))\n                    frame_end = max(0, min(frame_end, num_frames))\n                    \n                    labels[frame_start:frame_end, spk_idx] = 1.0\n        \n        return labels\n    \n    def __len__(self) -> int:\n        return len(self.chunks)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n        \"\"\"\n        Returns:\n            mel: (n_mels, frames)\n            labels: (encoder_frames, max_speakers)\n            metadata: Dict with chunk info\n        \"\"\"\n        chunk = self.chunks[idx]\n        \n        # Load audio chunk\n        start_frame = int(chunk['chunk_start'] * self.sample_rate)\n        num_frames = int((chunk['chunk_end'] - chunk['chunk_start']) * self.sample_rate)\n        \n        waveform, sr = torchaudio.load(\n            str(chunk['audio_path']),\n            frame_offset=start_frame,\n            num_frames=num_frames\n        )\n        \n        # Resample if needed\n        if sr != self.sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n            waveform = resampler(waveform)\n        \n        # Ensure correct length\n        if waveform.shape[1] < num_frames:\n            waveform = F.pad(waveform, (0, num_frames - waveform.shape[1]))\n        elif waveform.shape[1] > num_frames:\n            waveform = waveform[:, :num_frames]\n        \n        # Convert to mel\n        mel = self.audio_processor(waveform.squeeze(0))\n        \n        # Calculate encoder output frames\n        mel_frames = mel.shape[1]\n        encoder_frames = ConformerEncoder.compute_output_frames_static(mel_frames)\n        \n        # Create labels\n        labels = self._create_label_tensor(\n            chunk['speaker_segments'],\n            chunk['chunk_start'],\n            chunk['chunk_end'],\n            encoder_frames\n        )\n        \n        # Metadata for tracking\n        metadata = {\n            'conversation_id': chunk['conversation_id'],\n            'chunk_idx': chunk['chunk_idx'],\n            'chunk_start': chunk['chunk_start'],\n            'chunk_end': chunk['chunk_end']\n        }\n        \n        return mel, labels, metadata\n\n\n# ============================================================================\n# PHASE 3: LOSS FUNCTIONS WITH HARD NEGATIVE MINING\n# ============================================================================\n\nclass OverlapWeightedPITLoss(nn.Module):\n    \"\"\"PIT loss with emphasis on overlapping speech\"\"\"\n    def __init__(self, overlap_weight: float = 2.0):\n        super().__init__()\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        self.overlap_weight = overlap_weight\n        self._cached_perms = None\n    \n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            pred: (batch, frames, num_speakers) - logits\n            target: (batch, frames, num_speakers) - binary labels\n        Returns:\n            loss: scalar\n        \"\"\"\n        batch_size, frames, num_speakers = pred.shape\n        \n        # Detect overlap frames (>1 speaker active)\n        overlap_mask = (target.sum(dim=2) > 1).float()  # (batch, frames)\n        \n        # Create weights: higher for overlap regions\n        weights = torch.ones(batch_size, frames, 1, device=pred.device)\n        weights[overlap_mask > 0] = self.overlap_weight\n        \n        # Cache permutations\n        if self._cached_perms is None or len(self._cached_perms[0]) != num_speakers:\n            self._cached_perms = list(permutations(range(num_speakers)))\n        \n        min_loss = float('inf')\n        \n        for perm in self._cached_perms:\n            pred_perm = pred[:, :, list(perm)]\n            \n            # Weighted BCE loss\n            loss_per_element = self.bce(pred_perm, target)\n            loss = (loss_per_element * weights).mean()\n            \n            if loss < min_loss:\n                min_loss = loss\n                # Early stopping for efficiency\n                if min_loss < 0.01:\n                    break\n        \n        return min_loss\n\nclass HardNegativeContrastiveLoss(nn.Module):\n    \"\"\"\n    Enhanced contrastive loss with hard negative mining (Phase 3)\n    Mines hard negatives from memory bank\n    \"\"\"\n    def __init__(self, temperature: float = 0.1, max_samples: int = 500,\n                 hard_negative_ratio: float = 0.3):\n        super().__init__()\n        self.temperature = temperature\n        self.max_samples = max_samples\n        self.hard_negative_ratio = hard_negative_ratio\n    \n    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor,\n            memory_prototypes: Optional[torch.Tensor] = None,\n            contrastive_head: Optional[nn.Module] = None) -> torch.Tensor:  #  Add constrastive head parameter\n        \"\"\"\n        Args:\n            embeddings: (batch, frames, projection_dim) - L2 normalized\n            labels: (batch, frames, num_speakers) - binary\n            memory_prototypes: (num_speakers, projection_dim) - speaker prototypes from memory\n        \"\"\"\n        batch_size, frames, proj_dim = embeddings.shape\n        num_speakers = labels.shape[2]\n        \n        total_loss = 0\n        valid_speakers = 0\n        \n        for spk in range(num_speakers):\n            spk_labels = labels[:, :, spk]\n            active_mask = spk_labels > 0.5\n            \n            if active_mask.sum() < 2:\n                continue\n            \n            active_embeddings = embeddings[active_mask]\n            \n            # Sample if too many\n            if len(active_embeddings) > self.max_samples:\n                indices = torch.randperm(len(active_embeddings), device=embeddings.device)[:self.max_samples]\n                active_embeddings = active_embeddings[indices]\n            \n            if len(active_embeddings) < 2:\n                continue\n            \n            # Compute similarity matrix\n            sim_matrix = torch.mm(active_embeddings, active_embeddings.T) / self.temperature\n            \n            # Add hard negatives from memory if available\n            if memory_prototypes is not None and spk < len(memory_prototypes):\n                # Get memory prototype for OTHER speakers (hard negatives)\n                other_speaker_protos = []\n                for other_spk in range(num_speakers):\n                    if other_spk != spk:\n                        other_speaker_protos.append(memory_prototypes[other_spk])\n                \n                if len(other_speaker_protos) > 0:\n                    other_protos = torch.stack(other_speaker_protos)  # (n_others, d_model=128)\n                    \n                    if contrastive_head is not None:\n                        other_protos = contrastive_head.projection(other_protos)  # (n_others, 64)\n                        other_protos = F.normalize(other_protos, p=2, dim=-1)\n                                \n                    # Similarity to hard negatives\n                    hard_neg_sim = torch.mm(active_embeddings, other_protos.T) / self.temperature\n                    \n                    # Select hardest negatives (highest similarity to wrong speakers)\n                    n_hard = int(len(active_embeddings) * self.hard_negative_ratio)\n                    if n_hard > 0:\n                        hardest_sim, _ = hard_neg_sim.max(dim=1)  # Max similarity to wrong speaker\n                        hard_indices = torch.topk(hardest_sim, k=min(n_hard, len(hardest_sim)))[1]\n                        \n                        # Weight hard negatives more\n                        weights = torch.ones(len(active_embeddings), device=embeddings.device)\n                        weights[hard_indices] *= 2.0  # Double weight for hard negatives\n                    else:\n                        weights = torch.ones(len(active_embeddings), device=embeddings.device)\n                else:\n                    weights = torch.ones(len(active_embeddings), device=embeddings.device)\n            else:\n                weights = torch.ones(len(active_embeddings), device=embeddings.device)\n            \n            # Mask diagonal\n            mask = torch.ones_like(sim_matrix)\n            mask.fill_diagonal_(0)\n            \n            # InfoNCE loss with weights\n            exp_sim = torch.exp(sim_matrix) * mask\n            log_prob = sim_matrix - torch.log(exp_sim.sum(1, keepdim=True) + 1e-8)\n            loss = -(mask * log_prob).sum(1) / mask.sum(1)\n            \n            # Apply weights\n            weighted_loss = (loss * weights).sum() / weights.sum()\n            \n            total_loss += weighted_loss\n            valid_speakers += 1\n        \n        if valid_speakers == 0:\n            return torch.tensor(0.0, device=embeddings.device)\n        \n        return total_loss / valid_speakers\n\n\nclass MemoryConsistencyLoss(nn.Module):\n    \"\"\"\n    NEW: Memory consistency loss\n    Encourages consistent speaker representations across chunks\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, speaker_embeddings: torch.Tensor,\n                memory_prototypes: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            speaker_embeddings: (batch, num_speakers, d_model)\n            memory_prototypes: (num_speakers, d_model)\n        \n        Returns:\n            loss: scalar - encourages current embeddings to match memory\n        \"\"\"\n        # Normalize\n        speaker_embeddings = F.normalize(speaker_embeddings, p=2, dim=-1)\n        memory_prototypes = F.normalize(memory_prototypes, p=2, dim=-1)\n        \n        # Cosine similarity\n        similarity = torch.mm(\n            speaker_embeddings.reshape(-1, speaker_embeddings.shape[-1]),\n            memory_prototypes.T\n        )\n        \n        # We want diagonal to be high (each speaker matches its prototype)\n        batch_size = speaker_embeddings.shape[0]\n        num_speakers = speaker_embeddings.shape[1]\n        \n        # Create target: identity matrix repeated for batch\n        target = torch.eye(num_speakers, device=speaker_embeddings.device)\n        target = target.unsqueeze(0).repeat(batch_size, 1, 1).reshape(-1, num_speakers)\n        \n        # Cross-entropy loss\n        log_sim = F.log_softmax(similarity, dim=1)\n        loss = -(target * log_sim).sum(1).mean()\n        \n        return loss\n        \n# ============================================================================\n# PHASE 3: TRAINER\n# ============================================================================\n\nclass SimplifiedPhase3Trainer:\n    \"\"\"Simplified trainer - no memory bank, just efficient training\"\"\"\n    def __init__(self, model: nn.Module, train_loader: DataLoader,\n                 val_loader: DataLoader, device: str = 'cuda',\n                 learning_rate: float = 5e-5, weight_decay: float = 1e-4,\n                 contrastive_weight: float = 0.05):\n        \n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        self.contrastive_weight = contrastive_weight\n        \n        # âœ… Use overlap-weighted loss\n        self.pit_loss = OverlapWeightedPITLoss(overlap_weight=2.0)\n        self.contrastive_loss = HardNegativeContrastiveLoss(temperature=0.1)\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.98)\n        )\n        \n        # Scheduler\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer,\n            T_max=30,\n            eta_min=1e-6\n        )\n        \n        # Mixed precision\n        self.scaler = torch.amp.GradScaler('cuda') if device == 'cuda' else None\n        self.use_amp = device == 'cuda'\n        \n        # Tracking\n        self.best_val_loss = float('inf')\n        self.patience = 10\n        self.patience_counter = 0\n    \n    def train_epoch(self, epoch: int) -> Dict[str, float]:\n        \"\"\"Train one epoch\"\"\"\n        self.model.train()\n        total_pit_loss = 0\n        total_contrast_loss = 0\n        total_loss = 0\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n        \n        for batch_idx, (mel, labels, metadata) in enumerate(pbar):\n            mel = mel.to(self.device)\n            labels = labels.to(self.device)\n            \n            # Forward\n            with torch.amp.autocast('cuda', enabled=self.use_amp):\n                logits, embeddings = self.model(mel, return_embeddings=True)\n                \n                # PIT loss\n                pit_loss = self.pit_loss(logits, labels)\n                \n                # Contrastive loss\n                contrast_loss = self.contrastive_loss(\n                    embeddings, labels, \n                    memory_prototypes=None,\n                    contrastive_head=self.model.contrastive_head\n                )\n                \n                # Combined loss\n                loss = pit_loss + self.contrastive_weight * contrast_loss\n            \n            # Backward\n            self.optimizer.zero_grad()\n            if self.use_amp:\n                self.scaler.scale(loss).backward()\n                self.scaler.unscale_(self.optimizer)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                self.optimizer.step()\n            \n            total_pit_loss += pit_loss.item()\n            total_contrast_loss += contrast_loss.item()\n            total_loss += loss.item()\n            \n            pbar.set_postfix({\n                'pit': f'{pit_loss.item():.4f}',\n                'cont': f'{contrast_loss.item():.4f}',\n                'total': f'{loss.item():.4f}'\n            })\n        \n        n = len(self.train_loader)\n        return {\n            'pit_loss': total_pit_loss / n,\n            'contrast_loss': total_contrast_loss / n,\n            'total_loss': total_loss / n\n        }\n    \n    def validate(self, epoch: int) -> Dict[str, float]:\n        \"\"\"Validate\"\"\"\n        self.model.eval()\n        total_pit_loss = 0\n        total_contrast_loss = 0\n        total_loss = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n            \n            for mel, labels, metadata in pbar:\n                mel = mel.to(self.device)\n                labels = labels.to(self.device)\n                \n                with torch.amp.autocast('cuda', enabled=self.use_amp):\n                    logits, embeddings = self.model(mel, return_embeddings=True)\n                    \n                    pit_loss = self.pit_loss(logits, labels)\n                    contrast_loss = self.contrastive_loss(\n                        embeddings, labels,\n                        memory_prototypes=None,\n                        contrastive_head=self.model.contrastive_head\n                    )\n                    \n                    loss = pit_loss + self.contrastive_weight * contrast_loss\n                \n                total_pit_loss += pit_loss.item()\n                total_contrast_loss += contrast_loss.item()\n                total_loss += loss.item()\n                \n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        n = len(self.val_loader)\n        return {\n            'pit_loss': total_pit_loss / n,\n            'contrast_loss': total_contrast_loss / n,\n            'total_loss': total_loss / n\n        }\n    \n    def compute_der(self, loader: DataLoader) -> float:\n        \"\"\"Compute simplified DER\"\"\"\n        self.model.eval()\n        total_frames = 0\n        error_frames = 0\n        \n        with torch.no_grad():\n            for mel, labels, metadata in loader:\n                mel = mel.to(self.device)\n                labels = labels.to(self.device)\n                \n                with torch.amp.autocast('cuda', enabled=self.use_amp):\n                    activities = self.model.predict(mel)\n                \n                pred_binary = (activities > 0.5).float()\n                errors = torch.abs(pred_binary - labels).sum()\n                frames = labels.numel()\n                \n                error_frames += errors.item()\n                total_frames += frames\n        \n        der = (error_frames / total_frames) * 100 if total_frames > 0 else 0\n        return der\n    \n    def save_checkpoint(self, epoch: int, metrics: Dict, filepath: str):\n        \"\"\"Save checkpoint\"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'metrics': metrics,\n            'best_val_loss': self.best_val_loss,\n            'patience_counter': self.patience_counter\n        }, filepath)\n        print(f\"Checkpoint saved: {filepath}\")\n    \n    def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints_phase3_simple'):\n        \"\"\"Full training loop\"\"\"\n        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        \n        for epoch in range(1, num_epochs + 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch}/{num_epochs}\")\n            print(f\"{'='*60}\")\n            \n            # Train\n            train_metrics = self.train_epoch(epoch)\n            print(f\"Train - PIT: {train_metrics['pit_loss']:.4f}, \"\n                  f\"Contrast: {train_metrics['contrast_loss']:.4f}, \"\n                  f\"Total: {train_metrics['total_loss']:.4f}\")\n            \n            # Validate\n            val_metrics = self.validate(epoch)\n            print(f\"Val - PIT: {val_metrics['pit_loss']:.4f}, \"\n                  f\"Contrast: {val_metrics['contrast_loss']:.4f}, \"\n                  f\"Total: {val_metrics['total_loss']:.4f}\")\n            \n            # Compute DER\n            der = self.compute_der(self.val_loader)\n            print(f\"DER: {der:.2f}%\")\n            \n            # Update scheduler\n            self.scheduler.step()\n            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n            \n            # Save checkpoint\n            checkpoint_path = Path(checkpoint_dir) / f'simple_phase3_epoch_{epoch}.pth'\n            self.save_checkpoint(epoch, val_metrics, str(checkpoint_path))\n            \n            # Early stopping\n            if val_metrics['total_loss'] < self.best_val_loss - 1e-4:\n                self.best_val_loss = val_metrics['total_loss']\n                self.patience_counter = 0\n                best_path = Path(checkpoint_dir) / 'simple_phase3_best.pth'\n                self.save_checkpoint(epoch, val_metrics, str(best_path))\n                print(f\"âœ“ New best model! Val Loss: {val_metrics['total_loss']:.4f}, DER: {der:.2f}%\")\n            else:\n                self.patience_counter += 1\n                print(f\"Patience: {self.patience_counter}/{self.patience}\")\n                \n                if self.patience_counter >= self.patience:\n                    print(f\"\\nâš ï¸ Early stopping triggered after {epoch} epochs\")\n                    break\n        \n        print(f\"\\n{'='*60}\")\n        print(\"Simplified Phase 3 Training Complete!\")\n        print(f\"Best Val Loss: {self.best_val_loss:.4f}\")\n        print(f\"{'='*60}\")\n\n\nclass ConversationBatchSampler:\n    \"\"\"Sample batches from same conversation\"\"\"\n    def __init__(self, dataset, batch_size=4):\n        self.batch_size = batch_size\n        \n        # Handle if dataset is wrapped in Subset (from train_test_split)\n        if hasattr(dataset, 'dataset'):\n            # It's a Subset\n            base_dataset = dataset.dataset\n            subset_indices = dataset.indices\n            chunks = [base_dataset.chunks[i] for i in subset_indices]\n        else:\n            # It's the raw dataset\n            chunks = dataset.chunks\n            subset_indices = list(range(len(chunks)))\n        \n        # Group indices by conversation\n        self.conv_to_indices = {}\n        for enum_idx, chunk_idx in enumerate(subset_indices):\n            if hasattr(dataset, 'dataset'):\n                chunk = base_dataset.chunks[chunk_idx]\n            else:\n                chunk = chunks[enum_idx]\n            \n            conv_id = chunk['conversation_id']\n            if conv_id not in self.conv_to_indices:\n                self.conv_to_indices[conv_id] = []\n            self.conv_to_indices[conv_id].append(enum_idx)\n        \n        # Sort by chunk index within conversation\n        for conv_id in self.conv_to_indices:\n            indices = self.conv_to_indices[conv_id]\n            if hasattr(dataset, 'dataset'):\n                self.conv_to_indices[conv_id] = sorted(\n                    indices,\n                    key=lambda i: base_dataset.chunks[subset_indices[i]]['chunk_idx']\n                )\n            else:\n                self.conv_to_indices[conv_id] = sorted(\n                    indices,\n                    key=lambda i: chunks[i]['chunk_idx']\n                )\n    \n    def __iter__(self):\n        conv_ids = list(self.conv_to_indices.keys())\n        random.shuffle(conv_ids)\n        \n        for conv_id in conv_ids:\n            indices = self.conv_to_indices[conv_id]\n            \n            for i in range(0, len(indices), self.batch_size):\n                batch = indices[i:i + self.batch_size]\n                if len(batch) == self.batch_size:\n                    yield batch\n    \n    def __len__(self):\n        total = 0\n        for indices in self.conv_to_indices.values():\n            total += len(indices) // self.batch_size\n        return total\n\ndef smooth_speaker_boundaries(activities: np.ndarray, kernel_size: int = 11) -> np.ndarray:\n    \"\"\"\n    Apply median filter to smooth speaker activity boundaries\n    \n    Args:\n        activities: (frames, num_speakers) - numpy array\n        kernel_size: Size of median filter (use odd number, default 11 = 0.11s)\n    \n    Returns:\n        smoothed: (frames, num_speakers)\n    \"\"\"\n    from scipy.ndimage import median_filter\n    \n    smoothed = np.zeros_like(activities)\n    for spk in range(activities.shape[1]):\n        smoothed[:, spk] = median_filter(\n            activities[:, spk], \n            size=kernel_size, \n            mode='nearest'\n        )\n    return smoothed\n\ndef collate_fn_pad_chunks(batch):\n    \"\"\"\n    Custom collate function that pads variable-length chunks to same size\n    \n    Args:\n        batch: List of (mel, labels, metadata) tuples\n    \n    Returns:\n        Batched tensors with padding\n    \"\"\"\n    mels = []\n    labels = []\n    metadata = {\n        'conversation_id': [],\n        'chunk_idx': [],\n        'chunk_start': [],\n        'chunk_end': []\n    }\n    \n    # Find max sizes in this batch\n    max_mel_frames = max(mel.shape[1] for mel, _, _ in batch)\n    max_label_frames = max(label.shape[0] for _, label, _ in batch)\n    \n    for mel, label, meta in batch:\n        # Pad mel spectrogram to max length\n        if mel.shape[1] < max_mel_frames:\n            pad_size = max_mel_frames - mel.shape[1]\n            mel = F.pad(mel, (0, pad_size))  # Pad time dimension\n        mels.append(mel)\n        \n        # Pad labels to max length\n        if label.shape[0] < max_label_frames:\n            pad_size = max_label_frames - label.shape[0]\n            label = F.pad(label, (0, 0, 0, pad_size))  # Pad frame dimension\n        labels.append(label)\n        \n        # Collect metadata\n        for key in metadata:\n            metadata[key].append(meta[key])\n    \n    # Stack into batches\n    mels = torch.stack(mels)\n    labels = torch.stack(labels)\n    \n    return mels, labels, metadata\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    \"\"\"Main training function for Simplified Phase 3\"\"\"\n    \n    set_seed(42)\n    \n    # Simplified configuration\n    config = {\n        'audio_dir': '/kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio',\n        'rttm_dir': '/kaggle/input/voxconverse-dataset/labels/dev',\n        'phase2_checkpoint': '/kaggle/input/phase-2-on-vox/pytorch/default/1/contraeend_phase2_best.pth',\n        \n        # Optimized hyperparameters\n        'batch_size': 8,\n        'num_epochs': 30,\n        'learning_rate': 5e-5,\n        'weight_decay': 1e-4,\n        'contrastive_weight': 0.05,\n        \n        # Architecture (simple, proven)\n        'chunk_size': 20.0,\n        'overlap': 3.0,\n        'd_model': 128,\n        'encoder_layers': 6,\n        'decoder_layers': 2,\n        'n_heads': 4,\n        'num_speakers': 6,\n        'projection_dim': 64,\n        \n        # System\n        'num_workers': 4,\n        'persistent_workers': True,\n        'prefetch_factor': 2,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    }\n    \n    print(\"=\"*60)\n    print(\"Simplified Phase 3: Proven Techniques Only\")\n    print(\"=\"*60)\n    print(\"Improvements over Phase 2:\")\n    print(\"  âœ“ Attention pooling for speaker embeddings\")\n    print(\"  âœ“ Overlap-weighted PIT loss\")\n    print(\"  âœ“ Boundary smoothing (inference)\")\n    print(\"=\"*60)\n    print(f\"Device: {config['device']}\")\n    print(f\"Batch Size: {config['batch_size']}\")\n    print(f\"Chunk Size: {config['chunk_size']}s\")\n    print(\"=\"*60)\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Dataset (reuse from Phase 3)\n    print(\"\\nLoading dataset...\")\n    full_dataset = ChunkWiseDataset(\n        audio_dir=config['audio_dir'],\n        rttm_dir=config['rttm_dir'],\n        audio_processor=audio_processor,\n        chunk_size=config['chunk_size'],\n        overlap=config['overlap'],\n        max_speakers=config['num_speakers']\n    )\n    \n    # Split\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Train chunks: {len(train_dataset)}\")\n    print(f\"Val chunks: {len(val_dataset)}\")\n    \n    # DataLoaders (reuse collate function)\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        persistent_workers=config['persistent_workers'],\n        prefetch_factor=config['prefetch_factor'],\n        collate_fn=collate_fn_pad_chunks,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True,\n        persistent_workers=config['persistent_workers'],\n        prefetch_factor=config['prefetch_factor'],\n        collate_fn=collate_fn_pad_chunks,\n    )\n    \n    # Simplified model\n    print(\"\\nInitializing Simplified Phase 3 model...\")\n    model = SimplifiedPhase3EEND(\n        input_dim=83,\n        d_model=config['d_model'],\n        encoder_layers=config['encoder_layers'],\n        decoder_layers=config['decoder_layers'],\n        n_heads=config['n_heads'],\n        num_speakers=config['num_speakers'],\n        projection_dim=config['projection_dim']\n    )\n    \n    # Load Phase 2 weights\n    if os.path.exists(config['phase2_checkpoint']):\n        model.load_phase2_weights(config['phase2_checkpoint'])\n    else:\n        print(f\"âš ï¸ Warning: Phase 2 checkpoint not found\")\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total parameters: {total_params:,}\")\n    \n    # Simplified trainer\n    print(\"\\nInitializing trainer...\")\n    trainer = SimplifiedPhase3Trainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        device=config['device'],\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        contrastive_weight=config['contrastive_weight']\n    )\n    \n    # Train\n    print(\"\\nStarting training...\")\n    print(\"=\"*60)\n    \n    trainer.train(\n        num_epochs=config['num_epochs'],\n        checkpoint_dir='./checkpoints_phase3_simple'\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Simplified Phase 3 Complete!\")\n    print(\"=\"*60)\n\n\n# if __name__ == '__main__':\n#     main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference_on_test_set(model_path: str, audio_dir: str, output_dir: str):\n    \"\"\"\n    Run inference on VoxConverse test set (no RTTM labels)\n    \n    Args:\n        model_path: Path to trained Phase 3 checkpoint\n        audio_dir: /kaggle/input/voxconverse-dataset/voxconverse_test_wav/voxconverse_test_wav\n        output_dir: Where to save predicted RTTM files\n    \"\"\"\n    from pathlib import Path\n    import torch\n    \n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Load model\n    print(f\"Loading model from {model_path}\")\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    model = AdaptiveChunkEEND(\n        input_dim=83,\n        d_model=128,\n        encoder_layers=6,\n        decoder_layers=2,\n        n_heads=4,\n        num_speakers=6,  # VoxConverse\n        projection_dim=64,\n        memory_size=50\n    )\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Get all test files\n    audio_dir = Path(audio_dir)\n    audio_files = sorted(audio_dir.glob('*.wav'))\n    \n    print(f\"\\nProcessing {len(audio_files)} test files...\")\n    \n    for audio_file in tqdm(audio_files):\n        # Get audio info\n        info = torchaudio.info(str(audio_file))\n        duration = info.num_frames / info.sample_rate\n        \n        # Process in chunks\n        chunk_size = 30.0\n        stride = 25.0  # 5s overlap\n        \n        all_predictions = []\n        model.reset_memory()\n        \n        chunk_start = 0.0\n        while chunk_start < duration:\n            chunk_end = min(chunk_start + chunk_size, duration)\n            \n            # Load chunk\n            start_frame = int(chunk_start * 16000)\n            num_frames = int((chunk_end - chunk_start) * 16000)\n            \n            waveform, sr = torchaudio.load(\n                str(audio_file),\n                frame_offset=start_frame,\n                num_frames=num_frames\n            )\n            \n            if sr != 16000:\n                resampler = torchaudio.transforms.Resample(sr, 16000)\n                waveform = resampler(waveform)\n            \n            # Pad if needed\n            if waveform.shape[1] < num_frames:\n                waveform = F.pad(waveform, (0, num_frames - waveform.shape[1]))\n            \n            # Process\n            mel = audio_processor(waveform.squeeze(0))\n            mel = mel.unsqueeze(0).to(device)\n            \n            with torch.no_grad():\n                logits, speaker_embeddings = model(mel, use_memory=True, return_embeddings=False)\n                activities = torch.sigmoid(logits)  # (1, frames, num_speakers)\n            \n            # Update memory\n            encoded = model.encoder(mel)\n            model.update_memory(encoded, activities)\n            \n            # Store predictions\n            all_predictions.append({\n                'chunk_start': chunk_start,\n                'activities': activities.cpu().squeeze(0),  # (frames, num_speakers)\n            })\n            \n            chunk_start += stride\n            if chunk_end >= duration:\n                break\n        \n        # Merge predictions and create RTTM\n        rttm_path = output_dir / f\"{audio_file.stem}.rttm\"\n        create_rttm_from_predictions(all_predictions, rttm_path, frame_shift=0.01)\n    \n    print(f\"\\nâœ“ Predictions saved to {output_dir}\")\n\n\ndef create_rttm_from_predictions(predictions: List[Dict], output_path: str, \n                                 frame_shift: float = 0.01, threshold: float = 0.5):\n    \"\"\"Convert frame-level predictions to RTTM format\"\"\"\n    \n    with open(output_path, 'w') as f:\n        for pred in predictions:\n            chunk_start = pred['chunk_start']\n            activities = pred['activities']  # (frames, num_speakers)\n            \n            for spk_idx in range(activities.shape[1]):\n                spk_activity = activities[:, spk_idx] > threshold\n                \n                # Find continuous segments\n                in_segment = False\n                segment_start = 0\n                \n                for frame_idx, active in enumerate(spk_activity):\n                    if active and not in_segment:\n                        # Start new segment\n                        in_segment = True\n                        segment_start = frame_idx\n                    elif not active and in_segment:\n                        # End segment\n                        in_segment = False\n                        start_time = chunk_start + segment_start * frame_shift\n                        duration = (frame_idx - segment_start) * frame_shift\n                        \n                        # Write RTTM line\n                        # Format: SPEAKER <file> 1 <start> <duration> <NA> <NA> <speaker> <NA> <NA>\n                        f.write(f\"SPEAKER {Path(output_path).stem} 1 {start_time:.3f} {duration:.3f} <NA> <NA> speaker_{spk_idx} <NA> <NA>\\n\")\n                \n                # Handle segment that extends to end\n                if in_segment:\n                    start_time = chunk_start + segment_start * frame_shift\n                    duration = (len(spk_activity) - segment_start) * frame_shift\n                    f.write(f\"SPEAKER {Path(output_path).stem} 1 {start_time:.3f} {duration:.3f} <NA> <NA> speaker_{spk_idx} <NA> <NA>\\n\")\n\n# Run inference script\ninference_on_test_set(\n    model_path='/kaggle/working/checkpoints_phase3_simple/simple_phase3_best.pth',\n    audio_dir='/kaggle/input/voxconverse-dataset/voxconverse_test_wav/voxconverse_test_wav',\n    output_dir='./voxconverse_predictions'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List, Optional\nimport random\nfrom tqdm import tqdm\nimport math\nfrom itertools import permutations\nfrom scipy.optimize import linear_sum_assignment\nimport torch.nn.functional as F\nfrom torch.utils.data import BatchSampler\ndef set_seed(seed=36):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"âœ… CUDA is available!\")\n        print(f\"ðŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n        torch.cuda.empty_cache()\n    else:\n        device = torch.device(\"cpu\")\n        print(\"âš ï¸  CUDA not available, using CPU\")\n    return device\nset_seed()\nsetup_device()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference_simplified_phase3(model_path: str, audio_dir: str, output_dir: str,\n                                chunk_size: float = 20.0, overlap: float = 3.0):\n    \"\"\"\n    Run inference on test set with Simplified Phase 3 model\n    \n    Args:\n        model_path: Path to trained model checkpoint\n        audio_dir: Directory with test audio files\n        output_dir: Where to save RTTM predictions\n        chunk_size: Chunk size in seconds (match training: 20.0)\n        overlap: Overlap in seconds (match training: 3.0)\n    \"\"\"\n    from pathlib import Path\n    import torch\n    import torchaudio\n    import numpy as np\n    from tqdm import tqdm\n    \n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    print(\"=\"*60)\n    print(\"Simplified Phase 3 Inference\")\n    print(\"=\"*60)\n    print(f\"Model: {model_path}\")\n    print(f\"Audio dir: {audio_dir}\")\n    print(f\"Output dir: {output_dir}\")\n    print(f\"Chunk size: {chunk_size}s, Overlap: {overlap}s\")\n    print(f\"Device: {device}\")\n    print(\"=\"*60)\n    \n    # Load model\n    print(\"\\nðŸ“¦ Loading model...\")\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    model = SimplifiedPhase3EEND(\n        input_dim=83,\n        d_model=128,\n        encoder_layers=6,\n        decoder_layers=2,\n        n_heads=4,\n        num_speakers=6,\n        projection_dim=64\n    )\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    print(\"âœ“ Model loaded successfully\")\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Get test files\n    audio_dir = Path(audio_dir)\n    audio_files = sorted(audio_dir.glob('*.wav'))\n    \n    if len(audio_files) == 0:\n        print(f\"âŒ No audio files found in {audio_dir}\")\n        return\n    \n    print(f\"\\nðŸŽ¤ Found {len(audio_files)} test files\")\n    print(f\"Processing with chunk size {chunk_size}s, stride {chunk_size - overlap}s\\n\")\n    \n    # Process each file\n    for audio_file in tqdm(audio_files, desc=\"Processing\"):\n        try:\n            # Get audio info\n            info = torchaudio.info(str(audio_file))\n            duration = info.num_frames / info.sample_rate\n            sample_rate = info.sample_rate\n            \n            # Process in chunks\n            stride = chunk_size - overlap\n            all_predictions = []\n            \n            chunk_start = 0.0\n            while chunk_start < duration:\n                chunk_end = min(chunk_start + chunk_size, duration)\n                actual_chunk_duration = chunk_end - chunk_start\n                \n                # Skip very short chunks at the end\n                if actual_chunk_duration < chunk_size * 0.3:\n                    break\n                \n                # Load chunk\n                start_frame = int(chunk_start * sample_rate)\n                num_frames = int(actual_chunk_duration * sample_rate)\n                \n                waveform, sr = torchaudio.load(\n                    str(audio_file),\n                    frame_offset=start_frame,\n                    num_frames=num_frames\n                )\n                \n                # Resample if needed\n                if sr != 16000:\n                    resampler = torchaudio.transforms.Resample(sr, 16000)\n                    waveform = resampler(waveform)\n                \n                # Pad to expected length if needed\n                expected_samples = int(chunk_size * 16000)\n                if waveform.shape[1] < expected_samples:\n                    waveform = F.pad(waveform, (0, expected_samples - waveform.shape[1]))\n                elif waveform.shape[1] > expected_samples:\n                    waveform = waveform[:, :expected_samples]\n                \n                # Convert to mel spectrogram\n                mel = audio_processor(waveform.squeeze(0))\n                mel = mel.unsqueeze(0).to(device)\n                \n                # Inference\n                with torch.no_grad():\n                    activities = model.predict(mel)  # (1, frames, num_speakers)\n                    activities = activities.cpu().squeeze(0).numpy()  # (frames, num_speakers)\n                \n                # Apply boundary smoothing (one of improvements!)\n                activities = smooth_speaker_boundaries(activities, kernel_size=11)\n                \n                # Store predictions\n                all_predictions.append({\n                    'chunk_start': chunk_start,\n                    'chunk_end': chunk_end,\n                    'activities': activities,\n                })\n                \n                # Move to next chunk\n                chunk_start += stride\n                if chunk_end >= duration:\n                    break\n            \n            # Merge overlapping predictions\n            merged_predictions = merge_overlapping_chunks(\n                all_predictions, \n                overlap=overlap,\n                chunk_size=chunk_size\n            )\n            \n            # Create RTTM file\n            rttm_path = output_dir / f\"{audio_file.stem}.rttm\"\n            create_rttm_from_predictions(\n                merged_predictions, \n                rttm_path, \n                file_id=audio_file.stem,\n                frame_shift=0.01,\n                threshold=0.5\n            )\n            \n        except Exception as e:\n            print(f\"\\nâŒ Error processing {audio_file.name}: {e}\")\n            continue\n    \n    print(f\"\\nâœ“ Inference complete!\")\n    print(f\"ðŸ“ Predictions saved to: {output_dir}\")\n    print(\"=\"*60)\n\n\ndef merge_overlapping_chunks(predictions: List[Dict], overlap: float, \n                            chunk_size: float) -> np.ndarray:\n    \"\"\"\n    Merge predictions from overlapping chunks with weighted averaging\n    \n    Args:\n        predictions: List of dicts with 'chunk_start', 'chunk_end', 'activities'\n        overlap: Overlap duration in seconds\n        chunk_size: Chunk duration in seconds\n    \n    Returns:\n        merged: (total_frames, num_speakers) merged predictions\n    \"\"\"\n    if len(predictions) == 0:\n        return np.array([])\n    \n    if len(predictions) == 1:\n        return predictions[0]['activities']\n    \n    # Calculate total duration\n    last_chunk = predictions[-1]\n    total_duration = last_chunk['chunk_end']\n    frame_shift = 0.01  # 10ms frames\n    total_frames = int(total_duration / frame_shift)\n    num_speakers = predictions[0]['activities'].shape[1]\n    \n    # Initialize accumulator\n    merged = np.zeros((total_frames, num_speakers))\n    weights = np.zeros((total_frames, num_speakers))\n    \n    overlap_frames = int(overlap / frame_shift)\n    \n    for pred in predictions:\n        chunk_start = pred['chunk_start']\n        activities = pred['activities']\n        chunk_frames = activities.shape[0]\n        \n        # Calculate frame indices in global timeline\n        start_frame = int(chunk_start / frame_shift)\n        end_frame = start_frame + chunk_frames\n        end_frame = min(end_frame, total_frames)\n        actual_frames = end_frame - start_frame\n        \n        # Create weights: higher in middle, lower at boundaries\n        chunk_weights = np.ones((actual_frames, num_speakers))\n        \n        # Fade in at start (overlap region)\n        if start_frame > 0:  # Not the first chunk\n            fade_in_frames = min(overlap_frames, actual_frames)\n            fade_in = np.linspace(0, 1, fade_in_frames).reshape(-1, 1)\n            chunk_weights[:fade_in_frames] *= fade_in\n        \n        # Fade out at end (overlap region)\n        if end_frame < total_frames:  # Not the last chunk\n            fade_out_frames = min(overlap_frames, actual_frames)\n            fade_out = np.linspace(1, 0, fade_out_frames).reshape(-1, 1)\n            chunk_weights[-fade_out_frames:] *= fade_out\n        \n        # Accumulate\n        merged[start_frame:end_frame] += activities[:actual_frames] * chunk_weights\n        weights[start_frame:end_frame] += chunk_weights\n    \n    # Normalize by weights\n    weights = np.maximum(weights, 1e-8)  # Avoid division by zero\n    merged = merged / weights\n    \n    return merged\n\n\ndef create_rttm_from_predictions(activities: np.ndarray, output_path: str,\n                                 file_id: str, frame_shift: float = 0.01, \n                                 threshold: float = 0.5, \n                                 min_duration: float = 0.3):\n    \"\"\"\n    Convert frame-level predictions to RTTM format with post-processing\n    \n    Args:\n        activities: (frames, num_speakers) - speaker activity probabilities\n        output_path: Path to save RTTM file\n        file_id: File identifier for RTTM\n        frame_shift: Time per frame in seconds\n        threshold: Activity threshold\n        min_duration: Minimum segment duration in seconds\n    \"\"\"\n    num_speakers = activities.shape[1]\n    min_frames = int(min_duration / frame_shift)\n    \n    with open(output_path, 'w') as f:\n        for spk_idx in range(num_speakers):\n            spk_activity = activities[:, spk_idx] > threshold\n            \n            # Find continuous segments\n            segments = []\n            in_segment = False\n            segment_start = 0\n            \n            for frame_idx, active in enumerate(spk_activity):\n                if active and not in_segment:\n                    # Start new segment\n                    in_segment = True\n                    segment_start = frame_idx\n                elif not active and in_segment:\n                    # End segment\n                    in_segment = False\n                    segment_length = frame_idx - segment_start\n                    \n                    # Only keep segments longer than minimum\n                    if segment_length >= min_frames:\n                        segments.append((segment_start, frame_idx))\n                \n            # Handle segment extending to end\n            if in_segment:\n                segment_length = len(spk_activity) - segment_start\n                if segment_length >= min_frames:\n                    segments.append((segment_start, len(spk_activity)))\n            \n            # Write segments to RTTM\n            for start_frame, end_frame in segments:\n                start_time = start_frame * frame_shift\n                duration = (end_frame - start_frame) * frame_shift\n                \n                # RTTM format: SPEAKER <file> 1 <start> <duration> <NA> <NA> <speaker> <NA> <NA>\n                f.write(f\"SPEAKER {file_id} 1 {start_time:.3f} {duration:.3f} <NA> <NA> speaker_{spk_idx} <NA> <NA>\\n\")\n\n# Run inference on VoxConverse test set\ninference_simplified_phase3(\n    model_path='/kaggle/input/test/pytorch/default/1/simple_phase3_best.pth',\n    audio_dir='/kaggle/input/voxconverse-dataset/voxconverse_test_wav/voxconverse_test_wav',\n    output_dir='./voxconverse_test_predictions',\n    chunk_size=20.0,  # Match training\n    overlap=3.0       # Match training\n)\n\nprint(\"\\nðŸŽ‰ Inference complete!\")\nprint(\"ðŸ“ Predictions saved to: ./voxconverse_test_predictions\")\nprint(\"\\nNext steps:\")\nprint(\"1. If we find test labels files later, evaluate with pyannote.metrics right below\")\nprint(\"2. Compare predictions visually\")\nprint(\"3. Include in paper results section (I'll do this lol)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_with_pyannote(pred_dir: str, ref_dir: str):\n    \"\"\"\n    Evaluate predictions with pyannote.metrics\n    \n    Args:\n        pred_dir: Directory with predicted RTTM files\n        ref_dir: Directory with reference RTTM files\n    \"\"\"\n    from pyannote.core import Annotation, Segment\n    from pyannote.metrics.diarization import DiarizationErrorRate\n    from pathlib import Path\n    \n    pred_dir = Path(pred_dir)\n    ref_dir = Path(ref_dir)\n    \n    # Initialize metric with collar tolerance\n    metric = DiarizationErrorRate(collar=0.25, skip_overlap=False)\n    \n    # Get all prediction files\n    pred_files = sorted(pred_dir.glob('*.rttm'))\n    \n    print(f\"\\nðŸ“Š Evaluating {len(pred_files)} files...\")\n    \n    for pred_file in tqdm(pred_files):\n        ref_file = ref_dir / pred_file.name\n        \n        if not ref_file.exists():\n            print(f\"âš ï¸  No reference for {pred_file.name}\")\n            continue\n        \n        # Load predictions\n        pred_ann = load_rttm(pred_file)\n        ref_ann = load_rttm(ref_file)\n        \n        # Compute metric\n        metric(ref_ann, pred_ann)\n    \n    # Print results\n    print(\"\\n\" + \"=\"*60)\n    print(\"Diarization Error Rate (DER)\")\n    print(\"=\"*60)\n    print(f\"DER: {abs(metric):.2f}%\")\n    print(f\"  False Alarm: {metric['false alarm']:.2f}%\")\n    print(f\"  Missed Detection: {metric['missed detection']:.2f}%\")\n    print(f\"  Confusion: {metric['confusion']:.2f}%\")\n    print(\"=\"*60)\n\ndef load_rttm(rttm_path: str) -> Annotation:\n    \"\"\"Load RTTM file into pyannote Annotation\"\"\"\n    from pyannote.core import Annotation, Segment\n    \n    annotation = Annotation()\n    \n    with open(rttm_path, 'r') as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) < 8 or parts[0] != 'SPEAKER':\n                continue\n            \n            start = float(parts[3])\n            duration = float(parts[4])\n            speaker = parts[7]\n            \n            annotation[Segment(start, start + duration)] = speaker\n    \n    return annotation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tqdm","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell: Analyze VoxConverse Speaker Distribution\nfrom tqdm import tqdm\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef analyze_voxconverse_speakers(rttm_dir: str):\n    \"\"\"Analyze speaker distribution in VoxConverse\"\"\"\n    rttm_dir = Path(rttm_dir)\n    rttm_files = sorted(rttm_dir.glob('*.rttm'))\n    \n    total_speakers = []\n    max_simultaneous = []\n    \n    for rttm_file in tqdm(rttm_files, desc=\"Analyzing RTTM files\"):\n        # Parse RTTM\n        speaker_segments = {}\n        with open(rttm_file, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 8 or parts[0] != 'SPEAKER':\n                    continue\n                \n                start = float(parts[3])\n                dur = float(parts[4])\n                speaker = parts[7]\n                \n                if speaker not in speaker_segments:\n                    speaker_segments[speaker] = []\n                \n                speaker_segments[speaker].append({\n                    'start': start,\n                    'end': start + dur\n                })\n        \n        # Total unique speakers\n        total_speakers.append(len(speaker_segments))\n        \n        # Calculate max simultaneous speakers\n        events = []\n        for spk, segments in speaker_segments.items():\n            for seg in segments:\n                events.append((seg['start'], 1))\n                events.append((seg['end'], -1))\n        \n        events.sort()\n        \n        current = 0\n        max_sim = 0\n        for time, delta in events:\n            current += delta\n            max_sim = max(max_sim, current)\n        \n        max_simultaneous.append(max_sim)\n    \n    # Print statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"VoxConverse Speaker Statistics\")\n    print(\"=\"*60)\n    print(f\"Total conversations: {len(rttm_files)}\")\n    print(\"\\nTotal unique speakers per conversation:\")\n    print(f\"  Mean: {np.mean(total_speakers):.1f}\")\n    print(f\"  Median: {np.median(total_speakers):.0f}\")\n    print(f\"  Max: {np.max(total_speakers)}\")\n    print(f\"  95th percentile: {np.percentile(total_speakers, 95):.0f}\")\n    print(\"\\nMax simultaneous speakers:\")\n    print(f\"  Mean: {np.mean(max_simultaneous):.1f}\")\n    print(f\"  Median: {np.median(max_simultaneous):.0f}\")\n    print(f\"  Max: {np.max(max_simultaneous)}\")\n    print(f\"  95th percentile: {np.percentile(max_simultaneous, 95):.0f}\")\n    print(\"\\nâœ… Recommendation:\")\n    percentile_95 = np.percentile(max_simultaneous, 95)\n    if percentile_95 <= 6:\n        print(f\"   Use num_speakers=6 (covers 95% of cases)\")\n    elif percentile_95 <= 8:\n        print(f\"   Use num_speakers=8 (covers 95% of cases)\")\n    else:\n        print(f\"   Use num_speakers=10 (for safety)\")\n    print(\"=\"*60)\n    \n    # Plot\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].hist(total_speakers, bins=range(1, max(total_speakers)+2), edgecolor='black', alpha=0.7)\n    axes[0].set_xlabel('Total Unique Speakers in Conversation', fontsize=12)\n    axes[0].set_ylabel('Number of Conversations', fontsize=12)\n    axes[0].set_title('Distribution: Total Speakers', fontsize=14, fontweight='bold')\n    axes[0].axvline(6, color='red', linestyle='--', linewidth=2, label='Typical model capacity: 6')\n    axes[0].grid(alpha=0.3)\n    axes[0].legend()\n    \n    axes[1].hist(max_simultaneous, bins=range(1, max(max_simultaneous)+2), edgecolor='black', alpha=0.7, color='orange')\n    axes[1].set_xlabel('Max Simultaneous Speakers', fontsize=12)\n    axes[1].set_ylabel('Number of Conversations', fontsize=12)\n    axes[1].set_title('Distribution: Max Simultaneous Speakers', fontsize=14, fontweight='bold')\n    axes[1].axvline(6, color='red', linestyle='--', linewidth=2, label='Typical model capacity: 6')\n    axes[1].grid(alpha=0.3)\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.savefig('voxconverse_speaker_stats.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"\\nâœ“ Visualization saved to voxconverse_speaker_stats.png\\n\")\n    \n    return {\n        'total_speakers': total_speakers,\n        'max_simultaneous': max_simultaneous\n    }\n\n# Run analysis\nstats = analyze_voxconverse_speakers('/kaggle/input/voxconverse-dataset/labels/dev')\n\n# Print some examples\nprint(\"\\nðŸ“Š Sample conversations:\")\nprint(f\"Conversations with >6 total speakers: {sum(1 for x in stats['total_speakers'] if x > 6)}\")\nprint(f\"Conversations with >6 simultaneous speakers: {sum(1 for x in stats['max_simultaneous'] if x > 6)}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}
